{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#パッケージのimport\" data-toc-modified-id=\"パッケージのimport-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>パッケージのimport</a></div><div class=\"lev1 toc-item\"><a href=\"#動画の描画関数の宣言\" data-toc-modified-id=\"動画の描画関数の宣言-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>動画の描画関数の宣言</a></div><div class=\"lev1 toc-item\"><a href=\"#namedtuple\" data-toc-modified-id=\"namedtuple-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>namedtuple</a></div><div class=\"lev1 toc-item\"><a href=\"#定数の設定\" data-toc-modified-id=\"定数の設定-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>定数の設定</a></div><div class=\"lev1 toc-item\"><a href=\"#経験を保存するメモリクラスを定義します\" data-toc-modified-id=\"経験を保存するメモリクラスを定義します-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>経験を保存するメモリクラスを定義します</a></div><div class=\"lev2 toc-item\"><a href=\"#sandbox\" data-toc-modified-id=\"sandbox-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>sandbox</a></div><div class=\"lev1 toc-item\"><a href=\"#Brain\" data-toc-modified-id=\"Brain-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Brain</a></div><div class=\"lev1 toc-item\"><a href=\"#Agent\" data-toc-modified-id=\"Agent-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Agent</a></div><div class=\"lev1 toc-item\"><a href=\"#Environment\" data-toc-modified-id=\"Environment-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Environment</a></div><div class=\"lev2 toc-item\"><a href=\"#original\" data-toc-modified-id=\"original-81\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>original</a></div><div class=\"lev2 toc-item\"><a href=\"#多少改造したenvironment\" data-toc-modified-id=\"多少改造したenvironment-82\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>多少改造したenvironment</a></div><div class=\"lev1 toc-item\"><a href=\"#classでなくしてみる\" data-toc-modified-id=\"classでなくしてみる-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>classでなくしてみる</a></div><div class=\"lev2 toc-item\"><a href=\"#envのinit\" data-toc-modified-id=\"envのinit-91\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>envのinit</a></div><div class=\"lev3 toc-item\"><a href=\"#Agent作成\" data-toc-modified-id=\"Agent作成-911\"><span class=\"toc-item-num\">9.1.1&nbsp;&nbsp;</span>Agent作成</a></div><div class=\"lev2 toc-item\"><a href=\"#envのfor-loop\" data-toc-modified-id=\"envのfor-loop-92\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>envのfor loop</a></div><div class=\"lev3 toc-item\"><a href=\"#agent.get_actionはepsilon-greedy法により決定されたaction(この例の場合、右か左かのゼロイチ）を返す\" data-toc-modified-id=\"agent.get_actionはepsilon-greedy法により決定されたaction(この例の場合、右か左かのゼロイチ）を返す-921\"><span class=\"toc-item-num\">9.2.1&nbsp;&nbsp;</span>agent.get_actionは$\\epsilon$-greedy法により決定されたaction(この例の場合、右か左かのゼロイチ）を返す</a></div><div class=\"lev3 toc-item\"><a href=\"#actionを与えるとenvがどうなるかを計算してくれる：\" data-toc-modified-id=\"actionを与えるとenvがどうなるかを計算してくれる：-922\"><span class=\"toc-item-num\">9.2.2&nbsp;&nbsp;</span>actionを与えるとenvがどうなるかを計算してくれる：</a></div><div class=\"lev3 toc-item\"><a href=\"#終了していたら\" data-toc-modified-id=\"終了していたら-923\"><span class=\"toc-item-num\">9.2.3&nbsp;&nbsp;</span>終了していたら</a></div><div class=\"lev3 toc-item\"><a href=\"#エージェントが（状態、アクション、次のアクション、報酬）を記憶\" data-toc-modified-id=\"エージェントが（状態、アクション、次のアクション、報酬）を記憶-924\"><span class=\"toc-item-num\">9.2.4&nbsp;&nbsp;</span>エージェントが（状態、アクション、次のアクション、報酬）を記憶</a></div><div class=\"lev3 toc-item\"><a href=\"#Experience-ReplayでQ関数を更新する\" data-toc-modified-id=\"Experience-ReplayでQ関数を更新する-925\"><span class=\"toc-item-num\">9.2.5&nbsp;&nbsp;</span>Experience ReplayでQ関数を更新する</a></div><div class=\"lev2 toc-item\"><a href=\"#brain\" data-toc-modified-id=\"brain-93\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>brain</a></div><div class=\"lev3 toc-item\"><a href=\"#サンプルがたまった学習済みのbrainを拝借\" data-toc-modified-id=\"サンプルがたまった学習済みのbrainを拝借-931\"><span class=\"toc-item-num\">9.3.1&nbsp;&nbsp;</span>サンプルがたまった学習済みのbrainを拝借</a></div><div class=\"lev3 toc-item\"><a href=\"#mini-batch取得\" data-toc-modified-id=\"mini-batch取得-932\"><span class=\"toc-item-num\">9.3.2&nbsp;&nbsp;</span>mini batch取得</a></div><div class=\"lev3 toc-item\"><a href=\"#mini-batchをtorchが食える形式に変形\" data-toc-modified-id=\"mini-batchをtorchが食える形式に変形-933\"><span class=\"toc-item-num\">9.3.3&nbsp;&nbsp;</span>mini batchをtorchが食える形式に変形</a></div><div class=\"lev3 toc-item\"><a href=\"#推論モードへ切り替え\" data-toc-modified-id=\"推論モードへ切り替え-934\"><span class=\"toc-item-num\">9.3.4&nbsp;&nbsp;</span>推論モードへ切り替え</a></div><div class=\"lev3 toc-item\"><a href=\"#ネットワークが出力したQ(s_t,-a_t)を求める-(state_action_valueに格納）\" data-toc-modified-id=\"ネットワークが出力したQ(s_t,-a_t)を求める-(state_action_valueに格納）-935\"><span class=\"toc-item-num\">9.3.5&nbsp;&nbsp;</span>ネットワークが出力したQ(s_t, a_t)を求める (state_action_valueに格納）</a></div><div class=\"lev3 toc-item\"><a href=\"#教師となる行動価値関数（expected_state_action_values）を計算\" data-toc-modified-id=\"教師となる行動価値関数（expected_state_action_values）を計算-936\"><span class=\"toc-item-num\">9.3.6&nbsp;&nbsp;</span>教師となる行動価値関数（expected_state_action_values）を計算</a></div><div class=\"lev4 toc-item\"><a href=\"#doneでないnext-state:\" data-toc-modified-id=\"doneでないnext-state:-9361\"><span class=\"toc-item-num\">9.3.6.1&nbsp;&nbsp;</span>doneでないnext state:</a></div><div class=\"lev4 toc-item\"><a href=\"#next-statenにおける行動価値関数\" data-toc-modified-id=\"next-statenにおける行動価値関数-9362\"><span class=\"toc-item-num\">9.3.6.2&nbsp;&nbsp;</span>next statenにおける行動価値関数</a></div><div class=\"lev4 toc-item\"><a href=\"#next-stateにおける行動価値関数の最大値\" data-toc-modified-id=\"next-stateにおける行動価値関数の最大値-9363\"><span class=\"toc-item-num\">9.3.6.3&nbsp;&nbsp;</span>next stateにおける行動価値関数の最大値</a></div><div class=\"lev4 toc-item\"><a href=\"#教師となるQ(s_t,-a_t)値を、Q学習の式から求める\" data-toc-modified-id=\"教師となるQ(s_t,-a_t)値を、Q学習の式から求める-9364\"><span class=\"toc-item-num\">9.3.6.4&nbsp;&nbsp;</span>教師となるQ(s_t, a_t)値を、Q学習の式から求める</a></div><div class=\"lev3 toc-item\"><a href=\"#結合パラメータ更新\" data-toc-modified-id=\"結合パラメータ更新-937\"><span class=\"toc-item-num\">9.3.7&nbsp;&nbsp;</span>結合パラメータ更新</a></div><div class=\"lev4 toc-item\"><a href=\"#ネットワークを訓練モードに切り替える\" data-toc-modified-id=\"ネットワークを訓練モードに切り替える-9371\"><span class=\"toc-item-num\">9.3.7.1&nbsp;&nbsp;</span>ネットワークを訓練モードに切り替える</a></div><div class=\"lev4 toc-item\"><a href=\"#損失関数を計算する（smooth_l1_lossはHuberloss）\" data-toc-modified-id=\"損失関数を計算する（smooth_l1_lossはHuberloss）-9372\"><span class=\"toc-item-num\">9.3.7.2&nbsp;&nbsp;</span>損失関数を計算する（smooth_l1_lossはHuberloss）</a></div><div class=\"lev4 toc-item\"><a href=\"#結合パラメータを更新する\" data-toc-modified-id=\"結合パラメータを更新する-9373\"><span class=\"toc-item-num\">9.3.7.3&nbsp;&nbsp;</span>結合パラメータを更新する</a></div><div class=\"lev1 toc-item\"><a href=\"#sandbox\" data-toc-modified-id=\"sandbox-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>sandbox</a></div><div class=\"lev3 toc-item\"><a href=\"#経験を記憶するメモリオブジェクトを生成\" data-toc-modified-id=\"経験を記憶するメモリオブジェクトを生成-1001\"><span class=\"toc-item-num\">10.0.1&nbsp;&nbsp;</span>経験を記憶するメモリオブジェクトを生成</a></div><div class=\"lev3 toc-item\"><a href=\"#ニューラルネットワークを構築\" data-toc-modified-id=\"ニューラルネットワークを構築-1002\"><span class=\"toc-item-num\">10.0.2&nbsp;&nbsp;</span>ニューラルネットワークを構築</a></div><div class=\"lev3 toc-item\"><a href=\"#最適化手法の設定\" data-toc-modified-id=\"最適化手法の設定-1003\"><span class=\"toc-item-num\">10.0.3&nbsp;&nbsp;</span>最適化手法の設定</a></div><div class=\"lev3 toc-item\"><a href=\"#replay\" data-toc-modified-id=\"replay-1004\"><span class=\"toc-item-num\">10.0.4&nbsp;&nbsp;</span>replay</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "** 5.3、5.4  PyTorchでDQN **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パッケージのimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 動画の描画関数の宣言\n",
    "参考URL http://nbviewer.jupyter.org/github/patrickmineault/xcorr-notebooks/blob/master/Render%20OpenAI%20gym%20as%20GIF.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0),\n",
    "               dpi=72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames),\n",
    "                                   interval=50)\n",
    "\n",
    "    anim.save('movie_cartpole_DQN.mp4')  # 動画のファイル名と保存です\n",
    "    display(display_animation(anim, default_mode='loop'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本コードでは、namedtupleを使用します。\n",
    "\n",
    "namedtupleを使うことで、値をフィールド名とペアで格納できます。\n",
    "\n",
    "すると値に対して、フィールド名でアクセスできて便利です。\n",
    "\n",
    "https://docs.python.jp/3/library/collections.html#collections.namedtuple\n",
    "\n",
    "以下は使用例です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr(name_a='名前Aです', value_b=100)\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Tr = namedtuple('tr', ('name_a', 'value_b'))\n",
    "Tr_object = Tr('名前Aです', 100)\n",
    "\n",
    "print(Tr_object)  # 出力：tr(name_a='名前Aです', value_b=100)\n",
    "print(Tr_object.value_b)  # 出力：100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'名前Aです'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tr_object.name_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namedtupleを生成\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'next_state', 'reward'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = 'CartPole-v0'  # 使用する課題名\n",
    "GAMMA = 0.99  # 時間割引率\n",
    "MAX_STEPS = 200  # 1試行のstep数\n",
    "NUM_EPISODES = 500  # 最大試行回数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 経験を保存するメモリクラスを定義します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, CAPACITY):\n",
    "        self.capacity = CAPACITY  # メモリの最大長さ\n",
    "        self.memory = []  # 経験を保存する変数\n",
    "        self.index = 0  # 保存するindexを示す変数\n",
    "\n",
    "    def push(self, state, action, state_next, reward):\n",
    "        '''transition = (state, action, state_next, reward)をメモリに保存する'''\n",
    "\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)  # メモリが満タンでないときは足す\n",
    "\n",
    "        # namedtupleのTransitionを使用し、値とフィールド名をペアにして保存します\n",
    "        self.memory[self.index] = Transition(state, action, state_next, reward)\n",
    "\n",
    "        self.index = (self.index + 1) % self.capacity  # 保存するindexを1つずらす\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''batch_size分だけ、ランダムに保存内容を取り出す'''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''関数lenに対して、現在の変数memoryの長さを返す'''\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = ReplayMemory(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.push(1,2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.push(2,4,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain\n",
    "\n",
    "エージェントが持つ脳となるクラスです、DQNを実行します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "methodは\n",
    "\n",
    "* replay: Experience Replayでネットワークの結合パラメータを学習\n",
    "* decide_action: アクション決定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q関数をディープラーニングのネットワークをクラスとして定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "CAPACITY = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions  # CartPoleの行動（右に左に押す）の2を取得\n",
    "\n",
    "        # 経験を記憶するメモリオブジェクトを生成\n",
    "        self.memory = ReplayMemory(CAPACITY)\n",
    "\n",
    "        # ニューラルネットワークを構築\n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add_module('fc1', nn.Linear(num_states, 32))\n",
    "        self.model.add_module('relu1', nn.ReLU())\n",
    "        self.model.add_module('fc2', nn.Linear(32, 32))\n",
    "        self.model.add_module('relu2', nn.ReLU())\n",
    "        self.model.add_module('fc3', nn.Linear(32, num_actions))\n",
    "\n",
    "        print(self.model)  # ネットワークの形を出力\n",
    "\n",
    "        # 最適化手法の設定\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "\n",
    "    def replay(self):\n",
    "        '''Experience Replayでネットワークの結合パラメータを学習'''\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 1. メモリサイズの確認\n",
    "        # -----------------------------------------\n",
    "        # 1.1 メモリサイズがミニバッチより小さい間は何もしない\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2. ミニバッチの作成\n",
    "        # -----------------------------------------\n",
    "        # 2.1 メモリからミニバッチ分のデータを取り出す\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        # 2.2 各変数をミニバッチに対応する形に変形\n",
    "        # transitionsは1stepごとの(state, action, state_next, reward)が、BATCH_SIZE分格納されている\n",
    "        # つまり、(state, action, state_next, reward)×BATCH_SIZE\n",
    "        # これをミニバッチにしたい。つまり\n",
    "        # (state×BATCH_SIZE, action×BATCH_SIZE, state_next×BATCH_SIZE, reward×BATCH_SIZE)にする\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # 2.3 各変数の要素をミニバッチに対応する形に変形し、ネットワークで扱えるようVariableにする\n",
    "        # 例えばstateの場合、[torch.FloatTensor of size 1x4]がBATCH_SIZE分並んでいるのですが、\n",
    "        # それを torch.FloatTensor of size BATCH_SIZEx4 に変換します\n",
    "        # 状態、行動、報酬、non_finalの状態のミニバッチのVariableを作成\n",
    "        # catはConcatenates（結合）のことです。\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                           if s is not None])\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 3. 教師信号となるQ(s_t, a_t)値を求める\n",
    "        # -----------------------------------------\n",
    "        # 3.1 ネットワークを推論モードに切り替える\n",
    "        self.model.eval()\n",
    "\n",
    "        # 3.2 ネットワークが出力したQ(s_t, a_t)を求める\n",
    "        # self.model(state_batch)は、右左の両方のQ値を出力しており\n",
    "        # [torch.FloatTensor of size BATCH_SIZEx2]になっている。\n",
    "        # ここから実行したアクションa_tに対応するQ値を求めるため、action_batchで行った行動a_tが右か左かのindexを求め\n",
    "        # それに対応するQ値をgatherでひっぱり出す。\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # 3.3 max{Q(s_t+1, a)}値を求める。ただし次の状態があるかに注意。\n",
    "\n",
    "        # cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成\n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None,\n",
    "                                                    batch.next_state)))\n",
    "        # まずは全部0にしておく\n",
    "        next_state_values = torch.zeros(BATCH_SIZE)\n",
    "\n",
    "        # 次の状態があるindexの最大Q値を求める\n",
    "        # 出力にアクセスし、max(1)で列方向の最大値の[値、index]を求めます\n",
    "        # そしてそのQ値（index=0）を出力します\n",
    "        # detachでその値を取り出します\n",
    "        next_state_values[non_final_mask] = self.model(\n",
    "            non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "        # 3.4 教師となるQ(s_t, a_t)値を、Q学習の式から求める\n",
    "        expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 4. 結合パラメータの更新\n",
    "        # -----------------------------------------\n",
    "        # 4.1 ネットワークを訓練モードに切り替える\n",
    "        self.model.train()\n",
    "\n",
    "        # 4.2 損失関数を計算する（smooth_l1_lossはHuberloss）\n",
    "        # expected_state_action_valuesは\n",
    "        # sizeが[minbatch]になっているので、unsqueezeで[minibatch x 1]へ\n",
    "        loss = F.smooth_l1_loss(state_action_values,\n",
    "                                expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # 4.3 結合パラメータを更新する\n",
    "        self.optimizer.zero_grad()  # 勾配をリセット\n",
    "        loss.backward()  # バックプロパゲーションを計算\n",
    "        self.optimizer.step()  # 結合パラメータを更新\n",
    "\n",
    "    def decide_action(self, state, episode):\n",
    "        '''現在の状態に応じて、行動を決定する'''\n",
    "        # ε-greedy法で徐々に最適行動のみを採用する\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            self.model.eval()  # ネットワークを推論モードに切り替える\n",
    "            with torch.no_grad():\n",
    "                action = self.model(state).max(1)[1].view(1, 1)\n",
    "            # ネットワークの出力の最大値のindexを取り出します = max(1)[1]\n",
    "            # .view(1,1)は[torch.LongTensor of size 1]　を size 1x1 に変換します\n",
    "\n",
    "        else:\n",
    "            # 0,1の行動をランダムに返す\n",
    "            action = torch.LongTensor(\n",
    "                [[random.randrange(self.num_actions)]])  # 0,1の行動をランダムに返す\n",
    "            # actionは[torch.LongTensor of size 1x1]の形になります\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "CartPoleで動くエージェントクラスです、棒付き台車そのものになります\n",
    "\n",
    "methodは\n",
    "\n",
    "\n",
    "* 行動価値Qを更新\n",
    "*  状態を与えると行動を決定\n",
    "*  状態、選択するアクション、次の状態、報酬などを記憶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        '''課題の状態と行動の数を設定する'''\n",
    "        self.brain = Brain(num_states, num_actions)  # エージェントが行動を決定するための頭脳を生成\n",
    "\n",
    "    def update_q_function(self):\n",
    "        '''Q関数を更新する'''\n",
    "        self.brain.replay()\n",
    "\n",
    "    def get_action(self, state, episode):\n",
    "        '''行動を決定する'''\n",
    "        action = self.brain.decide_action(state, episode)\n",
    "        return action\n",
    "\n",
    "    def memorize(self, state, action, state_next, reward):\n",
    "        '''memoryオブジェクトに、state, action, state_next, rewardの内容を保存する'''\n",
    "        self.brain.memory.push(state, action, state_next, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "CartPoleを実行する環境のクラスです\n",
    "\n",
    "* for loopは二重になっている\n",
    "* 外側のループはEPISODEに関して\n",
    "    * Episode = 試行：一回ポールを立てて倒れるか200ステップ経過するまでを１エピソードと数える\n",
    "    * 内側のループはステップに関して\n",
    "        * 初期状態のBrainを使って、1ステップ目から左右にコントロールしていくことからスタート\n",
    "        * 各ステップごとに状態と遷移を記録する。\n",
    "        * 同様に各ステップごとに行動価値関数をアップデートしていく\n",
    "    * 10エピソード連続で200ステップまで持ちこたえられたら成功\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV)  # 実行する課題を設定\n",
    "        num_states = self.env.observation_space.shape[0]  # 課題の状態数4を取得\n",
    "        num_actions = self.env.action_space.n  # CartPoleの行動（右に左に押す）の2を取得\n",
    "        self.agent = Agent(num_states, num_actions)  # 環境内で行動するAgentを生成\n",
    "\n",
    "        \n",
    "    def run(self):\n",
    "        '''実行'''\n",
    "        episode_10_list = np.zeros(10)  # 10試行分の立ち続けたstep数を格納し、平均ステップ数を出力に利用\n",
    "        complete_episodes = 0  # 195step以上連続で立ち続けた試行数\n",
    "        episode_final = False  # 最後の試行フラグ\n",
    "        frames = []  # 最後の試行を動画にするために画像を格納する変数\n",
    "\n",
    "        for episode in range(NUM_EPISODES):  # 最大試行数分繰り返す\n",
    "            observation = self.env.reset()  # 環境の初期化\n",
    "\n",
    "            state = observation  # 観測をそのまま状態sとして使用\n",
    "            state = torch.from_numpy(state).type(\n",
    "                torch.FloatTensor)  # NumPy変数をPyTorchのテンソルに変換\n",
    "            state = torch.unsqueeze(state, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "            for step in range(MAX_STEPS):  # 1エピソードのループ\n",
    "\n",
    "                if episode_final is True:  # 最終試行ではframesに各時刻の画像を追加していく\n",
    "                    frames.append(self.env.render(mode='rgb_array'))\n",
    "\n",
    "                action = self.agent.get_action(state, episode)  # 行動を求める\n",
    "\n",
    "                # 行動a_tの実行により、s_{t+1}とdoneフラグを求める\n",
    "                # actionから.item()を指定して、中身を取り出す\n",
    "                observation_next, _, done, _ = self.env.step(\n",
    "                    action.item())  # rewardとinfoは使わないので_にする\n",
    "\n",
    "                # 報酬を与える。さらにepisodeの終了評価と、state_nextを設定する\n",
    "                if done:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\n",
    "                    state_next = None  # 次の状態はないので、Noneを格納\n",
    "\n",
    "                    # 直近10episodeの立てたstep数リストに追加\n",
    "                    episode_10_list = np.hstack(\n",
    "                        (episode_10_list[1:], step + 1))\n",
    "\n",
    "                    if step < 195:\n",
    "                        reward = torch.FloatTensor(\n",
    "                            [-1.0])  # 途中でこけたら罰則として報酬-1を与える\n",
    "                        complete_episodes = 0  # 連続成功記録をリセット\n",
    "                    else:\n",
    "                        reward = torch.FloatTensor([1.0])  # 立ったまま終了時は報酬1を与える\n",
    "                        complete_episodes = complete_episodes + 1  # 連続記録を更新\n",
    "                else:\n",
    "                    reward = torch.FloatTensor([0.0])  # 普段は報酬0\n",
    "                    state_next = observation_next  # 観測をそのまま状態とする\n",
    "                    state_next = torch.from_numpy(state_next).type(\n",
    "                        torch.FloatTensor)  # numpy変数をPyTorchのテンソルに変換\n",
    "                    state_next = torch.unsqueeze(state_next, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "                # メモリに経験を追加\n",
    "                self.agent.memorize(state, action, state_next, reward)\n",
    "\n",
    "                # Experience ReplayでQ関数を更新する\n",
    "                self.agent.update_q_function()\n",
    "\n",
    "                # 観測の更新\n",
    "                state = state_next\n",
    "\n",
    "                # 終了時の処理\n",
    "                if done:\n",
    "                    print('%d Episode: Finished after %d steps：10試行の平均step数 = %.1lf' % (\n",
    "                        episode, step + 1, episode_10_list.mean()))\n",
    "                    break\n",
    "\n",
    "            if episode_final is True:\n",
    "                # 動画を保存と描画\n",
    "                display_frames_as_gif(frames)\n",
    "                break\n",
    "\n",
    "            # 10連続で200step経ち続けたら成功\n",
    "            if complete_episodes >= 10:\n",
    "                print('10回連続成功')\n",
    "                episode_final = True  # 次の試行を描画を行う最終試行とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "0 Episode: Finished after 16 steps：10試行の平均step数 = 1.6\n",
      "1 Episode: Finished after 11 steps：10試行の平均step数 = 2.7\n",
      "2 Episode: Finished after 10 steps：10試行の平均step数 = 3.7\n",
      "3 Episode: Finished after 10 steps：10試行の平均step数 = 4.7\n",
      "4 Episode: Finished after 11 steps：10試行の平均step数 = 5.8\n",
      "5 Episode: Finished after 9 steps：10試行の平均step数 = 6.7\n",
      "6 Episode: Finished after 10 steps：10試行の平均step数 = 7.7\n",
      "7 Episode: Finished after 10 steps：10試行の平均step数 = 8.7\n",
      "8 Episode: Finished after 8 steps：10試行の平均step数 = 9.5\n",
      "9 Episode: Finished after 10 steps：10試行の平均step数 = 10.5\n",
      "10 Episode: Finished after 10 steps：10試行の平均step数 = 9.9\n",
      "11 Episode: Finished after 10 steps：10試行の平均step数 = 9.8\n",
      "12 Episode: Finished after 8 steps：10試行の平均step数 = 9.6\n",
      "13 Episode: Finished after 9 steps：10試行の平均step数 = 9.5\n",
      "14 Episode: Finished after 13 steps：10試行の平均step数 = 9.7\n",
      "15 Episode: Finished after 11 steps：10試行の平均step数 = 9.9\n",
      "16 Episode: Finished after 17 steps：10試行の平均step数 = 10.6\n",
      "17 Episode: Finished after 11 steps：10試行の平均step数 = 10.7\n",
      "18 Episode: Finished after 15 steps：10試行の平均step数 = 11.4\n",
      "19 Episode: Finished after 16 steps：10試行の平均step数 = 12.0\n",
      "20 Episode: Finished after 16 steps：10試行の平均step数 = 12.6\n",
      "21 Episode: Finished after 16 steps：10試行の平均step数 = 13.2\n",
      "22 Episode: Finished after 16 steps：10試行の平均step数 = 14.0\n",
      "23 Episode: Finished after 15 steps：10試行の平均step数 = 14.6\n",
      "24 Episode: Finished after 16 steps：10試行の平均step数 = 14.9\n",
      "25 Episode: Finished after 26 steps：10試行の平均step数 = 16.4\n",
      "26 Episode: Finished after 10 steps：10試行の平均step数 = 15.7\n",
      "27 Episode: Finished after 60 steps：10試行の平均step数 = 20.6\n",
      "28 Episode: Finished after 46 steps：10試行の平均step数 = 23.7\n",
      "29 Episode: Finished after 16 steps：10試行の平均step数 = 23.7\n",
      "30 Episode: Finished after 28 steps：10試行の平均step数 = 24.9\n",
      "31 Episode: Finished after 60 steps：10試行の平均step数 = 29.3\n",
      "32 Episode: Finished after 44 steps：10試行の平均step数 = 32.1\n",
      "33 Episode: Finished after 127 steps：10試行の平均step数 = 43.3\n",
      "34 Episode: Finished after 31 steps：10試行の平均step数 = 44.8\n",
      "35 Episode: Finished after 39 steps：10試行の平均step数 = 46.1\n",
      "36 Episode: Finished after 29 steps：10試行の平均step数 = 48.0\n",
      "37 Episode: Finished after 102 steps：10試行の平均step数 = 52.2\n",
      "38 Episode: Finished after 42 steps：10試行の平均step数 = 51.8\n",
      "39 Episode: Finished after 47 steps：10試行の平均step数 = 54.9\n",
      "40 Episode: Finished after 44 steps：10試行の平均step数 = 56.5\n",
      "41 Episode: Finished after 52 steps：10試行の平均step数 = 55.7\n",
      "42 Episode: Finished after 47 steps：10試行の平均step数 = 56.0\n",
      "43 Episode: Finished after 71 steps：10試行の平均step数 = 50.4\n",
      "44 Episode: Finished after 59 steps：10試行の平均step数 = 53.2\n",
      "45 Episode: Finished after 92 steps：10試行の平均step数 = 58.5\n",
      "46 Episode: Finished after 44 steps：10試行の平均step数 = 60.0\n",
      "47 Episode: Finished after 71 steps：10試行の平均step数 = 56.9\n",
      "48 Episode: Finished after 65 steps：10試行の平均step数 = 59.2\n",
      "49 Episode: Finished after 52 steps：10試行の平均step数 = 59.7\n",
      "50 Episode: Finished after 53 steps：10試行の平均step数 = 60.6\n",
      "51 Episode: Finished after 42 steps：10試行の平均step数 = 59.6\n",
      "52 Episode: Finished after 100 steps：10試行の平均step数 = 64.9\n",
      "53 Episode: Finished after 90 steps：10試行の平均step数 = 66.8\n",
      "54 Episode: Finished after 65 steps：10試行の平均step数 = 67.4\n",
      "55 Episode: Finished after 109 steps：10試行の平均step数 = 69.1\n",
      "56 Episode: Finished after 64 steps：10試行の平均step数 = 71.1\n",
      "57 Episode: Finished after 82 steps：10試行の平均step数 = 72.2\n",
      "58 Episode: Finished after 53 steps：10試行の平均step数 = 71.0\n",
      "59 Episode: Finished after 58 steps：10試行の平均step数 = 71.6\n",
      "60 Episode: Finished after 49 steps：10試行の平均step数 = 71.2\n",
      "61 Episode: Finished after 75 steps：10試行の平均step数 = 74.5\n",
      "62 Episode: Finished after 79 steps：10試行の平均step数 = 72.4\n",
      "63 Episode: Finished after 54 steps：10試行の平均step数 = 68.8\n",
      "64 Episode: Finished after 71 steps：10試行の平均step数 = 69.4\n",
      "65 Episode: Finished after 154 steps：10試行の平均step数 = 73.9\n",
      "66 Episode: Finished after 93 steps：10試行の平均step数 = 76.8\n",
      "67 Episode: Finished after 70 steps：10試行の平均step数 = 75.6\n",
      "68 Episode: Finished after 73 steps：10試行の平均step数 = 77.6\n",
      "69 Episode: Finished after 200 steps：10試行の平均step数 = 91.8\n",
      "70 Episode: Finished after 132 steps：10試行の平均step数 = 100.1\n",
      "71 Episode: Finished after 80 steps：10試行の平均step数 = 100.6\n",
      "72 Episode: Finished after 69 steps：10試行の平均step数 = 99.6\n",
      "73 Episode: Finished after 58 steps：10試行の平均step数 = 100.0\n",
      "74 Episode: Finished after 138 steps：10試行の平均step数 = 106.7\n",
      "75 Episode: Finished after 72 steps：10試行の平均step数 = 98.5\n",
      "76 Episode: Finished after 80 steps：10試行の平均step数 = 97.2\n",
      "77 Episode: Finished after 54 steps：10試行の平均step数 = 95.6\n",
      "78 Episode: Finished after 92 steps：10試行の平均step数 = 97.5\n",
      "79 Episode: Finished after 113 steps：10試行の平均step数 = 88.8\n",
      "80 Episode: Finished after 74 steps：10試行の平均step数 = 83.0\n",
      "81 Episode: Finished after 99 steps：10試行の平均step数 = 84.9\n",
      "82 Episode: Finished after 104 steps：10試行の平均step数 = 88.4\n",
      "83 Episode: Finished after 105 steps：10試行の平均step数 = 93.1\n",
      "84 Episode: Finished after 200 steps：10試行の平均step数 = 99.3\n",
      "85 Episode: Finished after 200 steps：10試行の平均step数 = 112.1\n",
      "86 Episode: Finished after 185 steps：10試行の平均step数 = 122.6\n",
      "87 Episode: Finished after 200 steps：10試行の平均step数 = 137.2\n",
      "88 Episode: Finished after 111 steps：10試行の平均step数 = 139.1\n",
      "89 Episode: Finished after 81 steps：10試行の平均step数 = 135.9\n",
      "90 Episode: Finished after 133 steps：10試行の平均step数 = 141.8\n",
      "91 Episode: Finished after 169 steps：10試行の平均step数 = 148.8\n",
      "92 Episode: Finished after 153 steps：10試行の平均step数 = 153.7\n",
      "93 Episode: Finished after 200 steps：10試行の平均step数 = 163.2\n",
      "94 Episode: Finished after 200 steps：10試行の平均step数 = 163.2\n",
      "95 Episode: Finished after 200 steps：10試行の平均step数 = 163.2\n",
      "96 Episode: Finished after 200 steps：10試行の平均step数 = 164.7\n",
      "97 Episode: Finished after 200 steps：10試行の平均step数 = 164.7\n",
      "98 Episode: Finished after 200 steps：10試行の平均step数 = 173.6\n",
      "99 Episode: Finished after 200 steps：10試行の平均step数 = 185.5\n",
      "100 Episode: Finished after 200 steps：10試行の平均step数 = 192.2\n",
      "101 Episode: Finished after 178 steps：10試行の平均step数 = 193.1\n",
      "102 Episode: Finished after 200 steps：10試行の平均step数 = 197.8\n",
      "103 Episode: Finished after 200 steps：10試行の平均step数 = 197.8\n",
      "104 Episode: Finished after 200 steps：10試行の平均step数 = 197.8\n",
      "105 Episode: Finished after 200 steps：10試行の平均step数 = 197.8\n",
      "106 Episode: Finished after 200 steps：10試行の平均step数 = 197.8\n",
      "107 Episode: Finished after 200 steps：10試行の平均step数 = 197.8\n",
      "108 Episode: Finished after 200 steps：10試行の平均step数 = 197.8\n",
      "109 Episode: Finished after 200 steps：10試行の平均step数 = 197.8\n",
      "110 Episode: Finished after 200 steps：10試行の平均step数 = 197.8\n",
      "111 Episode: Finished after 200 steps：10試行の平均step数 = 200.0\n",
      "10回連続成功\n"
     ]
    },
    {
     "ename": "NoSuchDisplayException",
     "evalue": "Cannot connect to \"None\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4bf7f6ad7ab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# main クラス\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcartpole_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcartpole_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-5ad9ba56351d>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mepisode_final\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 最終試行ではframesに各時刻の画像を追加していく\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 行動を求める\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error occured while running `from pyglet.gl import *`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1814\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_epydoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1815\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1816\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_platform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36mget_default_display\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1763\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDisplay\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         '''\n\u001b[0;32m-> 1765\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_epydoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pyglet/canvas/__init__.py\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_epydoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pyglet/canvas/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""
     ]
    }
   ],
   "source": [
    "# main クラス\n",
    "cartpole_env = Environment()\n",
    "cartpole_env.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多少改造したenvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEnvironment:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV)  # 実行する課題を設定\n",
    "        num_states = self.env.observation_space.shape[0]  # 課題の状態数4を取得\n",
    "        num_actions = self.env.action_space.n  # CartPoleの行動（右に左に押す）の2を取得\n",
    "        self.agent = Agent(num_states, num_actions)  # 環境内で行動するAgentを生成\n",
    "        self.frames = []\n",
    "\n",
    "        \n",
    "    def run(self):\n",
    "        '''実行'''\n",
    "        episode_10_list = np.zeros(10)  # 10試行分の立ち続けたstep数を格納し、平均ステップ数を出力に利用\n",
    "        complete_episodes = 0  # 195step以上連続で立ち続けた試行数\n",
    "        episode_final = False  # 最後の試行フラグ\n",
    "        self.frames = []  # 最後の試行を動画にするために画像を格納する変数\n",
    "\n",
    "        for episode in range(NUM_EPISODES):  # 最大試行数分繰り返す\n",
    "            observation = self.env.reset()  # 環境の初期化\n",
    "\n",
    "            state = observation  # 観測をそのまま状態sとして使用\n",
    "            state = torch.from_numpy(state).type(\n",
    "                torch.FloatTensor)  # NumPy変数をPyTorchのテンソルに変換\n",
    "            state = torch.unsqueeze(state, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "            for step in range(MAX_STEPS):  # 1エピソードのループ\n",
    "\n",
    "                if episode_final is True:  # 最終試行ではframesに各時刻の画像を追加していく\n",
    "                    self.frames.append(self.env.render(mode='rgb_array'))\n",
    "\n",
    "                action = self.agent.get_action(state, episode)  # 行動を求める\n",
    "\n",
    "                # 行動a_tの実行により、s_{t+1}とdoneフラグを求める\n",
    "                # actionから.item()を指定して、中身を取り出す\n",
    "                observation_next, _, done, _ = self.env.step(\n",
    "                    action.item())  # rewardとinfoは使わないので_にする\n",
    "\n",
    "                # 報酬を与える。さらにepisodeの終了評価と、state_nextを設定する\n",
    "                if done:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\n",
    "                    state_next = None  # 次の状態はないので、Noneを格納\n",
    "\n",
    "                    # 直近10episodeの立てたstep数リストに追加\n",
    "                    episode_10_list = np.hstack(\n",
    "                        (episode_10_list[1:], step + 1))\n",
    "\n",
    "                    if step < 195:\n",
    "                        reward = torch.FloatTensor(\n",
    "                            [-1.0])  # 途中でこけたら罰則として報酬-1を与える\n",
    "                        complete_episodes = 0  # 連続成功記録をリセット\n",
    "                    else:\n",
    "                        reward = torch.FloatTensor([1.0])  # 立ったまま終了時は報酬1を与える\n",
    "                        complete_episodes = complete_episodes + 1  # 連続記録を更新\n",
    "                else:\n",
    "                    reward = torch.FloatTensor([0.0])  # 普段は報酬0\n",
    "                    state_next = observation_next  # 観測をそのまま状態とする\n",
    "                    state_next = torch.from_numpy(state_next).type(\n",
    "                        torch.FloatTensor)  # numpy変数をPyTorchのテンソルに変換\n",
    "                    state_next = torch.unsqueeze(state_next, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "                # メモリに経験を追加\n",
    "                self.agent.memorize(state, action, state_next, reward)\n",
    "\n",
    "                # Experience ReplayでQ関数を更新する\n",
    "                self.agent.update_q_function()\n",
    "\n",
    "                # 観測の更新\n",
    "                state = state_next\n",
    "\n",
    "                # 終了時の処理\n",
    "                if done:\n",
    "                    print('%d Episode: Finished after %d steps：10試行の平均step数 = %.1lf' % (\n",
    "                        episode, step + 1, episode_10_list.mean()))\n",
    "                    break\n",
    "\n",
    "            if episode_final is True:\n",
    "                # 動画を保存と描画\n",
    "                print(\"length of the final frame is {}\".format(len(frames)))\n",
    "                display_frames_as_gif(self.frames)\n",
    "                break\n",
    "\n",
    "            # 10連続で200step経ち続けたら成功\n",
    "            if complete_episodes >= 10:\n",
    "                print('10回連続成功')\n",
    "                episode_final = True  # 次の試行を描画を行う最終試行とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main クラス\n",
    "cartpole_env = myEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_env.env.render(mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classでなくしてみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## envのinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "CAPACITY = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_env = gym.make(ENV)  # 実行する課題を設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = my_env.observation_space.shape[0]  # 課題の状態数4を取得\n",
    "num_actions = my_env.action_space.n  # CartPoleの行動（右に左に押す）の2を取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_agent = Agent(num_states, num_actions)  # 環境内で行動するAgentを生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_10_list = np.zeros(10)  # 10試行分の立ち続けたstep数を格納し、平均ステップ数を出力に利用\n",
    "complete_episodes = 0  # 195step以上連続で立ち続けた試行数\n",
    "episode_final = False  # 最後の試行フラグ\n",
    "frames = []  # 最後の試行を動画にするために画像を格納する変数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## envのfor loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = my_env.reset()  # 環境の初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = observation  # 観測をそのまま状態sとして使用\n",
    "state = torch.from_numpy(state).type(\n",
    "    torch.FloatTensor)  # NumPy変数をPyTorchのテンソルに変換\n",
    "state = torch.unsqueeze(state, 0)  # size 4をsize 1x4に変換\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for step in range(MAX_STEPS):  # 1エピソードのループ\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if episode_final is True:  # 最終試行ではframesに各時刻の画像を追加していく\n",
    "    frames.append(self.env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### agent.get_actionは$\\epsilon$-greedy法により決定されたaction(この例の場合、右か左かのゼロイチ）を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = my_agent.get_action(state, episode)  # 行動を求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actionを与えるとenvがどうなるかを計算してくれる："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行動a_tの実行により、s_{t+1}とdoneフラグを求める\n",
    "# actionから.item()を指定して、中身を取り出す\n",
    "observation_next, _, done, _ = my_env.step(\n",
    "    action.item())  # rewardとinfoは使わないので_にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 終了していたら\n",
    "\n",
    "* episode_10_list の先頭を削除して、末尾に「今回は何ステップ続いたか」を追加\n",
    "* 195ステップ未満だった場合、報酬-1を付与\n",
    "* 195ステップ以上だったら報酬＋１を付与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 報酬を与える。さらにepisodeの終了評価と、state_nextを設定する\n",
    "if done:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\n",
    "    state_next = None  # 次の状態はないので、Noneを格納\n",
    "\n",
    "    # 直近10episodeの立てたstep数リストに追加\n",
    "    episode_10_list = np.hstack(\n",
    "        (episode_10_list[1:], step + 1))\n",
    "\n",
    "    if step < 195:\n",
    "        reward = torch.FloatTensor(\n",
    "            [-1.0])  # 途中でこけたら罰則として報酬-1を与える\n",
    "        complete_episodes = 0  # 連続成功記録をリセット\n",
    "    else:\n",
    "        reward = torch.FloatTensor([1.0])  # 立ったまま終了時は報酬1を与える\n",
    "        complete_episodes = complete_episodes + 1  # 連続記録を更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not done:\n",
    "    reward = torch.FloatTensor([0.0])  # 普段は報酬0\n",
    "    state_next = observation_next  # 観測をそのまま状態とする\n",
    "    state_next = torch.from_numpy(state_next).type(\n",
    "        torch.FloatTensor)  # numpy変数をPyTorchのテンソルに変換\n",
    "    state_next = torch.unsqueeze(state_next, 0)  # size 4をsize 1x4に変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エージェントが（状態、アクション、次のアクション、報酬）を記憶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メモリに経験を追加\n",
    "my_agent.memorize(state, action, state_next, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience ReplayでQ関数を更新する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_agent.update_q_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 観測の更新\n",
    "state = state_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 終了時の処理\n",
    "if done:\n",
    "    print('%d Episode: Finished after %d steps：10試行の平均step数 = %.1lf' % (\n",
    "        episode, step + 1, episode_10_list.mean()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプルがたまった学習済みのbrainを拝借"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_brain = cartpole_env.agent.brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mini batch取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = my_brain.memory.sample(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mini batchをtorchが食える形式に変形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 各変数をミニバッチに対応する形に変形\n",
    "* transitionsは1stepごとの\n",
    "$$\\text{(state, action, state_next, reward)}$$  \n",
    "  が、BATCH_SIZE分格納されている\n",
    "  \n",
    "* つまり、\n",
    "  $$\\text{(state, action, state_next, reward)}\\times\\text{BATCH_SIZE}$$\n",
    "  \n",
    "* これをミニバッチにしたい。つまり\n",
    "    $$(\\text{state}\\times\\text{BATCH_SIZE}, ~\\text{action}\\times\\text{BATCH_SIZE}, ~\\text{state_next}\\times\\text{BATCH_SIZE},~ \\text{reward}\\times\\text{BATCH_SIZE})$$\n",
    "  にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = Transition(*zip(*transitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch[0][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.state[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 各変数の要素をミニバッチに対応する形に変形し、ネットワークで扱えるようVariableにする\n",
    "*  例えばstateの場合、[torch.FloatTensor of size 1x4]がBATCH_SIZE分並んでいる\n",
    "* それを torch.FloatTensor of size BATCH_SIZEx4 に変換します\n",
    "* 状態、行動、報酬、non_finalの状態のミニバッチのVariableを作成\n",
    "*  catはConcatenates（結合）のことです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_batch = torch.cat(batch.state)\n",
    "action_batch = torch.cat(batch.action)\n",
    "reward_batch = torch.cat(batch.reward)\n",
    "non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                   if s is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_batch.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論モードへ切り替え"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# 3. 教師信号となるQ(s_t, a_t)値を求める\n",
    "# -----------------------------------------\n",
    "# 3.1 ネットワークを推論モードに切り替える\n",
    "my_brain.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ネットワークが出力したQ(s_t, a_t)を求める (state_action_valueに格納）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ネットワークが出力したQ(s_t, a_t)を求める\n",
    "* self.model(state_batch)は、右左の両方のQ値を出力しており[torch.FloatTensor of size BATCH_SIZEx2]になっている。\n",
    "* ここから実行したアクションa_tに対応するQ値を求めるため、action_batchで行った行動a_tが右か左かのindexを求め、それに対応するQ値をgatherでひっぱり出す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_values = my_brain.model(state_batch).gather(1, action_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_action_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 教師となる行動価値関数（expected_state_action_values）を計算\n",
    "cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# まずは全部0にしておく\n",
    "next_state_values = torch.zeros(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### doneでないnext state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_final_next_states[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### next statenにおける行動価値関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_brain.model(non_final_next_states)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### next stateにおける行動価値関数の最大値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_brain.model(non_final_next_states).max(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように最大値と最大値を返すインデックスが返る    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(my_brain.model(non_final_next_states).max(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 次の状態があるindexの最大Q値を求める\n",
    "# 出力にアクセスし、max(1)で列方向の最大値の[値、index]を求めます\n",
    "# そしてそのQ値（index=0）を出力します\n",
    "# detachでその値を取り出します\n",
    "next_state_values[non_final_mask] = my_brain.model(\n",
    "    non_final_next_states).max(1)[0].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\uparrow$ next stateがdoneでないsampleに関してはnext stateにおける行動価値関数のmax(最適行動をとったときの値)が、next stateがdoneならゼロが格納されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "git commit -a -m \"checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 教師となるQ(s_t, a_t)値を、Q学習の式から求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_state_action_values = reward_batch + GAMMA * next_state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結合パラメータ更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ネットワークを訓練モードに切り替える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_brain.model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 損失関数を計算する（smooth_l1_lossはHuberloss）\n",
    "expected_state_action_valuesは\n",
    "sizeが[minbatch]になっているので、unsqueezeで[minibatch x 1]へ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.smooth_l1_loss(state_action_values,\n",
    "                        expected_state_action_values.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(state_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(expected_state_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 結合パラメータを更新する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_brain.optimizer.zero_grad()  # 勾配をリセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()  # バックプロパゲーションを計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizerはlearning rateを指定済みのAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_brain.optimizer.step()  # 結合パラメータを更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_brain.model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 経験を記憶するメモリオブジェクトを生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_memory = ReplayMemory(CAPACITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master e6ad660] checkpoint\n",
      " 1 file changed, 11 insertions(+), 2 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git commit -a -m \"checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラルネットワークを構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = nn.Sequential()\n",
    "my_model.add_module('fc1', nn.Linear(num_states, 32))\n",
    "my_model.add_module('relu1', nn.ReLU())\n",
    "my_model.add_module('fc2', nn.Linear(32, 32))\n",
    "my_model.add_module('relu2', nn.ReLU())\n",
    "my_model.add_module('fc3', nn.Linear(32, num_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最適化手法の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_optimizer = optim.Adam(my_model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "209px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "695px",
    "left": "1.98485px",
    "right": "1669.6px",
    "top": "66.9792px",
    "width": "193px"
   },
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
