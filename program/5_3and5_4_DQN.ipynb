{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#パッケージのimport\" data-toc-modified-id=\"パッケージのimport-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>パッケージのimport</a></div><div class=\"lev1 toc-item\"><a href=\"#動画の描画関数の宣言\" data-toc-modified-id=\"動画の描画関数の宣言-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>動画の描画関数の宣言</a></div><div class=\"lev1 toc-item\"><a href=\"#namedtuple\" data-toc-modified-id=\"namedtuple-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>namedtuple</a></div><div class=\"lev1 toc-item\"><a href=\"#定数の設定\" data-toc-modified-id=\"定数の設定-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>定数の設定</a></div><div class=\"lev1 toc-item\"><a href=\"#経験を保存するメモリクラスを定義します\" data-toc-modified-id=\"経験を保存するメモリクラスを定義します-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>経験を保存するメモリクラスを定義します</a></div><div class=\"lev2 toc-item\"><a href=\"#sandbox\" data-toc-modified-id=\"sandbox-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>sandbox</a></div><div class=\"lev1 toc-item\"><a href=\"#Brain\" data-toc-modified-id=\"Brain-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Brain</a></div><div class=\"lev2 toc-item\"><a href=\"#brain-解説\" data-toc-modified-id=\"brain-解説-61\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>brain 解説</a></div><div class=\"lev3 toc-item\"><a href=\"#initialization\" data-toc-modified-id=\"initialization-611\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>initialization</a></div><div class=\"lev3 toc-item\"><a href=\"#replay\" data-toc-modified-id=\"replay-612\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;</span>replay</a></div><div class=\"lev4 toc-item\"><a href=\"#メモリサイズの確認\" data-toc-modified-id=\"メモリサイズの確認-6121\"><span class=\"toc-item-num\">6.1.2.1&nbsp;&nbsp;</span>メモリサイズの確認</a></div><div class=\"lev4 toc-item\"><a href=\"#ミニバッチの作成\" data-toc-modified-id=\"ミニバッチの作成-6122\"><span class=\"toc-item-num\">6.1.2.2&nbsp;&nbsp;</span>ミニバッチの作成</a></div><div class=\"lev4 toc-item\"><a href=\"#教師信号となるQ(s_t,-a_t)値を求める\" data-toc-modified-id=\"教師信号となるQ(s_t,-a_t)値を求める-6123\"><span class=\"toc-item-num\">6.1.2.3&nbsp;&nbsp;</span>教師信号となるQ(s_t, a_t)値を求める</a></div><div class=\"lev3 toc-item\"><a href=\"#decide_action\" data-toc-modified-id=\"decide_action-613\"><span class=\"toc-item-num\">6.1.3&nbsp;&nbsp;</span>decide_action</a></div><div class=\"lev2 toc-item\"><a href=\"#解説終わり\" data-toc-modified-id=\"解説終わり-62\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>解説終わり</a></div><div class=\"lev1 toc-item\"><a href=\"#Agent\" data-toc-modified-id=\"Agent-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Agent</a></div><div class=\"lev1 toc-item\"><a href=\"#CartPoleを実行する環境のクラスです\" data-toc-modified-id=\"CartPoleを実行する環境のクラスです-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>CartPoleを実行する環境のクラスです</a></div><div class=\"lev1 toc-item\"><a href=\"#classでなくしてみる\" data-toc-modified-id=\"classでなくしてみる-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>classでなくしてみる</a></div><div class=\"lev2 toc-item\"><a href=\"#envのinit\" data-toc-modified-id=\"envのinit-91\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>envのinit</a></div><div class=\"lev3 toc-item\"><a href=\"#Agent作成\" data-toc-modified-id=\"Agent作成-911\"><span class=\"toc-item-num\">9.1.1&nbsp;&nbsp;</span>Agent作成</a></div><div class=\"lev2 toc-item\"><a href=\"#envのfor-loop\" data-toc-modified-id=\"envのfor-loop-92\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>envのfor loop</a></div><div class=\"lev3 toc-item\"><a href=\"#agent.get_actionはepsilon-greedy法により決定されたaction(この例の場合、右か左かのゼロイチ）を返す\" data-toc-modified-id=\"agent.get_actionはepsilon-greedy法により決定されたaction(この例の場合、右か左かのゼロイチ）を返す-921\"><span class=\"toc-item-num\">9.2.1&nbsp;&nbsp;</span>agent.get_actionは$\\epsilon$-greedy法により決定されたaction(この例の場合、右か左かのゼロイチ）を返す</a></div><div class=\"lev3 toc-item\"><a href=\"#actionを与えるとenvがどうなるかを計算してくれる：\" data-toc-modified-id=\"actionを与えるとenvがどうなるかを計算してくれる：-922\"><span class=\"toc-item-num\">9.2.2&nbsp;&nbsp;</span>actionを与えるとenvがどうなるかを計算してくれる：</a></div><div class=\"lev3 toc-item\"><a href=\"#終了していたら\" data-toc-modified-id=\"終了していたら-923\"><span class=\"toc-item-num\">9.2.3&nbsp;&nbsp;</span>終了していたら</a></div><div class=\"lev3 toc-item\"><a href=\"#エージェントが（状態、アクション、次のアクション、報酬）を記憶\" data-toc-modified-id=\"エージェントが（状態、アクション、次のアクション、報酬）を記憶-924\"><span class=\"toc-item-num\">9.2.4&nbsp;&nbsp;</span>エージェントが（状態、アクション、次のアクション、報酬）を記憶</a></div><div class=\"lev3 toc-item\"><a href=\"#Experience-ReplayでQ関数を更新する\" data-toc-modified-id=\"Experience-ReplayでQ関数を更新する-925\"><span class=\"toc-item-num\">9.2.5&nbsp;&nbsp;</span>Experience ReplayでQ関数を更新する</a></div><div class=\"lev2 toc-item\"><a href=\"#brain\" data-toc-modified-id=\"brain-93\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>brain</a></div><div class=\"lev3 toc-item\"><a href=\"#サンプルがたまった学習済みのbrainを拝借\" data-toc-modified-id=\"サンプルがたまった学習済みのbrainを拝借-931\"><span class=\"toc-item-num\">9.3.1&nbsp;&nbsp;</span>サンプルがたまった学習済みのbrainを拝借</a></div><div class=\"lev3 toc-item\"><a href=\"#mini-batch取得\" data-toc-modified-id=\"mini-batch取得-932\"><span class=\"toc-item-num\">9.3.2&nbsp;&nbsp;</span>mini batch取得</a></div><div class=\"lev3 toc-item\"><a href=\"#mini-batchをtorchが食える形式に変形\" data-toc-modified-id=\"mini-batchをtorchが食える形式に変形-933\"><span class=\"toc-item-num\">9.3.3&nbsp;&nbsp;</span>mini batchをtorchが食える形式に変形</a></div><div class=\"lev3 toc-item\"><a href=\"#推論モードへ切り替え\" data-toc-modified-id=\"推論モードへ切り替え-934\"><span class=\"toc-item-num\">9.3.4&nbsp;&nbsp;</span>推論モードへ切り替え</a></div><div class=\"lev3 toc-item\"><a href=\"#ネットワークが出力したQ(s_t,-a_t)を求める\" data-toc-modified-id=\"ネットワークが出力したQ(s_t,-a_t)を求める-935\"><span class=\"toc-item-num\">9.3.5&nbsp;&nbsp;</span>ネットワークが出力したQ(s_t, a_t)を求める</a></div><div class=\"lev3 toc-item\"><a href=\"#cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成\" data-toc-modified-id=\"cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成-936\"><span class=\"toc-item-num\">9.3.6&nbsp;&nbsp;</span>cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成</a></div><div class=\"lev4 toc-item\"><a href=\"#doneでないnext-state:\" data-toc-modified-id=\"doneでないnext-state:-9361\"><span class=\"toc-item-num\">9.3.6.1&nbsp;&nbsp;</span>doneでないnext state:</a></div><div class=\"lev4 toc-item\"><a href=\"#next-statenにおける行動価値関数\" data-toc-modified-id=\"next-statenにおける行動価値関数-9362\"><span class=\"toc-item-num\">9.3.6.2&nbsp;&nbsp;</span>next statenにおける行動価値関数</a></div><div class=\"lev4 toc-item\"><a href=\"#next-stateにおける行動価値関数の最大値\" data-toc-modified-id=\"next-stateにおける行動価値関数の最大値-9363\"><span class=\"toc-item-num\">9.3.6.3&nbsp;&nbsp;</span>next stateにおける行動価値関数の最大値</a></div><div class=\"lev1 toc-item\"><a href=\"#sandbox\" data-toc-modified-id=\"sandbox-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>sandbox</a></div><div class=\"lev3 toc-item\"><a href=\"#経験を記憶するメモリオブジェクトを生成\" data-toc-modified-id=\"経験を記憶するメモリオブジェクトを生成-1001\"><span class=\"toc-item-num\">10.0.1&nbsp;&nbsp;</span>経験を記憶するメモリオブジェクトを生成</a></div><div class=\"lev3 toc-item\"><a href=\"#ニューラルネットワークを構築\" data-toc-modified-id=\"ニューラルネットワークを構築-1002\"><span class=\"toc-item-num\">10.0.2&nbsp;&nbsp;</span>ニューラルネットワークを構築</a></div><div class=\"lev3 toc-item\"><a href=\"#最適化手法の設定\" data-toc-modified-id=\"最適化手法の設定-1003\"><span class=\"toc-item-num\">10.0.3&nbsp;&nbsp;</span>最適化手法の設定</a></div><div class=\"lev3 toc-item\"><a href=\"#replay\" data-toc-modified-id=\"replay-1004\"><span class=\"toc-item-num\">10.0.4&nbsp;&nbsp;</span>replay</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "** 5.3、5.4  PyTorchでDQN **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パッケージのimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 動画の描画関数の宣言\n",
    "参考URL http://nbviewer.jupyter.org/github/patrickmineault/xcorr-notebooks/blob/master/Render%20OpenAI%20gym%20as%20GIF.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0),\n",
    "               dpi=72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames),\n",
    "                                   interval=50)\n",
    "\n",
    "    anim.save('movie_cartpole_DQN.mp4')  # 動画のファイル名と保存です\n",
    "    display(display_animation(anim, default_mode='loop'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本コードでは、namedtupleを使用します。\n",
    "\n",
    "namedtupleを使うことで、値をフィールド名とペアで格納できます。\n",
    "\n",
    "すると値に対して、フィールド名でアクセスできて便利です。\n",
    "\n",
    "https://docs.python.jp/3/library/collections.html#collections.namedtuple\n",
    "\n",
    "以下は使用例です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr(name_a='名前Aです', value_b=100)\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Tr = namedtuple('tr', ('name_a', 'value_b'))\n",
    "Tr_object = Tr('名前Aです', 100)\n",
    "\n",
    "print(Tr_object)  # 出力：tr(name_a='名前Aです', value_b=100)\n",
    "print(Tr_object.value_b)  # 出力：100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'名前Aです'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tr_object.name_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namedtupleを生成\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'next_state', 'reward'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = 'CartPole-v0'  # 使用する課題名\n",
    "GAMMA = 0.99  # 時間割引率\n",
    "MAX_STEPS = 200  # 1試行のstep数\n",
    "NUM_EPISODES = 500  # 最大試行回数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 経験を保存するメモリクラスを定義します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, CAPACITY):\n",
    "        self.capacity = CAPACITY  # メモリの最大長さ\n",
    "        self.memory = []  # 経験を保存する変数\n",
    "        self.index = 0  # 保存するindexを示す変数\n",
    "\n",
    "    def push(self, state, action, state_next, reward):\n",
    "        '''transition = (state, action, state_next, reward)をメモリに保存する'''\n",
    "\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)  # メモリが満タンでないときは足す\n",
    "\n",
    "        # namedtupleのTransitionを使用し、値とフィールド名をペアにして保存します\n",
    "        self.memory[self.index] = Transition(state, action, state_next, reward)\n",
    "\n",
    "        self.index = (self.index + 1) % self.capacity  # 保存するindexを1つずらす\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''batch_size分だけ、ランダムに保存内容を取り出す'''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''関数lenに対して、現在の変数memoryの長さを返す'''\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = ReplayMemory(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.push(1,2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.push(2,4,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Transition(state=1, action=2, next_state=3, reward=4),\n",
       " Transition(state=2, action=4, next_state=3, reward=1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain\n",
    "\n",
    "エージェントが持つ脳となるクラスです、DQNを実行します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "methodは\n",
    "\n",
    "* replay: Experience Replayでネットワークの結合パラメータを学習\n",
    "* decide_action: アクション決定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q関数をディープラーニングのネットワークをクラスとして定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "CAPACITY = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions  # CartPoleの行動（右に左に押す）の2を取得\n",
    "\n",
    "        # 経験を記憶するメモリオブジェクトを生成\n",
    "        self.memory = ReplayMemory(CAPACITY)\n",
    "\n",
    "        # ニューラルネットワークを構築\n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add_module('fc1', nn.Linear(num_states, 32))\n",
    "        self.model.add_module('relu1', nn.ReLU())\n",
    "        self.model.add_module('fc2', nn.Linear(32, 32))\n",
    "        self.model.add_module('relu2', nn.ReLU())\n",
    "        self.model.add_module('fc3', nn.Linear(32, num_actions))\n",
    "\n",
    "        print(self.model)  # ネットワークの形を出力\n",
    "\n",
    "        # 最適化手法の設定\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "\n",
    "    def replay(self):\n",
    "        '''Experience Replayでネットワークの結合パラメータを学習'''\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 1. メモリサイズの確認\n",
    "        # -----------------------------------------\n",
    "        # 1.1 メモリサイズがミニバッチより小さい間は何もしない\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2. ミニバッチの作成\n",
    "        # -----------------------------------------\n",
    "        # 2.1 メモリからミニバッチ分のデータを取り出す\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        # 2.2 各変数をミニバッチに対応する形に変形\n",
    "        # transitionsは1stepごとの(state, action, state_next, reward)が、BATCH_SIZE分格納されている\n",
    "        # つまり、(state, action, state_next, reward)×BATCH_SIZE\n",
    "        # これをミニバッチにしたい。つまり\n",
    "        # (state×BATCH_SIZE, action×BATCH_SIZE, state_next×BATCH_SIZE, reward×BATCH_SIZE)にする\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # 2.3 各変数の要素をミニバッチに対応する形に変形し、ネットワークで扱えるようVariableにする\n",
    "        # 例えばstateの場合、[torch.FloatTensor of size 1x4]がBATCH_SIZE分並んでいるのですが、\n",
    "        # それを torch.FloatTensor of size BATCH_SIZEx4 に変換します\n",
    "        # 状態、行動、報酬、non_finalの状態のミニバッチのVariableを作成\n",
    "        # catはConcatenates（結合）のことです。\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                           if s is not None])\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 3. 教師信号となるQ(s_t, a_t)値を求める\n",
    "        # -----------------------------------------\n",
    "        # 3.1 ネットワークを推論モードに切り替える\n",
    "        self.model.eval()\n",
    "\n",
    "        # 3.2 ネットワークが出力したQ(s_t, a_t)を求める\n",
    "        # self.model(state_batch)は、右左の両方のQ値を出力しており\n",
    "        # [torch.FloatTensor of size BATCH_SIZEx2]になっている。\n",
    "        # ここから実行したアクションa_tに対応するQ値を求めるため、action_batchで行った行動a_tが右か左かのindexを求め\n",
    "        # それに対応するQ値をgatherでひっぱり出す。\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # 3.3 max{Q(s_t+1, a)}値を求める。ただし次の状態があるかに注意。\n",
    "\n",
    "        # cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成\n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None,\n",
    "                                                    batch.next_state)))\n",
    "        # まずは全部0にしておく\n",
    "        next_state_values = torch.zeros(BATCH_SIZE)\n",
    "\n",
    "        # 次の状態があるindexの最大Q値を求める\n",
    "        # 出力にアクセスし、max(1)で列方向の最大値の[値、index]を求めます\n",
    "        # そしてそのQ値（index=0）を出力します\n",
    "        # detachでその値を取り出します\n",
    "        next_state_values[non_final_mask] = self.model(\n",
    "            non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "        # 3.4 教師となるQ(s_t, a_t)値を、Q学習の式から求める\n",
    "        expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 4. 結合パラメータの更新\n",
    "        # -----------------------------------------\n",
    "        # 4.1 ネットワークを訓練モードに切り替える\n",
    "        self.model.train()\n",
    "\n",
    "        # 4.2 損失関数を計算する（smooth_l1_lossはHuberloss）\n",
    "        # expected_state_action_valuesは\n",
    "        # sizeが[minbatch]になっているので、unsqueezeで[minibatch x 1]へ\n",
    "        loss = F.smooth_l1_loss(state_action_values,\n",
    "                                expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # 4.3 結合パラメータを更新する\n",
    "        self.optimizer.zero_grad()  # 勾配をリセット\n",
    "        loss.backward()  # バックプロパゲーションを計算\n",
    "        self.optimizer.step()  # 結合パラメータを更新\n",
    "\n",
    "    def decide_action(self, state, episode):\n",
    "        '''現在の状態に応じて、行動を決定する'''\n",
    "        # ε-greedy法で徐々に最適行動のみを採用する\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            self.model.eval()  # ネットワークを推論モードに切り替える\n",
    "            with torch.no_grad():\n",
    "                action = self.model(state).max(1)[1].view(1, 1)\n",
    "            # ネットワークの出力の最大値のindexを取り出します = max(1)[1]\n",
    "            # .view(1,1)は[torch.LongTensor of size 1]　を size 1x1 に変換します\n",
    "\n",
    "        else:\n",
    "            # 0,1の行動をランダムに返す\n",
    "            action = torch.LongTensor(\n",
    "                [[random.randrange(self.num_actions)]])  # 0,1の行動をランダムに返す\n",
    "            # actionは[torch.LongTensor of size 1x1]の形になります\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## brain 解説"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions  # CartPoleの行動（右に左に押す）の2を取得\n",
    "\n",
    "        # 経験を記憶するメモリオブジェクトを生成\n",
    "        self.memory = ReplayMemory(CAPACITY)\n",
    "\n",
    "        # ニューラルネットワークを構築\n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add_module('fc1', nn.Linear(num_states, 32))\n",
    "        self.model.add_module('relu1', nn.ReLU())\n",
    "        self.model.add_module('fc2', nn.Linear(32, 32))\n",
    "        self.model.add_module('relu2', nn.ReLU())\n",
    "        self.model.add_module('fc3', nn.Linear(32, num_actions))\n",
    "\n",
    "        print(self.model)  # ネットワークの形を出力\n",
    "\n",
    "        # 最適化手法の設定\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience Replayでネットワークの結合パラメータを学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def replay(self):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### メモリサイズの確認\n",
    "メモリサイズがミニバッチより小さい間は何もしない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 170680a] since quite a long time ago\n",
      " 1 file changed, 91 insertions(+), 29 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git commit -a -m \"since quite a long time ago\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ミニバッチの作成\n",
    "##### メモリからミニバッチ分のデータを取り出す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        transitions = self.memory.sample(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 各変数をミニバッチに対応する形に変形\n",
    "* transitionsは1stepごとの\n",
    "$$\\text{(state, action, state_next, reward)}$$  \n",
    "  が、BATCH_SIZE分格納されている\n",
    "  \n",
    "* つまり、\n",
    "  $$\\text{(state, action, state_next, reward)}\\times\\text{BATCH_SIZE}$$\n",
    "  \n",
    "* これをミニバッチにしたい。つまり\n",
    "    $$(\\text{state}\\times\\text{BATCH_SIZE}, ~\\text{action}\\times\\text{BATCH_SIZE}, ~\\text{state_next}\\times\\text{BATCH_SIZE},~ \\text{reward}\\times\\text{BATCH_SIZE})$$\n",
    "  にする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        batch = Transition(*zip(*transitions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 各変数の要素をミニバッチに対応する形に変形し、ネットワークで扱えるようVariableにする\n",
    "*  例えばstateの場合、[torch.FloatTensor of size 1x4]がBATCH_SIZE分並んでいる\n",
    "* それを torch.FloatTensor of size BATCH_SIZEx4 に変換します\n",
    "```python \n",
    "        # 状態、行動、報酬、non_finalの状態のミニバッチのVariableを作成\n",
    "        # catはConcatenates（結合）のことです。\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                           if s is not None])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 教師信号となるQ(s_t, a_t)値を求める\n",
    "##### ネットワークを推論モードに切り替える\n",
    "        self.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ネットワークが出力したQ(s_t, a_t)を求める\n",
    "* self.model(state_batch)は、右左の両方のQ値を出力しており[torch.FloatTensor of size BATCH_SIZEx2]になっている。\n",
    "* ここから実行したアクションa_tに対応するQ値を求めるため、action_batchで行った行動a_tが右か左かのindexを求め、それに対応するQ値をgatherでひっぱり出す。\n",
    "        \n",
    "```python\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        state_action_values = self.model(state_batch).gather(1, action_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        # 3.3 max{Q(s_t+1, a)}値を求める。ただし次の状態があるかに注意。\n",
    "\n",
    "        # cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成\n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None,\n",
    "                                                    batch.next_state)))\n",
    "        # まずは全部0にしておく\n",
    "        next_state_values = torch.zeros(BATCH_SIZE)\n",
    "\n",
    "        # 次の状態があるindexの最大Q値を求める\n",
    "        # 出力にアクセスし、max(1)で列方向の最大値の[値、index]を求めます\n",
    "        # そしてそのQ値（index=0）を出力します\n",
    "        # detachでその値を取り出します\n",
    "        next_state_values[non_final_mask] = self.model(\n",
    "            non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "        # 3.4 教師となるQ(s_t, a_t)値を、Q学習の式から求める\n",
    "        expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 4. 結合パラメータの更新\n",
    "        # -----------------------------------------\n",
    "        # 4.1 ネットワークを訓練モードに切り替える\n",
    "        self.model.train()\n",
    "\n",
    "        # 4.2 損失関数を計算する（smooth_l1_lossはHuberloss）\n",
    "        # expected_state_action_valuesは\n",
    "        # sizeが[minbatch]になっているので、unsqueezeで[minibatch x 1]へ\n",
    "        loss = F.smooth_l1_loss(state_action_values,\n",
    "                                expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # 4.3 結合パラメータを更新する\n",
    "        self.optimizer.zero_grad()  # 勾配をリセット\n",
    "        loss.backward()  # バックプロパゲーションを計算\n",
    "        self.optimizer.step()  # 結合パラメータを更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decide_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def decide_action(self, state, episode):\n",
    "        '''現在の状態に応じて、行動を決定する'''\n",
    "        # ε-greedy法で徐々に最適行動のみを採用する\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            self.model.eval()  # ネットワークを推論モードに切り替える\n",
    "            with torch.no_grad():\n",
    "                action = self.model(state).max(1)[1].view(1, 1)\n",
    "            # ネットワークの出力の最大値のindexを取り出します = max(1)[1]\n",
    "            # .view(1,1)は[torch.LongTensor of size 1]　を size 1x1 に変換します\n",
    "\n",
    "        else:\n",
    "            # 0,1の行動をランダムに返す\n",
    "            action = torch.LongTensor(\n",
    "                [[random.randrange(self.num_actions)]])  # 0,1の行動をランダムに返す\n",
    "            # actionは[torch.LongTensor of size 1x1]の形になります\n",
    "\n",
    "        return action\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解説終わり"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions  # CartPoleの行動（右に左に押す）の2を取得\n",
    "\n",
    "\n",
    "        print(self.model)  # ネットワークの形を出力\n",
    "\n",
    "        # \n",
    "       \n",
    "\n",
    "    def replay(self):\n",
    "        '''Experience Replayでネットワークの結合パラメータを学習'''\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 1. メモリサイズの確認\n",
    "        # -----------------------------------------\n",
    "        # 1.1 メモリサイズがミニバッチより小さい間は何もしない\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2. ミニバッチの作成\n",
    "        # -----------------------------------------\n",
    "        # 2.1 メモリからミニバッチ分のデータを取り出す\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        # 2.2 各変数をミニバッチに対応する形に変形\n",
    "        # transitionsは1stepごとの(state, action, state_next, reward)が、BATCH_SIZE分格納されている\n",
    "        # つまり、(state, action, state_next, reward)×BATCH_SIZE\n",
    "        # これをミニバッチにしたい。つまり\n",
    "        # (state×BATCH_SIZE, action×BATCH_SIZE, state_next×BATCH_SIZE, reward×BATCH_SIZE)にする\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # 2.3 各変数の要素をミニバッチに対応する形に変形し、ネットワークで扱えるようVariableにする\n",
    "        # 例えばstateの場合、[torch.FloatTensor of size 1x4]がBATCH_SIZE分並んでいるのですが、\n",
    "        # それを torch.FloatTensor of size BATCH_SIZEx4 に変換します\n",
    "        # 状態、行動、報酬、non_finalの状態のミニバッチのVariableを作成\n",
    "        # catはConcatenates（結合）のことです。\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                           if s is not None])\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 3. 教師信号となるQ(s_t, a_t)値を求める\n",
    "        # -----------------------------------------\n",
    "        # 3.1 ネットワークを推論モードに切り替える\n",
    "        self.model.eval()\n",
    "\n",
    "        # 3.2 ネットワークが出力したQ(s_t, a_t)を求める\n",
    "        # self.model(state_batch)は、右左の両方のQ値を出力しており\n",
    "        # [torch.FloatTensor of size BATCH_SIZEx2]になっている。\n",
    "        # ここから実行したアクションa_tに対応するQ値を求めるため、action_batchで行った行動a_tが右か左かのindexを求め\n",
    "        # それに対応するQ値をgatherでひっぱり出す。\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # 3.3 max{Q(s_t+1, a)}値を求める。ただし次の状態があるかに注意。\n",
    "\n",
    "        # cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成\n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None,\n",
    "                                                    batch.next_state)))\n",
    "        # まずは全部0にしておく\n",
    "        next_state_values = torch.zeros(BATCH_SIZE)\n",
    "\n",
    "        # 次の状態があるindexの最大Q値を求める\n",
    "        # 出力にアクセスし、max(1)で列方向の最大値の[値、index]を求めます\n",
    "        # そしてそのQ値（index=0）を出力します\n",
    "        # detachでその値を取り出します\n",
    "        next_state_values[non_final_mask] = self.model(\n",
    "            non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "        # 3.4 教師となるQ(s_t, a_t)値を、Q学習の式から求める\n",
    "        expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 4. 結合パラメータの更新\n",
    "        # -----------------------------------------\n",
    "        # 4.1 ネットワークを訓練モードに切り替える\n",
    "        self.model.train()\n",
    "\n",
    "        # 4.2 損失関数を計算する（smooth_l1_lossはHuberloss）\n",
    "        # expected_state_action_valuesは\n",
    "        # sizeが[minbatch]になっているので、unsqueezeで[minibatch x 1]へ\n",
    "        loss = F.smooth_l1_loss(state_action_values,\n",
    "                                expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # 4.3 結合パラメータを更新する\n",
    "        self.optimizer.zero_grad()  # 勾配をリセット\n",
    "        loss.backward()  # バックプロパゲーションを計算\n",
    "        self.optimizer.step()  # 結合パラメータを更新\n",
    "\n",
    "    def decide_action(self, state, episode):\n",
    "        '''現在の状態に応じて、行動を決定する'''\n",
    "        # ε-greedy法で徐々に最適行動のみを採用する\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            self.model.eval()  # ネットワークを推論モードに切り替える\n",
    "            with torch.no_grad():\n",
    "                action = self.model(state).max(1)[1].view(1, 1)\n",
    "            # ネットワークの出力の最大値のindexを取り出します = max(1)[1]\n",
    "            # .view(1,1)は[torch.LongTensor of size 1]　を size 1x1 に変換します\n",
    "\n",
    "        else:\n",
    "            # 0,1の行動をランダムに返す\n",
    "            action = torch.LongTensor(\n",
    "                [[random.randrange(self.num_actions)]])  # 0,1の行動をランダムに返す\n",
    "            # actionは[torch.LongTensor of size 1x1]の形になります\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myBrain0:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions  # CartPoleの行動（右に左に押す）の2を取得\n",
    "\n",
    "        # 経験を記憶するメモリオブジェクトを生成\n",
    "        self.memory = ReplayMemory(CAPACITY)\n",
    "\n",
    "        # ニューラルネットワークを構築\n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add_module('fc1', nn.Linear(num_states, 32))\n",
    "        self.model.add_module('relu1', nn.ReLU())\n",
    "        self.model.add_module('fc2', nn.Linear(32, 32))\n",
    "        self.model.add_module('relu2', nn.ReLU())\n",
    "        self.model.add_module('fc3', nn.Linear(32, num_actions))\n",
    "\n",
    "        print(self.model)  # ネットワークの形を出力\n",
    "\n",
    "        # 最適化手法の設定\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* メモリサイズの確認\n",
    "    * メモリサイズがミニバッチより小さい間は何もしない\n",
    "* ミニバッチの作成\n",
    "    * メモリからミニバッチ分のデータを取り出す\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "```python\n",
    "        batch = Transition(*zip(*transitions))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$1+2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 各変数をミニバッチに対応する形に変形\n",
    "    * transitionsは1stepごとの(state, action, state_next, reward)が、BATCH_SIZE分格納されている\n",
    "    * つまり$\\(state, action, state_next, reward)\\times\\text{BATCH_SIZE}$    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    * これをミニバッチにしたい。つまり\n",
    "    $$(\\text{state}\\times\\text{BATCH_SIZE}, \\\\text{action}\\times\\text{BATCH_SIZE}, \\text{state_next}\\times\\text{BATCH_SIZE},\\text{reward}\\times\\text{BATCH_SIZE})$$\n",
    "    にする\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * 各変数の要素をミニバッチに対応する形に変形し、ネットワークで扱えるようVariableにする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        例えばstateの場合、[torch.FloatTensor of size 1x4]がBATCH_SIZE分並んでいるのですが、\n",
    "        それを torch.FloatTensor of size BATCH_SIZEx4 に変換します\n",
    "        状態、行動、報酬、non_finalの状態のミニバッチのVariableを作成\n",
    "        catはConcatenates（結合）のことです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myBrain1(myBrain0):\n",
    "    def replay(self):\n",
    "        '''Experience Replayでネットワークの結合パラメータを学習'''\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 1. メモリサイズの確認\n",
    "        # -----------------------------------------\n",
    "        # 1.1 メモリサイズがミニバッチより小さい間は何もしない\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2. ミニバッチの作成\n",
    "        # -----------------------------------------\n",
    "        # 2.1 メモリからミニバッチ分のデータを取り出す\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        # 2.2 各変数をミニバッチに対応する形に変形\n",
    "\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # 2.3 各変数の要素をミニバッチに対応する形に変形し、ネットワークで扱えるようVariableにする\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                           if s is not None])\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 3. 教師信号となるQ(s_t, a_t)値を求める\n",
    "        # -----------------------------------------\n",
    "        # 3.1 ネットワークを推論モードに切り替える\n",
    "        self.model.eval()\n",
    "\n",
    "        # 3.2 ネットワークが出力したQ(s_t, a_t)を求める\n",
    "\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # 3.3 max{Q(s_t+1, a)}値を求める。ただし次の状態があるかに注意。\n",
    "\n",
    "        # cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成\n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None,\n",
    "                                                    batch.next_state)))\n",
    "        # まずは全部0にしておく\n",
    "        next_state_values = torch.zeros(BATCH_SIZE)\n",
    "\n",
    "        # 次の状態があるindexの最大Q値を求める\n",
    "        # 出力にアクセスし、max(1)で列方向の最大値の[値、index]を求めます\n",
    "        # そしてそのQ値（index=0）を出力します\n",
    "        # detachでその値を取り出します\n",
    "        next_state_values[non_final_mask] = self.model(\n",
    "            non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "        # 3.4 教師となるQ(s_t, a_t)値を、Q学習の式から求める\n",
    "        expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 4. 結合パラメータの更新\n",
    "        # -----------------------------------------\n",
    "        # 4.1 ネットワークを訓練モードに切り替える\n",
    "        self.model.train()\n",
    "\n",
    "        # 4.2 損失関数を計算する（smooth_l1_lossはHuberloss）\n",
    "        # expected_state_action_valuesは\n",
    "        # sizeが[minbatch]になっているので、unsqueezeで[minibatch x 1]へ\n",
    "        loss = F.smooth_l1_loss(state_action_values,\n",
    "                                expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # 4.3 結合パラメータを更新する\n",
    "        self.optimizer.zero_grad()  # 勾配をリセット\n",
    "        loss.backward()  # バックプロパゲーションを計算\n",
    "        self.optimizer.step()  # 結合パラメータを更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myBrain2(myBrain1):\n",
    "    def decide_action(self, state, episode):\n",
    "        '''現在の状態に応じて、行動を決定する'''\n",
    "        # \n",
    "        -greedy法で徐々に最適行動のみを採用する\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            self.model.eval()  # ネットワークを推論モードに切り替える\n",
    "            with torch.no_grad():\n",
    "                action = self.model(state).max(1)[1].view(1, 1)\n",
    "            # ネットワークの出力の最大値のindexを取り出します = max(1)[1]\n",
    "            # .view(1,1)は[torch.LongTensor of size 1]　を size 1x1 に変換します\n",
    "\n",
    "        else:\n",
    "            # 0,1の行動をランダムに返す\n",
    "            action = torch.LongTensor(\n",
    "                [[random.randrange(self.num_actions)]])  # 0,1の行動をランダムに返す\n",
    "            # actionは[torch.LongTensor of size 1x1]の形になります\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "CartPoleで動くエージェントクラスです、棒付き台車そのものになります\n",
    "\n",
    "methodは\n",
    "\n",
    "\n",
    "* 行動価値Qを更新\n",
    "*  状態を与えると行動を決定\n",
    "*  状態、選択するアクション、次の状態、報酬などを記憶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        '''課題の状態と行動の数を設定する'''\n",
    "        self.brain = Brain(num_states, num_actions)  # エージェントが行動を決定するための頭脳を生成\n",
    "\n",
    "    def update_q_function(self):\n",
    "        '''Q関数を更新する'''\n",
    "        self.brain.replay()\n",
    "\n",
    "    def get_action(self, state, episode):\n",
    "        '''行動を決定する'''\n",
    "        action = self.brain.decide_action(state, episode)\n",
    "        return action\n",
    "\n",
    "    def memorize(self, state, action, state_next, reward):\n",
    "        '''memoryオブジェクトに、state, action, state_next, rewardの内容を保存する'''\n",
    "        self.brain.memory.push(state, action, state_next, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPoleを実行する環境のクラスです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV)  # 実行する課題を設定\n",
    "        num_states = self.env.observation_space.shape[0]  # 課題の状態数4を取得\n",
    "        num_actions = self.env.action_space.n  # CartPoleの行動（右に左に押す）の2を取得\n",
    "        self.agent = Agent(num_states, num_actions)  # 環境内で行動するAgentを生成\n",
    "\n",
    "        \n",
    "    def run(self):\n",
    "        '''実行'''\n",
    "        episode_10_list = np.zeros(10)  # 10試行分の立ち続けたstep数を格納し、平均ステップ数を出力に利用\n",
    "        complete_episodes = 0  # 195step以上連続で立ち続けた試行数\n",
    "        episode_final = False  # 最後の試行フラグ\n",
    "        frames = []  # 最後の試行を動画にするために画像を格納する変数\n",
    "\n",
    "        for episode in range(NUM_EPISODES):  # 最大試行数分繰り返す\n",
    "            observation = self.env.reset()  # 環境の初期化\n",
    "\n",
    "            state = observation  # 観測をそのまま状態sとして使用\n",
    "            state = torch.from_numpy(state).type(\n",
    "                torch.FloatTensor)  # NumPy変数をPyTorchのテンソルに変換\n",
    "            state = torch.unsqueeze(state, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "            for step in range(MAX_STEPS):  # 1エピソードのループ\n",
    "\n",
    "                if episode_final is True:  # 最終試行ではframesに各時刻の画像を追加していく\n",
    "                    frames.append(self.env.render(mode='rgb_array'))\n",
    "\n",
    "                action = self.agent.get_action(state, episode)  # 行動を求める\n",
    "\n",
    "                # 行動a_tの実行により、s_{t+1}とdoneフラグを求める\n",
    "                # actionから.item()を指定して、中身を取り出す\n",
    "                observation_next, _, done, _ = self.env.step(\n",
    "                    action.item())  # rewardとinfoは使わないので_にする\n",
    "\n",
    "                # 報酬を与える。さらにepisodeの終了評価と、state_nextを設定する\n",
    "                if done:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\n",
    "                    state_next = None  # 次の状態はないので、Noneを格納\n",
    "\n",
    "                    # 直近10episodeの立てたstep数リストに追加\n",
    "                    episode_10_list = np.hstack(\n",
    "                        (episode_10_list[1:], step + 1))\n",
    "\n",
    "                    if step < 195:\n",
    "                        reward = torch.FloatTensor(\n",
    "                            [-1.0])  # 途中でこけたら罰則として報酬-1を与える\n",
    "                        complete_episodes = 0  # 連続成功記録をリセット\n",
    "                    else:\n",
    "                        reward = torch.FloatTensor([1.0])  # 立ったまま終了時は報酬1を与える\n",
    "                        complete_episodes = complete_episodes + 1  # 連続記録を更新\n",
    "                else:\n",
    "                    reward = torch.FloatTensor([0.0])  # 普段は報酬0\n",
    "                    state_next = observation_next  # 観測をそのまま状態とする\n",
    "                    state_next = torch.from_numpy(state_next).type(\n",
    "                        torch.FloatTensor)  # numpy変数をPyTorchのテンソルに変換\n",
    "                    state_next = torch.unsqueeze(state_next, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "                # メモリに経験を追加\n",
    "                self.agent.memorize(state, action, state_next, reward)\n",
    "\n",
    "                # Experience ReplayでQ関数を更新する\n",
    "                self.agent.update_q_function()\n",
    "\n",
    "                # 観測の更新\n",
    "                state = state_next\n",
    "\n",
    "                # 終了時の処理\n",
    "                if done:\n",
    "                    print('%d Episode: Finished after %d steps：10試行の平均step数 = %.1lf' % (\n",
    "                        episode, step + 1, episode_10_list.mean()))\n",
    "                    break\n",
    "\n",
    "            if episode_final is True:\n",
    "                # 動画を保存と描画\n",
    "                display_frames_as_gif(frames)\n",
    "                break\n",
    "\n",
    "            # 10連続で200step経ち続けたら成功\n",
    "            if complete_episodes >= 10:\n",
    "                print('10回連続成功')\n",
    "                episode_final = True  # 次の試行を描画を行う最終試行とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "0 Episode: Finished after 8 steps：10試行の平均step数 = 0.8\n",
      "1 Episode: Finished after 13 steps：10試行の平均step数 = 2.1\n",
      "2 Episode: Finished after 10 steps：10試行の平均step数 = 3.1\n",
      "3 Episode: Finished after 10 steps：10試行の平均step数 = 4.1\n",
      "4 Episode: Finished after 10 steps：10試行の平均step数 = 5.1\n",
      "5 Episode: Finished after 9 steps：10試行の平均step数 = 6.0\n",
      "6 Episode: Finished after 9 steps：10試行の平均step数 = 6.9\n",
      "7 Episode: Finished after 9 steps：10試行の平均step数 = 7.8\n",
      "8 Episode: Finished after 8 steps：10試行の平均step数 = 8.6\n",
      "9 Episode: Finished after 12 steps：10試行の平均step数 = 9.8\n",
      "10 Episode: Finished after 10 steps：10試行の平均step数 = 10.0\n",
      "11 Episode: Finished after 10 steps：10試行の平均step数 = 9.7\n",
      "12 Episode: Finished after 10 steps：10試行の平均step数 = 9.7\n",
      "13 Episode: Finished after 9 steps：10試行の平均step数 = 9.6\n",
      "14 Episode: Finished after 10 steps：10試行の平均step数 = 9.6\n",
      "15 Episode: Finished after 9 steps：10試行の平均step数 = 9.6\n",
      "16 Episode: Finished after 9 steps：10試行の平均step数 = 9.6\n",
      "17 Episode: Finished after 9 steps：10試行の平均step数 = 9.6\n",
      "18 Episode: Finished after 9 steps：10試行の平均step数 = 9.7\n",
      "19 Episode: Finished after 8 steps：10試行の平均step数 = 9.3\n",
      "20 Episode: Finished after 9 steps：10試行の平均step数 = 9.2\n",
      "21 Episode: Finished after 9 steps：10試行の平均step数 = 9.1\n",
      "22 Episode: Finished after 11 steps：10試行の平均step数 = 9.2\n",
      "23 Episode: Finished after 9 steps：10試行の平均step数 = 9.2\n",
      "24 Episode: Finished after 9 steps：10試行の平均step数 = 9.1\n",
      "25 Episode: Finished after 10 steps：10試行の平均step数 = 9.2\n",
      "26 Episode: Finished after 8 steps：10試行の平均step数 = 9.1\n",
      "27 Episode: Finished after 9 steps：10試行の平均step数 = 9.1\n",
      "28 Episode: Finished after 10 steps：10試行の平均step数 = 9.2\n",
      "29 Episode: Finished after 9 steps：10試行の平均step数 = 9.3\n",
      "30 Episode: Finished after 9 steps：10試行の平均step数 = 9.3\n",
      "31 Episode: Finished after 9 steps：10試行の平均step数 = 9.3\n",
      "32 Episode: Finished after 9 steps：10試行の平均step数 = 9.1\n",
      "33 Episode: Finished after 10 steps：10試行の平均step数 = 9.2\n",
      "34 Episode: Finished after 14 steps：10試行の平均step数 = 9.7\n",
      "35 Episode: Finished after 12 steps：10試行の平均step数 = 9.9\n",
      "36 Episode: Finished after 14 steps：10試行の平均step数 = 10.5\n",
      "37 Episode: Finished after 12 steps：10試行の平均step数 = 10.8\n",
      "38 Episode: Finished after 12 steps：10試行の平均step数 = 11.0\n",
      "39 Episode: Finished after 14 steps：10試行の平均step数 = 11.5\n",
      "40 Episode: Finished after 16 steps：10試行の平均step数 = 12.2\n",
      "41 Episode: Finished after 13 steps：10試行の平均step数 = 12.6\n",
      "42 Episode: Finished after 15 steps：10試行の平均step数 = 13.2\n",
      "43 Episode: Finished after 13 steps：10試行の平均step数 = 13.5\n",
      "44 Episode: Finished after 14 steps：10試行の平均step数 = 13.5\n",
      "45 Episode: Finished after 14 steps：10試行の平均step数 = 13.7\n",
      "46 Episode: Finished after 15 steps：10試行の平均step数 = 13.8\n",
      "47 Episode: Finished after 15 steps：10試行の平均step数 = 14.1\n",
      "48 Episode: Finished after 18 steps：10試行の平均step数 = 14.7\n",
      "49 Episode: Finished after 21 steps：10試行の平均step数 = 15.4\n",
      "50 Episode: Finished after 17 steps：10試行の平均step数 = 15.5\n",
      "51 Episode: Finished after 16 steps：10試行の平均step数 = 15.8\n",
      "52 Episode: Finished after 16 steps：10試行の平均step数 = 15.9\n",
      "53 Episode: Finished after 9 steps：10試行の平均step数 = 15.5\n",
      "54 Episode: Finished after 9 steps：10試行の平均step数 = 15.0\n",
      "55 Episode: Finished after 12 steps：10試行の平均step数 = 14.8\n",
      "56 Episode: Finished after 9 steps：10試行の平均step数 = 14.2\n",
      "57 Episode: Finished after 10 steps：10試行の平均step数 = 13.7\n",
      "58 Episode: Finished after 10 steps：10試行の平均step数 = 12.9\n",
      "59 Episode: Finished after 9 steps：10試行の平均step数 = 11.7\n",
      "60 Episode: Finished after 9 steps：10試行の平均step数 = 10.9\n",
      "61 Episode: Finished after 9 steps：10試行の平均step数 = 10.2\n",
      "62 Episode: Finished after 9 steps：10試行の平均step数 = 9.5\n",
      "63 Episode: Finished after 9 steps：10試行の平均step数 = 9.5\n",
      "64 Episode: Finished after 11 steps：10試行の平均step数 = 9.7\n",
      "65 Episode: Finished after 9 steps：10試行の平均step数 = 9.4\n",
      "66 Episode: Finished after 24 steps：10試行の平均step数 = 10.9\n",
      "67 Episode: Finished after 12 steps：10試行の平均step数 = 11.1\n",
      "68 Episode: Finished after 8 steps：10試行の平均step数 = 10.9\n",
      "69 Episode: Finished after 9 steps：10試行の平均step数 = 10.9\n",
      "70 Episode: Finished after 9 steps：10試行の平均step数 = 10.9\n",
      "71 Episode: Finished after 19 steps：10試行の平均step数 = 11.9\n",
      "72 Episode: Finished after 25 steps：10試行の平均step数 = 13.5\n",
      "73 Episode: Finished after 20 steps：10試行の平均step数 = 14.6\n",
      "74 Episode: Finished after 38 steps：10試行の平均step数 = 17.3\n",
      "75 Episode: Finished after 39 steps：10試行の平均step数 = 20.3\n",
      "76 Episode: Finished after 26 steps：10試行の平均step数 = 20.5\n",
      "77 Episode: Finished after 36 steps：10試行の平均step数 = 22.9\n",
      "78 Episode: Finished after 25 steps：10試行の平均step数 = 24.6\n",
      "79 Episode: Finished after 34 steps：10試行の平均step数 = 27.1\n",
      "80 Episode: Finished after 28 steps：10試行の平均step数 = 29.0\n",
      "81 Episode: Finished after 55 steps：10試行の平均step数 = 32.6\n",
      "82 Episode: Finished after 51 steps：10試行の平均step数 = 35.2\n",
      "83 Episode: Finished after 87 steps：10試行の平均step数 = 41.9\n",
      "84 Episode: Finished after 35 steps：10試行の平均step数 = 41.6\n",
      "85 Episode: Finished after 78 steps：10試行の平均step数 = 45.5\n",
      "86 Episode: Finished after 36 steps：10試行の平均step数 = 46.5\n",
      "87 Episode: Finished after 41 steps：10試行の平均step数 = 47.0\n",
      "88 Episode: Finished after 33 steps：10試行の平均step数 = 47.8\n",
      "89 Episode: Finished after 35 steps：10試行の平均step数 = 47.9\n",
      "90 Episode: Finished after 65 steps：10試行の平均step数 = 51.6\n",
      "91 Episode: Finished after 49 steps：10試行の平均step数 = 51.0\n",
      "92 Episode: Finished after 99 steps：10試行の平均step数 = 55.8\n",
      "93 Episode: Finished after 55 steps：10試行の平均step数 = 52.6\n",
      "94 Episode: Finished after 128 steps：10試行の平均step数 = 61.9\n",
      "95 Episode: Finished after 66 steps：10試行の平均step数 = 60.7\n",
      "96 Episode: Finished after 48 steps：10試行の平均step数 = 61.9\n",
      "97 Episode: Finished after 44 steps：10試行の平均step数 = 62.2\n",
      "98 Episode: Finished after 47 steps：10試行の平均step数 = 63.6\n",
      "99 Episode: Finished after 52 steps：10試行の平均step数 = 65.3\n",
      "100 Episode: Finished after 99 steps：10試行の平均step数 = 68.7\n",
      "101 Episode: Finished after 49 steps：10試行の平均step数 = 68.7\n",
      "102 Episode: Finished after 71 steps：10試行の平均step数 = 65.9\n",
      "103 Episode: Finished after 71 steps：10試行の平均step数 = 67.5\n",
      "104 Episode: Finished after 63 steps：10試行の平均step数 = 61.0\n",
      "105 Episode: Finished after 59 steps：10試行の平均step数 = 60.3\n",
      "106 Episode: Finished after 62 steps：10試行の平均step数 = 61.7\n",
      "107 Episode: Finished after 67 steps：10試行の平均step数 = 64.0\n",
      "108 Episode: Finished after 55 steps：10試行の平均step数 = 64.8\n",
      "109 Episode: Finished after 58 steps：10試行の平均step数 = 65.4\n",
      "110 Episode: Finished after 62 steps：10試行の平均step数 = 61.7\n",
      "111 Episode: Finished after 52 steps：10試行の平均step数 = 62.0\n",
      "112 Episode: Finished after 94 steps：10試行の平均step数 = 64.3\n",
      "113 Episode: Finished after 72 steps：10試行の平均step数 = 64.4\n",
      "114 Episode: Finished after 66 steps：10試行の平均step数 = 64.7\n",
      "115 Episode: Finished after 99 steps：10試行の平均step数 = 68.7\n",
      "116 Episode: Finished after 76 steps：10試行の平均step数 = 70.1\n",
      "117 Episode: Finished after 75 steps：10試行の平均step数 = 70.9\n",
      "118 Episode: Finished after 170 steps：10試行の平均step数 = 82.4\n",
      "119 Episode: Finished after 91 steps：10試行の平均step数 = 85.7\n",
      "120 Episode: Finished after 93 steps：10試行の平均step数 = 88.8\n",
      "121 Episode: Finished after 99 steps：10試行の平均step数 = 93.5\n",
      "122 Episode: Finished after 83 steps：10試行の平均step数 = 92.4\n",
      "123 Episode: Finished after 93 steps：10試行の平均step数 = 94.5\n",
      "124 Episode: Finished after 87 steps：10試行の平均step数 = 96.6\n",
      "125 Episode: Finished after 96 steps：10試行の平均step数 = 96.3\n",
      "126 Episode: Finished after 96 steps：10試行の平均step数 = 98.3\n",
      "127 Episode: Finished after 93 steps：10試行の平均step数 = 100.1\n",
      "128 Episode: Finished after 124 steps：10試行の平均step数 = 95.5\n",
      "129 Episode: Finished after 103 steps：10試行の平均step数 = 96.7\n",
      "130 Episode: Finished after 106 steps：10試行の平均step数 = 98.0\n",
      "131 Episode: Finished after 116 steps：10試行の平均step数 = 99.7\n",
      "132 Episode: Finished after 111 steps：10試行の平均step数 = 102.5\n",
      "133 Episode: Finished after 97 steps：10試行の平均step数 = 102.9\n",
      "134 Episode: Finished after 159 steps：10試行の平均step数 = 110.1\n",
      "135 Episode: Finished after 200 steps：10試行の平均step数 = 120.5\n",
      "136 Episode: Finished after 200 steps：10試行の平均step数 = 130.9\n",
      "137 Episode: Finished after 200 steps：10試行の平均step数 = 141.6\n",
      "138 Episode: Finished after 105 steps：10試行の平均step数 = 139.7\n",
      "139 Episode: Finished after 200 steps：10試行の平均step数 = 149.4\n",
      "140 Episode: Finished after 200 steps：10試行の平均step数 = 158.8\n",
      "141 Episode: Finished after 200 steps：10試行の平均step数 = 167.2\n",
      "142 Episode: Finished after 200 steps：10試行の平均step数 = 176.1\n",
      "143 Episode: Finished after 200 steps：10試行の平均step数 = 186.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 Episode: Finished after 200 steps：10試行の平均step数 = 190.5\n",
      "145 Episode: Finished after 200 steps：10試行の平均step数 = 190.5\n",
      "146 Episode: Finished after 200 steps：10試行の平均step数 = 190.5\n",
      "147 Episode: Finished after 200 steps：10試行の平均step数 = 190.5\n",
      "148 Episode: Finished after 200 steps：10試行の平均step数 = 200.0\n",
      "10回連続成功\n"
     ]
    },
    {
     "ename": "NoSuchDisplayException",
     "evalue": "Cannot connect to \"None\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-4bf7f6ad7ab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# main クラス\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcartpole_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcartpole_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-79-5ad9ba56351d>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mepisode_final\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 最終試行ではframesに各時刻の画像を追加していく\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 行動を求める\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Error occured while running `from pyglet.gl import *`\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HINT: make sure you have OpenGL install. On Ubuntu, you can run 'apt-get install python-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work: 'xvfb-run -s \\\"-screen 0 1400x900x24\\\" python <your_script.py>'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1814\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_epydoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1815\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1816\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_platform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36mget_default_display\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1763\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDisplay\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         '''\n\u001b[0;32m-> 1765\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_epydoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pyglet/canvas/__init__.py\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_epydoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pyglet/canvas/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""
     ]
    }
   ],
   "source": [
    "# main クラス\n",
    "cartpole_env = Environment()\n",
    "cartpole_env.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classでなくしてみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## envのinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "CAPACITY = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_env = gym.make(ENV)  # 実行する課題を設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = my_env.observation_space.shape[0]  # 課題の状態数4を取得\n",
    "num_actions = my_env.action_space.n  # CartPoleの行動（右に左に押す）の2を取得"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "my_agent = Agent(num_states, num_actions)  # 環境内で行動するAgentを生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(4,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_10_list = np.zeros(10)  # 10試行分の立ち続けたstep数を格納し、平均ステップ数を出力に利用\n",
    "complete_episodes = 0  # 195step以上連続で立ち続けた試行数\n",
    "episode_final = False  # 最後の試行フラグ\n",
    "frames = []  # 最後の試行を動画にするために画像を格納する変数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## envのfor loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = my_env.reset()  # 環境の初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = observation  # 観測をそのまま状態sとして使用\n",
    "state = torch.from_numpy(state).type(\n",
    "    torch.FloatTensor)  # NumPy変数をPyTorchのテンソルに変換\n",
    "state = torch.unsqueeze(state, 0)  # size 4をsize 1x4に変換\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0196,  0.0067,  0.0202,  0.0110]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for step in range(MAX_STEPS):  # 1エピソードのループ\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if episode_final is True:  # 最終試行ではframesに各時刻の画像を追加していく\n",
    "    frames.append(self.env.render(mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### agent.get_actionは$\\epsilon$-greedy法により決定されたaction(この例の場合、右か左かのゼロイチ）を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = my_agent.get_action(state, episode)  # 行動を求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actionを与えるとenvがどうなるかを計算してくれる："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行動a_tの実行により、s_{t+1}とdoneフラグを求める\n",
    "# actionから.item()を指定して、中身を取り出す\n",
    "observation_next, _, done, _ = my_env.step(\n",
    "    action.item())  # rewardとinfoは使わないので_にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 終了していたら\n",
    "\n",
    "* episode_10_list の先頭を削除して、末尾に「今回は何ステップ続いたか」を追加\n",
    "* 195ステップ未満だった場合、報酬-1を付与\n",
    "* 195ステップ以上だったら報酬＋１を付与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 報酬を与える。さらにepisodeの終了評価と、state_nextを設定する\n",
    "if done:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\n",
    "    state_next = None  # 次の状態はないので、Noneを格納\n",
    "\n",
    "    # 直近10episodeの立てたstep数リストに追加\n",
    "    episode_10_list = np.hstack(\n",
    "        (episode_10_list[1:], step + 1))\n",
    "\n",
    "    if step < 195:\n",
    "        reward = torch.FloatTensor(\n",
    "            [-1.0])  # 途中でこけたら罰則として報酬-1を与える\n",
    "        complete_episodes = 0  # 連続成功記録をリセット\n",
    "    else:\n",
    "        reward = torch.FloatTensor([1.0])  # 立ったまま終了時は報酬1を与える\n",
    "        complete_episodes = complete_episodes + 1  # 連続記録を更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not done:\n",
    "    reward = torch.FloatTensor([0.0])  # 普段は報酬0\n",
    "    state_next = observation_next  # 観測をそのまま状態とする\n",
    "    state_next = torch.from_numpy(state_next).type(\n",
    "        torch.FloatTensor)  # numpy変数をPyTorchのテンソルに変換\n",
    "    state_next = torch.unsqueeze(state_next, 0)  # size 4をsize 1x4に変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### エージェントが（状態、アクション、次のアクション、報酬）を記憶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# メモリに経験を追加\n",
    "my_agent.memorize(state, action, state_next, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience ReplayでQ関数を更新する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_agent.update_q_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 観測の更新\n",
    "state = state_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 終了時の処理\n",
    "if done:\n",
    "    print('%d Episode: Finished after %d steps：10試行の平均step数 = %.1lf' % (\n",
    "        episode, step + 1, episode_10_list.mean()))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプルがたまった学習済みのbrainを拝借"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_brain = cartpole_env.agent.brain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mini batch取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = my_brain.memory.sample(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Transition(state=tensor([[-0.0474, -0.1739,  0.0059,  0.2582]]), action=tensor([[1]]), next_state=tensor([[-0.0508,  0.0211,  0.0110, -0.0326]]), reward=tensor([0.])),\n",
       " Transition(state=tensor([[ 0.0222, -0.1858, -0.0523,  0.0235]]), action=tensor([[1]]), next_state=tensor([[ 0.0184,  0.0100, -0.0519, -0.2852]]), reward=tensor([0.])),\n",
       " Transition(state=tensor([[-0.2261, -0.5464, -0.0204,  0.2635]]), action=tensor([[1]]), next_state=tensor([[-0.2370, -0.3510, -0.0151, -0.0355]]), reward=tensor([0.]))]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mini batchをtorchが食える形式に変形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 各変数をミニバッチに対応する形に変形\n",
    "# transitionsは1stepごとの(state, action, state_next, reward)が、BATCH_SIZE分格納されている\n",
    "# つまり、(state, action, state_next, reward)×BATCH_SIZE\n",
    "# これをミニバッチにしたい。つまり\n",
    "# (state×BATCH_SIZE, action×BATCH_SIZE, state_next×BATCH_SIZE, reward×BATCH_SIZE)にする\n",
    "batch = Transition(*zip(*transitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0474, -0.1739,  0.0059,  0.2582]]),\n",
       " tensor([[ 0.0222, -0.1858, -0.0523,  0.0235]]),\n",
       " tensor([[-0.2261, -0.5464, -0.0204,  0.2635]]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0474, -0.1739,  0.0059,  0.2582]]),\n",
       " tensor([[ 0.0222, -0.1858, -0.0523,  0.0235]]),\n",
       " tensor([[-0.2261, -0.5464, -0.0204,  0.2635]]))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.state[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 各変数の要素をミニバッチに対応する形に変形し、ネットワークで扱えるようVariableにする\n",
    "# 例えばstateの場合、[torch.FloatTensor of size 1x4]がBATCH_SIZE分並んでいるのですが、\n",
    "# それを torch.FloatTensor of size BATCH_SIZEx4 に変換します\n",
    "# 状態、行動、報酬、non_finalの状態のミニバッチのVariableを作成\n",
    "# catはConcatenates（結合）のことです。\n",
    "state_batch = torch.cat(batch.state)\n",
    "action_batch = torch.cat(batch.action)\n",
    "reward_batch = torch.cat(batch.reward)\n",
    "non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                   if s is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_batch.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論モードへ切り替え"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=4, out_features=32, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------------------\n",
    "# 3. 教師信号となるQ(s_t, a_t)値を求める\n",
    "# -----------------------------------------\n",
    "# 3.1 ネットワークを推論モードに切り替える\n",
    "my_brain.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ネットワークが出力したQ(s_t, a_t)を求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.model(state_batch)は、右左の両方のQ値を出力しており\n",
    "# [torch.FloatTensor of size BATCH_SIZEx2]になっている。\n",
    "# ここから実行したアクションa_tに対応するQ値を求めるため、action_batchで行った行動a_tが右か左かのindexを求め\n",
    "# それに対応するQ値をgatherでひっぱり出す。\n",
    "state_action_values = my_brain.model(state_batch).gather(1, action_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_action_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0811],\n",
       "        [-0.0088],\n",
       "        [-0.1474],\n",
       "        [ 0.1030],\n",
       "        [ 0.1201],\n",
       "        [-0.2402],\n",
       "        [-0.0771],\n",
       "        [ 0.0712],\n",
       "        [-1.0143],\n",
       "        [-0.3686],\n",
       "        [ 0.0666],\n",
       "        [-0.0042],\n",
       "        [-0.1668],\n",
       "        [ 0.0476],\n",
       "        [ 0.0465],\n",
       "        [ 0.1613],\n",
       "        [-0.2929],\n",
       "        [-0.6168],\n",
       "        [ 0.1461],\n",
       "        [ 0.0671],\n",
       "        [-0.2055],\n",
       "        [ 0.0821],\n",
       "        [-0.2846],\n",
       "        [-0.7647],\n",
       "        [ 0.0921],\n",
       "        [ 0.1298],\n",
       "        [-0.5426],\n",
       "        [-0.3158],\n",
       "        [-0.2658],\n",
       "        [-0.0334],\n",
       "        [ 0.1501],\n",
       "        [ 0.0792]], grad_fn=<GatherBackward>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_action_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 1, 1, 1, 1, 1], dtype=torch.uint8)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# まずは全部0にしておく\n",
    "next_state_values = torch.zeros(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### doneでないnext state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0508,  0.0211,  0.0110, -0.0326],\n",
       "        [ 0.0184,  0.0100, -0.0519, -0.2852],\n",
       "        [-0.2370, -0.3510, -0.0151, -0.0355],\n",
       "        [-0.0136,  0.2277, -0.0011, -0.2597],\n",
       "        [ 0.0445,  0.1779,  0.0080, -0.1628],\n",
       "        [-0.3392, -0.5192, -0.0106,  0.0641],\n",
       "        [-0.5775,  0.2264,  0.1267, -0.1718],\n",
       "        [-0.0049, -0.0482,  0.0122, -0.0173],\n",
       "        [-0.0262, -0.3208, -0.1448, -0.2931],\n",
       "        [ 0.0143, -0.1475, -0.0332,  0.1395]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_final_next_states[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### next statenにおける行動価値関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0671,  0.0825],\n",
       "        [-0.0150, -0.0931],\n",
       "        [-0.1746, -0.1917],\n",
       "        [ 0.1026,  0.0694],\n",
       "        [ 0.1074,  0.0962],\n",
       "        [-0.2518, -0.2642],\n",
       "        [-0.0532, -0.0609],\n",
       "        [ 0.0588,  0.0784],\n",
       "        [-0.3069, -0.4145],\n",
       "        [ 0.0250,  0.0737]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_brain.model(non_final_next_states)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### next stateにおける行動価値関数の最大値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0825, -0.0150, -0.1746,  0.1026,  0.1074, -0.2518, -0.0532,  0.0784,\n",
       "         -0.3069,  0.0737,  0.0038, -0.1840,  0.0346,  0.0513,  0.1618, -0.3278,\n",
       "         -0.5720,  0.1650,  0.0815, -0.1134,  0.0891, -0.2461, -0.7838,  0.0817,\n",
       "         -0.3577, -0.2595, -0.0408,  0.1413,  0.0772], grad_fn=<MaxBackward0>),\n",
       " tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "         1, 1, 0, 0, 0]))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_brain.model(non_final_next_states).max(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように最大値と最大値を返すインデックスが返る    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(my_brain.model(non_final_next_states).max(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 次の状態があるindexの最大Q値を求める\n",
    "# 出力にアクセスし、max(1)で列方向の最大値の[値、index]を求めます\n",
    "# そしてそのQ値（index=0）を出力します\n",
    "# detachでその値を取り出します\n",
    "next_state_values[non_final_mask] = my_brain.model(\n",
    "    non_final_next_states).max(1)[0].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0825, -0.0150, -0.1746,  0.1026,  0.1074, -0.2518, -0.0532,  0.0784,\n",
       "         0.0000, -0.3069,  0.0737,  0.0038, -0.1840,  0.0346,  0.0513,  0.1618,\n",
       "        -0.3278, -0.5720,  0.1650,  0.0815, -0.1134,  0.0891, -0.2461, -0.7838,\n",
       "         0.0817,  0.0000,  0.0000, -0.3577, -0.2595, -0.0408,  0.1413,  0.0772])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\uparrow$ next stateがdoneでないsampleに関してはnext stateにおける行動価値関数のmax(最適行動をとったときの値)が、next stateがdoneならゼロが格納されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 679651f] checkpoint\n",
      " 1 file changed, 159 insertions(+), 6 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git commit -a -m \"checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 教師となるQ(s_t, a_t)値を、Q学習の式から求める\n",
    "expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
    "\n",
    "# -----------------------------------------\n",
    "# 4. 結合パラメータの更新\n",
    "# -----------------------------------------\n",
    "# 4.1 ネットワークを訓練モードに切り替える\n",
    "self.model.train()\n",
    "\n",
    "# 4.2 損失関数を計算する（smooth_l1_lossはHuberloss）\n",
    "# expected_state_action_valuesは\n",
    "# sizeが[minbatch]になっているので、unsqueezeで[minibatch x 1]へ\n",
    "loss = F.smooth_l1_loss(state_action_values,\n",
    "                        expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "# 4.3 結合パラメータを更新する\n",
    "self.optimizer.zero_grad()  # 勾配をリセット\n",
    "loss.backward()  # バックプロパゲーションを計算\n",
    "self.optimizer.step()  # 結合パラメータを更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 経験を記憶するメモリオブジェクトを生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_memory = ReplayMemory(CAPACITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ReplayMemory at 0x7f1ec86a88d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラルネットワークを構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = nn.Sequential()\n",
    "my_model.add_module('fc1', nn.Linear(num_states, 32))\n",
    "my_model.add_module('relu1', nn.ReLU())\n",
    "my_model.add_module('fc2', nn.Linear(32, 32))\n",
    "my_model.add_module('relu2', nn.ReLU())\n",
    "my_model.add_module('fc3', nn.Linear(32, num_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最適化手法の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_optimizer = optim.Adam(my_model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "209px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "1421px",
    "left": "0px",
    "right": "1669.6px",
    "top": "67px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
