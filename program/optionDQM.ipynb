{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 51ab998] checkpoint\n",
      " 1 file changed, 570 insertions(+), 592 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "git commit -a -m \"checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#パッケージのimport\" data-toc-modified-id=\"パッケージのimport-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>パッケージのimport</a></div><div class=\"lev1 toc-item\"><a href=\"#namedtuple\" data-toc-modified-id=\"namedtuple-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>namedtuple</a></div><div class=\"lev1 toc-item\"><a href=\"#定数の設定\" data-toc-modified-id=\"定数の設定-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>定数の設定</a></div><div class=\"lev1 toc-item\"><a href=\"#経験を保存するメモリクラスを定義します\" data-toc-modified-id=\"経験を保存するメモリクラスを定義します-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>経験を保存するメモリクラスを定義します</a></div><div class=\"lev2 toc-item\"><a href=\"#sandbox\" data-toc-modified-id=\"sandbox-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>sandbox</a></div><div class=\"lev1 toc-item\"><a href=\"#brainstorming\" data-toc-modified-id=\"brainstorming-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>brainstorming</a></div><div class=\"lev1 toc-item\"><a href=\"#Brain\" data-toc-modified-id=\"Brain-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Brain</a></div><div class=\"lev1 toc-item\"><a href=\"#Agent\" data-toc-modified-id=\"Agent-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Agent</a></div><div class=\"lev1 toc-item\"><a href=\"#pricer\" data-toc-modified-id=\"pricer-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>pricer</a></div><div class=\"lev2 toc-item\"><a href=\"#class-version-zero\" data-toc-modified-id=\"class-version-zero-81\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>class version zero</a></div><div class=\"lev1 toc-item\"><a href=\"#replacing-step()-and-state-initialization\" data-toc-modified-id=\"replacing-step()-and-state-initialization-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>replacing step() and state initialization</a></div><div class=\"lev2 toc-item\"><a href=\"#sandbox\" data-toc-modified-id=\"sandbox-91\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>sandbox</a></div><div class=\"lev1 toc-item\"><a href=\"#Environment改（大幅改修した）\" data-toc-modified-id=\"Environment改（大幅改修した）-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Environment改（大幅改修した）</a></div><div class=\"lev2 toc-item\"><a href=\"#Version-1\" data-toc-modified-id=\"Version-1-101\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Version 1</a></div><div class=\"lev3 toc-item\"><a href=\"#To-be-determined\" data-toc-modified-id=\"To-be-determined-1011\"><span class=\"toc-item-num\">10.1.1&nbsp;&nbsp;</span>To be determined</a></div><div class=\"lev3 toc-item\"><a href=\"#implementation\" data-toc-modified-id=\"implementation-1012\"><span class=\"toc-item-num\">10.1.2&nbsp;&nbsp;</span>implementation</a></div><div class=\"lev1 toc-item\"><a href=\"#predict\" data-toc-modified-id=\"predict-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>predict</a></div><div class=\"lev1 toc-item\"><a href=\"#sandbox\" data-toc-modified-id=\"sandbox-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>sandbox</a></div><div class=\"lev1 toc-item\"><a href=\"#参考（書籍のoriginal-Environment）\" data-toc-modified-id=\"参考（書籍のoriginal-Environment）-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>参考（書籍のoriginal Environment）</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "** 5.3、5.4  PyTorchでDQN **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パッケージのimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本コードでは、namedtupleを使用します。\n",
    "\n",
    "namedtupleを使うことで、値をフィールド名とペアで格納できます。\n",
    "\n",
    "すると値に対して、フィールド名でアクセスできて便利です。\n",
    "\n",
    "https://docs.python.jp/3/library/collections.html#collections.namedtuple\n",
    "\n",
    "以下は使用例です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr(name_a='名前Aです', value_b=100)\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Tr = namedtuple('tr', ('name_a', 'value_b'))\n",
    "Tr_object = Tr('名前Aです', 100)\n",
    "\n",
    "print(Tr_object)  # 出力：tr(name_a='名前Aです', value_b=100)\n",
    "print(Tr_object.value_b)  # 出力：100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tr_object.name_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "namedtupleを生成\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ('s', 't', 'action', 'next_s' , \"next_t\", 'reward', 'done'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#namedtupleを生成\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'next_state', 'reward'))#,'done'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_price = 127.62\n",
    "volatility = 0.20 # the historical vols or implied vols\n",
    "dividend_rate =  0.0163\n",
    "risk_free_rate = 0.001\n",
    "maturity = 0.1\n",
    "dt = 0.1\n",
    "\n",
    "strike_price = 130\n",
    "\n",
    "#steps = 200\n",
    "pricer_steps = 100\n",
    "\n",
    "#dt = maturity / steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENV = 'CartPole-v0'  # 使用する課題名\n",
    "GAMMA = np.exp(-dt * risk_free_rate)  # 時間割引率\n",
    "MAX_STEPS = 200  # 1試行のstep数\n",
    "#NUM_EPISODES = 500  # 最大試行回数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 経験を保存するメモリクラスを定義します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 概ね書籍のままの実装でいけるはず（？）$\\Rightarrow$ doneを追加した$\\Rightarrow$やっぱりやめた。（終了判定の方を変更するほうが素直なため）\n",
    "* ただし、おそらくサンプル取得時に完全にランダムにしないで3項ツリー的な３つのnext stateをまとめてmini batfchに含めたほうがいいのではないかという気がする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, CAPACITY):\n",
    "        self.capacity = CAPACITY  # メモリの最大長さ\n",
    "        self.memory = []  # 経験を保存する変数\n",
    "        self.index = 0  # 保存するindexを示す変数\n",
    "\n",
    "    def push(self, state, action, state_next, reward):# , done):\n",
    "        '''transition = (state, action, state_next, reward)をメモリに保存する'''\n",
    "\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)  # メモリが満タンでないときは足す\n",
    "\n",
    "        # namedtupleのTransitionを使用し、値とフィールド名をペアにして保存します\n",
    "        self.memory[self.index] = Transition(state, action, state_next, reward)# , done)\n",
    "\n",
    "        self.index = (self.index + 1) % self.capacity  # 保存するindexを1つずらす\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''batch_size分だけ、ランダムに保存内容を取り出す'''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''関数lenに対して、現在の変数memoryの長さを返す'''\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = ReplayMemory(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tmp.push(100,0.1, 0 , 101)#,  True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# brainstorming\n",
    "\n",
    "* 素直な実装ではnum_statesは２となる。（$S$および$t$）\n",
    "* brainもたぶん書籍のままの実装でいける？？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain\n",
    "\n",
    "エージェントが持つ脳となるクラスです、DQNを実行します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "methodは\n",
    "\n",
    "* replay: Experience Replayでネットワークの結合パラメータを学習\n",
    "* decide_action: アクション決定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q関数をディープラーニングのネットワークをクラスとして定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "CAPACITY = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* もともと次がdoneの場合は学習に使っていない.?と思ったがそんなことはなかった。\n",
    "* そのままだと学習が正しく進むわけがないので修正していく、というのは勘違いでここはいじらなくてよかった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions  # CartPoleの行動（右に左に押す）の2を取得\n",
    "\n",
    "        # 経験を記憶するメモリオブジェクトを生成\n",
    "        self.memory = ReplayMemory(CAPACITY)\n",
    "\n",
    "        # ニューラルネットワークを構築\n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add_module('fc1', nn.Linear(num_states, 144))\n",
    "        self.model.add_module('relu1', nn.ReLU())\n",
    "        #self.model.add_module('fc2', nn.Linear(32, 32))\n",
    "        #self.model.add_module('relu2', nn.ReLU())\n",
    "        self.model.add_module('fc3', nn.Linear(144, num_actions))\n",
    "\n",
    "        print(self.model)  # ネットワークの形を出力\n",
    "        print(list(self.model.parameters()))\n",
    "\n",
    "        # 最適化手法の設定\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "\n",
    "    def replay(self):\n",
    "        '''Experience Replayでネットワークの結合パラメータを学習'''\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 1. メモリサイズの確認\n",
    "        # -----------------------------------------\n",
    "        # 1.1 メモリサイズがミニバッチより小さい間は何もしない\n",
    "        #print(\"memory .size : {}\".format(len(self.memory)))\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2. ミニバッチの作成\n",
    "        # -----------------------------------------\n",
    "        # 2.1 メモリからミニバッチ分のデータを取り出す\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        # 2.2 各変数をミニバッチに対応する形に変形\n",
    "        # transitionsは1stepごとの(state, action, state_next, reward)が、BATCH_SIZE分格納されている\n",
    "        # つまり、(state, action, state_next, reward)×BATCH_SIZE\n",
    "        # これをミニバッチにしたい。つまり\n",
    "        # (state×BATCH_SIZE, action×BATCH_SIZE, state_next×BATCH_SIZE, reward×BATCH_SIZE)にする\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # 2.3 各変数の要素をミニバッチに対応する形に変形し、ネットワークで扱えるようVariableにする\n",
    "        # 例えばstateの場合、[torch.FloatTensor of size 1x4]がBATCH_SIZE分並んでいるのですが、\n",
    "        # それを torch.FloatTensor of size BATCH_SIZEx4 に変換します\n",
    "        # 状態、行動、報酬、non_finalの状態のミニバッチのVariableを作成\n",
    "        # catはConcatenates（結合）のことです。\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        #done_batch = torch.cat(batch.done)\n",
    "        try:\n",
    "            non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                           if s is not None])\n",
    "        except:\n",
    "            print()\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 3. 教師信号となるQ(s_t, a_t)値を求める\n",
    "        # -----------------------------------------\n",
    "        # 3.1 ネットワークを推論モードに切り替える\n",
    "        self.model.eval()\n",
    "\n",
    "        # 3.2 ネットワークが出力したQ(s_t, a_t)を求める\n",
    "        # self.model(state_batch)は、右左の両方のQ値を出力しており\n",
    "        # [torch.FloatTensor of size BATCH_SIZEx2]になっている。\n",
    "        # ここから実行したアクションa_tに対応するQ値を求めるため、action_batchで行った行動a_tが右か左かのindexを求め\n",
    "        # それに対応するQ値をgatherでひっぱり出す。\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # 3.3 max{Q(s_t+1, a)}値を求める。ただし次の状態があるかに注意。\n",
    "\n",
    "        # cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成\n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None,\n",
    "                                                    batch.next_state)))\n",
    "        # まずは全部0にしておく\n",
    "        next_state_values = torch.zeros(BATCH_SIZE)\n",
    "\n",
    "        # 次の状態があるindexの最大Q値を求める\n",
    "        # 出力にアクセスし、max(1)で列方向の最大値の[値、index]を求めます\n",
    "        # そしてそのQ値（index=0）を出力します\n",
    "        # detachでその値を取り出します\n",
    "        next_state_values[non_final_mask] = self.model(\n",
    "            non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "        # 3.4 教師となるQ(s_t, a_t)値を、Q学習の式から求める\n",
    "        expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 4. 結合パラメータの更新\n",
    "        # -----------------------------------------\n",
    "        # 4.1 ネットワークを訓練モードに切り替える\n",
    "        self.model.train()\n",
    "\n",
    "        # 4.2 損失関数を計算する（smooth_l1_lossはHuberloss）\n",
    "        # expected_state_action_valuesは\n",
    "        # sizeが[minbatch]になっているので、unsqueezeで[minibatch x 1]へ\n",
    "        loss = F.smooth_l1_loss(state_action_values,\n",
    "                                expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # 4.3 結合パラメータを更新する\n",
    "        self.optimizer.zero_grad()  # 勾配をリセット\n",
    "        loss.backward()  # バックプロパゲーションを計算\n",
    "        self.optimizer.step()  # 結合パラメータを更新\n",
    "\n",
    "    def decide_action(self, state, episode):\n",
    "        '''現在の状態に応じて、行動を決定する'''\n",
    "        # ε-greedy法で徐々に最適行動のみを採用する\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            self.model.eval()  # ネットワークを推論モードに切り替える\n",
    "            with torch.no_grad():\n",
    "                action = self.model(state).max(1)[1].view(1, 1)\n",
    "            # ネットワークの出力の最大値のindexを取り出します = max(1)[1]\n",
    "            # .view(1,1)は[torch.LongTensor of size 1]　を size 1x1 に変換します\n",
    "\n",
    "        else:\n",
    "            # 0,1の行動をランダムに返す\n",
    "            action = torch.LongTensor(\n",
    "                [[random.randrange(self.num_actions)]])  # 0,1の行動をランダムに返す\n",
    "            # actionは[torch.LongTensor of size 1x1]の形になります\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "CartPoleで動くエージェントクラスです、棒付き台車そのものになります\n",
    "\n",
    "methodは\n",
    "\n",
    "\n",
    "* 行動価値Qを更新\n",
    "*  状態を与えると行動を決定\n",
    "*  状態、選択するアクション、次の状態、報酬などを記憶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        '''課題の状態と行動の数を設定する'''\n",
    "        self.brain = Brain(num_states, num_actions)  # エージェントが行動を決定するための頭脳を生成\n",
    "\n",
    "    def update_q_function(self):\n",
    "        '''Q関数を更新する'''\n",
    "        self.brain.replay()\n",
    "\n",
    "    def get_action(self, state, episode):\n",
    "        '''行動を決定する'''\n",
    "        action = self.brain.decide_action(state, episode)\n",
    "        return action\n",
    "\n",
    "    def memorize(self, state, action, state_next, reward): #, done):\n",
    "        '''memoryオブジェクトに、state, action, state_next, rewardの内容を保存する'''\n",
    "        self.brain.memory.push(state, action, state_next, reward)# , done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pricer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import QuantLib as ql "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class version zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cl_am_call:\n",
    "    def price(self , spot_price, strike_price, maturity):\n",
    "        if maturity <= 0:\n",
    "            exercise_value = np.max([0 , spot_price - strike_price])\n",
    "            return (exercise_value)\n",
    "        dummy_strike = strike_price / spot_price\n",
    "        \n",
    "\n",
    "        option_type = ql.Option.Call\n",
    "        payoff = ql.PlainVanillaPayoff(option_type, dummy_strike)\n",
    "        \n",
    "        maturity_date = self.calculation_date + int(365.0 * maturity)\n",
    "        settlement = self.calculation_date\n",
    "        am_exercise = ql.AmericanExercise(settlement, maturity_date)\n",
    "        \n",
    "        american_option = ql.VanillaOption(payoff, am_exercise)\n",
    "        american_option.setPricingEngine(self.binomial_engine)\n",
    "        ql.Settings.instance().evaluationDate = self.calculation_date\n",
    "\n",
    "        return (american_option.NPV() * spot_price)\n",
    "        \n",
    "\n",
    "    def __init__(self , volatility , dividend_rate , risk_free_rate  ,steps):\n",
    "        day_count = ql.Actual365Fixed()\n",
    "        #calendar = ql.UnitedStates()\n",
    "        calendar = ql.Japan()\n",
    "        self.calculation_date = ql.Date(8, 5, 2015)\n",
    "        dummy_spot = 1\n",
    "        \n",
    "        self.spot_handle = ql.QuoteHandle(ql.SimpleQuote(1.0))\n",
    "\n",
    "        ql.Settings.instance().evaluationDate = self.calculation_date\n",
    "\n",
    "\n",
    "        self.flat_ts = ql.YieldTermStructureHandle(\n",
    "            ql.FlatForward(self.calculation_date, risk_free_rate, day_count)\n",
    "        )\n",
    "\n",
    "        self.dividend_yield = ql.YieldTermStructureHandle(\n",
    "            ql.FlatForward(self.calculation_date, dividend_rate, day_count)\n",
    "        )\n",
    "\n",
    "        #### volatility\n",
    "\n",
    "        self.flat_vol_ts = ql.BlackVolTermStructureHandle(\n",
    "            ql.BlackConstantVol(self.calculation_date, calendar, volatility, day_count)\n",
    "        )\n",
    "\n",
    "        #### BS framework\n",
    "\n",
    "        self.bsm_process = ql.BlackScholesMertonProcess(self.spot_handle, \n",
    "                                                   self.dividend_yield, \n",
    "                                                   self.flat_ts, \n",
    "                                                   self.flat_vol_ts)\n",
    "\n",
    "\n",
    "        self.binomial_engine = ql.BinomialVanillaEngine(self.bsm_process, \"crr\", steps)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_call = cl_am_call(volatility ,  dividend_rate ,  risk_free_rate , pricer_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_call.price(spot_price, strike_price, maturity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_call.price(1 , 100, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replacing step() and state initialization\n",
    "\n",
    "* initializationをどうするかな・・\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  decision2next_state:\n",
    "    \n",
    "    def __init__(self ,  S0 , volatility , dividend_rate , risk_free_rate  , dt , Maturity):\n",
    "        self.S0 = S0\n",
    "        self.volatility = volatility\n",
    "        self.dividend_rate = dividend_rate\n",
    "        self.risk_free_rate = risk_free_rate\n",
    "        self.dt = dt\n",
    "        self.Maturity = Maturity\n",
    "        \n",
    "        self.barbeta_dt = (risk_free_rate - dividend_rate - volatility**2 * 0.5) * dt\n",
    "        self.sd = volatility * np.sqrt(dt)\n",
    "    \n",
    "    def reset(self):\n",
    "        T0 = 0    \n",
    "        if True:\n",
    "            state0 = np.array([self.S0 , T0])\n",
    "        else :\n",
    "            state0 = torch.Tensor(np.array([[self.S0 , T0]]))\n",
    "        return state0\n",
    "        \n",
    "    \n",
    "    def step(self , state , action):\n",
    "             #stateはtorch.tensor torch.Size([1, 2])\n",
    "            if False:\n",
    "                #next_state = state.copy()\n",
    "                S = state[0]\n",
    "                T = state[1]\n",
    "            else:               \n",
    "                #next_state = state.clone()\n",
    "                S = state[0][0].item()\n",
    "                T = state[0][1].item()    \n",
    "            T_next = T + self.dt\n",
    "            S_return = self.barbeta_dt + self.sd * np.random.randn()  \n",
    "            \n",
    "            S_next = S * np.exp(S_return)\n",
    "            if True:\n",
    "                state_next =np.array([S_next , T_next])\n",
    "            else:\n",
    "                state_next = torch.Tensor(np.array([[S_next , T_next]]))\n",
    "            done = (action == 1) or (T >= self.Maturity) \n",
    "            if done:\n",
    "                S_next = None\n",
    "            return state_next , done\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stepper = decision2next_state(spot_price, volatility , dividend_rate , risk_free_rate , 0.1 , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_state = torch.Tensor(np.array([[100 , 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_state = np.array([100,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tmp_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stepper.step(tmp_state , 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stepper.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment改（大幅改修した）\n",
    "CartPoleを実行する環境のクラスです\n",
    "\n",
    "* for loopは二重になっている\n",
    "* 外側のループはEPISODEに関して\n",
    "    * Episode = 試行：一回ポールを立てて倒れるか200ステップ経過するまでを１エピソードと数える\n",
    "    * 内側のループはステップに関して\n",
    "        * 初期状態のBrainを使って、1ステップ目から左右にコントロールしていくことからスタート\n",
    "        * 各ステップごとに状態と遷移を記録する。\n",
    "        * 同様に各ステップごとに行動価値関数をアップデートしていく\n",
    "    * 10エピソード連続で200ステップまで持ちこたえられたら成功\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1\n",
    "* まずはMCパスを発生させて、episode $\\approx$ pathであるような形でやってみよう\n",
    "* 後々、MCパスはtrinomial tree上のみを推移するようにするかもしれないが、ここでは素直にMCパスを普通に発生させてみよう\n",
    "    * MCパス発生はquantlibにやらせてもいいが自分で実装してしまってもいいかな\n",
    "* env.stepを\n",
    "```\n",
    " 1 time step推進\n",
    "```\n",
    "に置き換える。\n",
    "\n",
    "\n",
    "### To be determined\n",
    "* QLはここに取り込む？ $\\Rightarrow$ maybe yes\n",
    "* MCもここで？ $\\Rightarrow$ maybe yes\n",
    "    * gymの場合は時間推進はgymが面倒見てくれていた。そのgymはEnvironmentクラスのメンバーになっている。\n",
    "* 書籍にあったような20回連続でみたいな終了判定基準はもはや適切ではない。ではどのような終了判定基準が良いか\n",
    "```\n",
    "現状では正解がわかっているのでいろいろズルをしよう。例えば、10パス連続で最適行使の判定を正解できたときetc\n",
    "```\n",
    "* reward設計\n",
    "```\n",
    "行使した場合にはrewardを払って行動価値関数がゼロのnext stateに飛ぶ\n",
    "```\n",
    "とする."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import QuantLib as ql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEnvironment:\n",
    "\n",
    "    def __init__(self, S0 , vol , q , r , K , T , dt ,pricer_steps):\n",
    "        #self.env = gym.make(ENV)  # 実行する課題を設定\n",
    "        self.S0 = S0\n",
    "        self.vol = vol\n",
    "        self.q = q\n",
    "        self.r = r\n",
    "        self.K = K\n",
    "        self.T = T\n",
    "        self.dt = dt\n",
    "        self.stepper = decision2next_state(S0 , vol , q , r , dt, T)\n",
    "        \n",
    "        num_states = 2# S and t\n",
    "        num_actions = 2 # exercise or hold \n",
    "        self.agent = Agent(num_states, num_actions)  # 環境内で行動するAgentを生成\n",
    "        self.pricer = cl_am_call(vol ,  q ,  r , steps = pricer_steps)\n",
    "\n",
    "        \n",
    "    def run(self , n_episodes , is_silent = False):\n",
    "        '''実行'''\n",
    "        episode_10_list = np.zeros(10)  # 10試行分の立ち続けたstep数を格納し、平均ステップ数を出力に利用\n",
    "        #complete episodesを終了条件にするのはもはや適切ではない\n",
    "        complete_episodes = 0  # 195step以上連続で立ち続けた試行数\n",
    "        episode_final = False  # 最後の試行フラグ\n",
    "        frames = []  # 最後の試行を動画にするために画像を格納する変数\n",
    "\n",
    "        for episode in range(n_episodes):  # 最大試行数分繰り返す\n",
    "            #observation = self.env.reset()  # 環境の初期化\n",
    "            observation = self.stepper.reset()\n",
    "            if episode % 200 == 0:\n",
    "                print(\"finished {} episodes\".format(episode))\n",
    "\n",
    "            state = observation  # 観測をそのまま状態sとして使用\n",
    "            if True:\n",
    "                #print(type(state))\n",
    "                state = torch.from_numpy(state).type(torch.FloatTensor)  # NumPy変数をPyTorchのテンソルに変換\n",
    "                state = torch.unsqueeze(state, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "            for step in range(MAX_STEPS):  # 1エピソードのループ\n",
    "\n",
    "                action = self.agent.get_action(state, episode)  # 行動を求める。\n",
    "                ### * episodeを食わせるのはQ学習の定義を見れば納得できる。ここは書籍のままの\n",
    "        \n",
    "                \n",
    "                # 行動a_tの実行により、s_{t+1}とdoneフラグを求める\n",
    "                # actionから.item()を指定して、中身を取り出す\n",
    " \n",
    "                observation_next , done = self.stepper.step(state , action.item())\n",
    "                #print(\"done is {}\".format(done))\n",
    "\n",
    "                # 報酬を与える。さらにepisodeの終了評価と、state_nextを設定する\n",
    "                if done:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\n",
    "                    state_next = None  # 次の状態はないので、Noneを格納\n",
    "       \n",
    "\n",
    "                    # 直近10episodeの立てたstep数リストに追加\n",
    "                    episode_10_list = np.hstack(\n",
    "                        (episode_10_list[1:], step + 1))\n",
    "                    if True:\n",
    "                        exercise_value = state[0][0].item() - self.K   \n",
    "                        if state[0][1].item() >= self.T:\n",
    "                            exercise_value = np.max([exercise_value , 0.0])\n",
    "                        \n",
    "                        reward = torch.FloatTensor([exercise_value]) \n",
    "                        complete_episodes = complete_episodes + 1  ## * 暫定的な処理\n",
    "                    \n",
    "                    else:\n",
    "\n",
    "                        if step < 195:\n",
    "                            reward = torch.FloatTensor(\n",
    "                                [-1.0])  # 途中でこけたら罰則として報酬-1を与える\n",
    "                            complete_episodes = 0  # 連続成功記録をリセット\n",
    "                        else:\n",
    "                            reward = torch.FloatTensor([1.0])  # 立ったまま終了時は報酬1を与える\n",
    "                            complete_episodes = complete_episodes + 1  # 連続記録を更新\n",
    "                else:\n",
    "                    reward = torch.FloatTensor([0.0])  # 普段は報酬0\n",
    "                    state_next = observation_next  # 観測をそのまま状態とする\n",
    "                    if True:\n",
    "                        state_next = torch.from_numpy(state_next).type(\n",
    "                        torch.FloatTensor)  # numpy変数をPyTorchのテンソルに変換\n",
    "                        state_next = torch.unsqueeze(state_next, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "                # メモリに経験を追加\n",
    "                self.agent.memorize(state, action, state_next, reward)\n",
    "\n",
    "                # Experience ReplayでQ関数を更新する\n",
    "                try:\n",
    "                    self.agent.update_q_function()\n",
    "                except:\n",
    "                    print(\"something is wrong\")\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "                # 終了時の処理\n",
    "                if done:\n",
    "                    #print('%d Episode: Finished after %d steps：10試行の平均step数 = %.1lf' % (\n",
    "                    #    episode, step + 1, episode_10_list.mean()))\n",
    "                    #print(\"exercise value is {}\".format(exercise_value))\n",
    "                    #print(type(state))\n",
    "\n",
    "                    error = exercise_value - self.pricer.price(state[0][0].item(), self.K , self.T - state[0][1].item())\n",
    "                    if not is_silent:\n",
    "                        print(\"S : {:.2f} / t : {:.2f} / exercise value : {:.2f} / error : {:.2f}\".format(state[0][0].item() , state[0][1].item() , exercise_value , error))\n",
    "                    break\n",
    "                    ## ** error \n",
    "                                # 観測の更新\n",
    "                state = state_next\n",
    "            if episode_final:\n",
    "                break\n",
    "\n",
    "            # 10連続で200step経ち続けたら成功\n",
    "            if complete_episodes >= n_episodes:\n",
    "                #print('10回連続成功')\n",
    "                episode_final = True  # 次の試行を描画を行う最終試行とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (fc1): Linear(in_features=2, out_features=144, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc3): Linear(in_features=144, out_features=2, bias=True)\n",
      ")\n",
      "[Parameter containing:\n",
      "tensor([[-0.1102,  0.0035],\n",
      "        [ 0.5662, -0.1123],\n",
      "        [ 0.1315, -0.0650],\n",
      "        [-0.3551,  0.6924],\n",
      "        [-0.2900, -0.5138],\n",
      "        [ 0.1834, -0.4382],\n",
      "        [-0.0530, -0.2082],\n",
      "        [-0.1559,  0.3539],\n",
      "        [-0.0784,  0.6325],\n",
      "        [-0.6977,  0.0419],\n",
      "        [-0.3498, -0.2602],\n",
      "        [-0.6266, -0.0454],\n",
      "        [ 0.4907,  0.3227],\n",
      "        [ 0.2133, -0.2530],\n",
      "        [-0.0333, -0.3682],\n",
      "        [-0.3158, -0.5012],\n",
      "        [ 0.0977, -0.3502],\n",
      "        [-0.4339, -0.5088],\n",
      "        [ 0.5344,  0.0107],\n",
      "        [ 0.0657,  0.5568],\n",
      "        [-0.3610, -0.4328],\n",
      "        [-0.6095, -0.1023],\n",
      "        [ 0.6239, -0.6690],\n",
      "        [ 0.1790,  0.3228],\n",
      "        [-0.2319, -0.5122],\n",
      "        [ 0.4858,  0.3932],\n",
      "        [-0.4790,  0.5999],\n",
      "        [-0.4623,  0.1708],\n",
      "        [ 0.5893, -0.2254],\n",
      "        [ 0.1323,  0.6698],\n",
      "        [-0.0409,  0.4436],\n",
      "        [-0.4229, -0.4987],\n",
      "        [-0.0339, -0.4895],\n",
      "        [ 0.1786,  0.4481],\n",
      "        [ 0.2683,  0.2329],\n",
      "        [-0.2490,  0.5685],\n",
      "        [-0.4142, -0.6950],\n",
      "        [ 0.5602,  0.3524],\n",
      "        [ 0.1261, -0.3246],\n",
      "        [ 0.6869,  0.2676],\n",
      "        [ 0.4804,  0.4750],\n",
      "        [ 0.0379, -0.4080],\n",
      "        [ 0.5938,  0.4944],\n",
      "        [-0.1348,  0.1515],\n",
      "        [ 0.6953,  0.3289],\n",
      "        [ 0.0735,  0.4229],\n",
      "        [ 0.5180,  0.5299],\n",
      "        [-0.5105,  0.0358],\n",
      "        [-0.2693,  0.3430],\n",
      "        [-0.4961, -0.3023],\n",
      "        [-0.6529,  0.6847],\n",
      "        [-0.0095, -0.0078],\n",
      "        [ 0.3096, -0.2632],\n",
      "        [ 0.3928, -0.5453],\n",
      "        [ 0.0691, -0.4908],\n",
      "        [-0.0605,  0.6569],\n",
      "        [ 0.5966, -0.6886],\n",
      "        [ 0.1893, -0.0096],\n",
      "        [-0.0353, -0.2979],\n",
      "        [ 0.5004, -0.5107],\n",
      "        [ 0.6367,  0.3694],\n",
      "        [ 0.1963,  0.3551],\n",
      "        [ 0.1777,  0.4756],\n",
      "        [ 0.5081, -0.5760],\n",
      "        [ 0.4310,  0.1864],\n",
      "        [ 0.5858,  0.3946],\n",
      "        [-0.0834,  0.5352],\n",
      "        [-0.3754,  0.0196],\n",
      "        [ 0.2409, -0.2519],\n",
      "        [ 0.0218, -0.5975],\n",
      "        [-0.3615, -0.5947],\n",
      "        [ 0.2261, -0.4775],\n",
      "        [-0.4735, -0.1396],\n",
      "        [ 0.6821,  0.6048],\n",
      "        [-0.0225, -0.5116],\n",
      "        [-0.3968, -0.7010],\n",
      "        [-0.6065,  0.2889],\n",
      "        [ 0.6284, -0.4884],\n",
      "        [ 0.2365,  0.3679],\n",
      "        [-0.1987, -0.4336],\n",
      "        [-0.5226, -0.4359],\n",
      "        [ 0.2572, -0.0084],\n",
      "        [ 0.3664, -0.2519],\n",
      "        [-0.0331, -0.0882],\n",
      "        [-0.1614,  0.1908],\n",
      "        [-0.7050, -0.5474],\n",
      "        [-0.5488,  0.3889],\n",
      "        [-0.3677,  0.1734],\n",
      "        [ 0.3944,  0.1466],\n",
      "        [-0.5026, -0.1997],\n",
      "        [-0.4672, -0.6472],\n",
      "        [-0.5696, -0.7029],\n",
      "        [-0.3793, -0.2827],\n",
      "        [-0.3605, -0.5696],\n",
      "        [ 0.4687, -0.0038],\n",
      "        [-0.6672,  0.3824],\n",
      "        [-0.5256,  0.0064],\n",
      "        [-0.5509, -0.5378],\n",
      "        [-0.5810,  0.1364],\n",
      "        [-0.6385,  0.5476],\n",
      "        [ 0.6477, -0.0611],\n",
      "        [ 0.4453,  0.2916],\n",
      "        [ 0.7030, -0.7064],\n",
      "        [-0.0056,  0.3316],\n",
      "        [-0.5500, -0.6397],\n",
      "        [ 0.0471, -0.4518],\n",
      "        [ 0.0572,  0.2863],\n",
      "        [-0.0323, -0.2398],\n",
      "        [-0.2051, -0.3790],\n",
      "        [-0.6685,  0.3222],\n",
      "        [ 0.6344, -0.5392],\n",
      "        [ 0.0785, -0.2805],\n",
      "        [-0.3360, -0.3173],\n",
      "        [ 0.1170, -0.2065],\n",
      "        [ 0.6259, -0.5963],\n",
      "        [ 0.5143, -0.1047],\n",
      "        [-0.0488,  0.4032],\n",
      "        [-0.4315, -0.0714],\n",
      "        [-0.1827,  0.6294],\n",
      "        [-0.3085, -0.6467],\n",
      "        [ 0.6259,  0.6750],\n",
      "        [ 0.2718,  0.5145],\n",
      "        [ 0.6685, -0.3291],\n",
      "        [ 0.6917, -0.0936],\n",
      "        [ 0.6979, -0.0705],\n",
      "        [-0.1323,  0.6826],\n",
      "        [ 0.5276,  0.6650],\n",
      "        [ 0.0236,  0.5823],\n",
      "        [-0.4635, -0.3393],\n",
      "        [ 0.6338, -0.4152],\n",
      "        [-0.2083,  0.5029],\n",
      "        [-0.5260, -0.1635],\n",
      "        [ 0.0205, -0.1674],\n",
      "        [-0.2297, -0.0092],\n",
      "        [ 0.1965, -0.4962],\n",
      "        [-0.4821,  0.4231],\n",
      "        [-0.4985,  0.6435],\n",
      "        [-0.5836, -0.2443],\n",
      "        [ 0.0335, -0.3992],\n",
      "        [ 0.2079,  0.2862],\n",
      "        [ 0.0393,  0.3354],\n",
      "        [-0.1508,  0.2619],\n",
      "        [-0.6877, -0.0302],\n",
      "        [ 0.1490,  0.1696]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.2436,  0.5837, -0.4708, -0.4539,  0.0254, -0.4423, -0.4752, -0.6458,\n",
      "        -0.2461,  0.5767,  0.0727, -0.4611,  0.6961,  0.4940, -0.5067,  0.2556,\n",
      "        -0.1486, -0.6323, -0.3739, -0.1701, -0.3889,  0.4936,  0.6902, -0.6780,\n",
      "         0.4979,  0.6655, -0.2270, -0.3294,  0.1860,  0.3108, -0.1464,  0.2446,\n",
      "        -0.2831, -0.5191, -0.1830, -0.5791,  0.0817,  0.4924,  0.6036,  0.3869,\n",
      "         0.1402, -0.4306,  0.0275,  0.5817,  0.4937,  0.4407,  0.0686, -0.6901,\n",
      "        -0.2585,  0.1734,  0.5544,  0.6775,  0.3102,  0.0124, -0.0907, -0.1328,\n",
      "         0.6314,  0.7045, -0.2922,  0.6737, -0.3893,  0.1365,  0.5101,  0.5948,\n",
      "         0.1238,  0.1336,  0.1670,  0.2819, -0.2982, -0.6458, -0.5477, -0.5077,\n",
      "         0.3990, -0.0067, -0.6020,  0.5703, -0.6227, -0.4708,  0.5065,  0.6712,\n",
      "        -0.6357,  0.2249,  0.3379,  0.0703,  0.0746,  0.6561,  0.4127,  0.1903,\n",
      "        -0.5322,  0.6437, -0.5347, -0.5585, -0.0447,  0.5498,  0.3432,  0.1654,\n",
      "         0.3848,  0.1763, -0.1651, -0.3573, -0.2801,  0.3698, -0.5581,  0.6769,\n",
      "         0.0514, -0.1989, -0.6202,  0.5534, -0.6661,  0.6563,  0.5365, -0.3152,\n",
      "        -0.3984, -0.1991, -0.4766,  0.6640,  0.1045, -0.0535,  0.3002, -0.0983,\n",
      "         0.1131, -0.6327, -0.1145, -0.5332,  0.6605,  0.0420,  0.0409,  0.1782,\n",
      "        -0.6242, -0.5512, -0.6959, -0.6618, -0.0287,  0.1751,  0.3128,  0.4305,\n",
      "        -0.4267, -0.3408, -0.0549,  0.5079,  0.4979, -0.6063, -0.6726, -0.6271],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-4.5062e-02, -8.2427e-02, -3.3119e-02,  8.9978e-03,  3.4057e-03,\n",
      "          8.1633e-02,  7.7562e-02, -7.0925e-02,  5.1951e-02, -5.8360e-02,\n",
      "         -2.7411e-02,  6.6866e-02, -5.7456e-02,  1.9010e-02,  1.8906e-02,\n",
      "         -6.0569e-03,  8.0750e-02, -3.8220e-02,  6.6340e-02,  8.2887e-02,\n",
      "          5.1300e-02, -6.7852e-02, -6.2876e-03, -1.6460e-03, -8.2027e-02,\n",
      "          1.6950e-02,  5.4274e-02, -2.9260e-02,  2.6126e-02, -7.0248e-02,\n",
      "         -7.0208e-02, -5.0534e-02,  7.6664e-02, -2.0599e-02, -8.2865e-02,\n",
      "         -4.1912e-02,  4.0595e-02, -5.3176e-02, -2.1993e-02,  7.0028e-02,\n",
      "          1.1792e-02, -6.2492e-02,  2.0007e-02, -7.5362e-02,  3.7701e-02,\n",
      "         -2.5617e-02, -3.8167e-02,  4.2228e-02,  4.9665e-02,  6.8226e-02,\n",
      "         -1.6083e-03, -4.0074e-02, -7.1455e-02, -1.2676e-02,  8.1286e-06,\n",
      "          6.0118e-02,  5.9438e-03,  7.3951e-02,  5.6607e-02,  7.0294e-02,\n",
      "         -7.4195e-02, -6.9025e-02, -3.9728e-02,  1.7315e-02,  4.4653e-02,\n",
      "          7.3580e-02, -4.0929e-02, -2.1570e-02,  5.8518e-03,  3.9572e-02,\n",
      "          1.9475e-02, -4.3612e-02, -5.2628e-02, -4.0848e-02,  6.9208e-02,\n",
      "         -5.2510e-02,  5.1750e-02,  4.7102e-02, -1.8535e-02, -7.7252e-02,\n",
      "         -5.7959e-03,  1.6230e-02,  1.6123e-02,  5.3138e-02, -4.6969e-03,\n",
      "         -6.7675e-02, -7.5273e-02, -5.2337e-03, -1.1283e-02, -5.6681e-02,\n",
      "          7.6609e-02, -7.9330e-02,  8.0238e-02, -7.3357e-02, -6.4869e-02,\n",
      "         -6.3509e-02,  2.3229e-02,  5.3212e-02,  5.4295e-02,  5.4866e-02,\n",
      "         -6.9583e-02,  6.2174e-02, -1.0001e-02,  3.5778e-02, -3.0010e-02,\n",
      "         -2.8190e-02,  8.1798e-02, -6.8918e-02, -5.2931e-02,  6.4208e-03,\n",
      "          3.8623e-02,  5.8854e-02, -8.8813e-03, -1.6561e-02,  2.2844e-02,\n",
      "          7.6252e-02,  1.6201e-02, -1.0242e-02,  1.4760e-02,  4.2936e-02,\n",
      "         -4.0035e-02, -4.3414e-02,  5.8674e-02,  6.5228e-02,  1.6221e-02,\n",
      "          2.8031e-02,  6.5564e-03,  4.5609e-02, -1.5545e-02, -4.9063e-02,\n",
      "         -5.6424e-02,  8.1447e-03,  4.2872e-02,  5.2623e-02,  7.9174e-02,\n",
      "         -7.1484e-02, -7.8784e-02, -7.4819e-02,  1.4872e-02,  7.5035e-02,\n",
      "          1.0490e-02,  5.9136e-02, -6.6061e-02,  2.6956e-03],\n",
      "        [ 7.7585e-02, -7.2320e-02,  6.7875e-02,  7.8823e-02, -4.5930e-02,\n",
      "          3.4613e-02, -2.1098e-02, -6.7605e-02,  2.7903e-02, -2.5695e-02,\n",
      "          8.2532e-02, -4.9183e-02, -1.8409e-02, -7.4362e-02,  5.7865e-02,\n",
      "          6.9778e-02,  2.9472e-02, -2.2721e-02,  4.9381e-02,  3.1064e-02,\n",
      "         -5.9886e-02, -1.5116e-02,  6.3999e-02,  7.6748e-02,  5.3803e-02,\n",
      "          6.1145e-02, -6.2603e-02, -5.7336e-02,  3.5106e-02, -2.8716e-02,\n",
      "         -2.7500e-02,  7.7933e-02,  4.0725e-02,  6.4165e-02, -2.7879e-02,\n",
      "          6.4615e-02, -1.0340e-02, -5.2472e-03, -5.7989e-02, -4.2875e-02,\n",
      "          5.2032e-02, -6.9481e-02, -2.3665e-02, -7.8859e-02, -2.4119e-02,\n",
      "          4.2471e-02,  4.7397e-04, -3.3904e-02, -4.7851e-02,  8.1786e-02,\n",
      "          5.4788e-02,  4.2554e-02, -2.9967e-02,  5.2478e-02,  5.5189e-03,\n",
      "         -6.3317e-03,  2.1478e-04,  6.2147e-02, -7.4757e-02,  7.3072e-02,\n",
      "         -1.4702e-02, -8.0381e-02, -4.1089e-02,  1.2722e-02,  4.3134e-02,\n",
      "          3.7882e-02,  1.0267e-02, -3.0229e-02,  5.7513e-02,  3.9433e-02,\n",
      "         -4.7596e-02, -7.4151e-02, -3.3455e-02,  1.6680e-02, -1.3185e-02,\n",
      "         -2.1020e-02,  2.4497e-02, -2.6209e-02,  7.4604e-02, -4.1116e-02,\n",
      "         -8.2279e-02, -3.4549e-02, -2.2426e-02, -7.1973e-02,  6.1026e-02,\n",
      "          1.7818e-02,  7.7151e-02,  7.4235e-02,  7.5733e-02, -3.4384e-02,\n",
      "          5.1031e-02, -1.7868e-02, -5.8799e-02, -6.3511e-02, -3.8811e-02,\n",
      "         -7.1701e-02, -4.5315e-02,  6.9271e-02, -1.8438e-02, -2.7102e-02,\n",
      "          6.5615e-02,  5.2136e-03, -7.5307e-02, -4.7221e-03,  1.7626e-02,\n",
      "         -6.8554e-02, -4.4922e-02, -8.1951e-02, -1.3849e-02,  7.8908e-02,\n",
      "         -6.4380e-02, -6.0782e-02, -4.7601e-02, -2.8791e-02,  6.8633e-02,\n",
      "          3.1850e-03,  1.3275e-02,  1.4710e-02, -3.2786e-02,  3.8851e-02,\n",
      "          1.1343e-02, -1.7670e-02, -6.2243e-02,  6.5118e-02, -1.0947e-02,\n",
      "          2.7260e-02,  6.5366e-02,  1.1177e-02,  2.3378e-02, -1.3891e-02,\n",
      "         -6.0495e-02,  8.1767e-02, -1.8084e-02, -6.2046e-03,  3.2925e-02,\n",
      "         -4.7106e-02,  3.6246e-02,  5.1493e-02, -4.6060e-02, -7.5327e-02,\n",
      "         -2.1372e-02,  6.0112e-02, -6.5527e-02, -6.8911e-02]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.0012,  0.0210], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# main クラス\n",
    "cartpole_env = myEnvironment(spot_price , volatility , dividend_rate , risk_free_rate , strike_price , maturity , dt , pricer_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished 0 episodes\n",
      "finished 200 episodes\n",
      "finished 400 episodes\n",
      "finished 600 episodes\n",
      "finished 800 episodes\n",
      "finished 1000 episodes\n",
      "finished 1200 episodes\n",
      "finished 1400 episodes\n",
      "finished 1600 episodes\n",
      "finished 1800 episodes\n",
      "finished 2000 episodes\n",
      "finished 2200 episodes\n",
      "finished 2400 episodes\n",
      "finished 2600 episodes\n",
      "finished 2800 episodes\n",
      "finished 3000 episodes\n",
      "finished 3200 episodes\n",
      "finished 3400 episodes\n",
      "finished 3600 episodes\n",
      "finished 3800 episodes\n",
      "finished 4000 episodes\n",
      "finished 4200 episodes\n",
      "finished 4400 episodes\n",
      "finished 4600 episodes\n",
      "finished 4800 episodes\n",
      "finished 5000 episodes\n",
      "finished 5200 episodes\n",
      "finished 5400 episodes\n",
      "finished 5600 episodes\n",
      "finished 5800 episodes\n",
      "finished 6000 episodes\n",
      "finished 6200 episodes\n",
      "finished 6400 episodes\n",
      "finished 6600 episodes\n",
      "finished 6800 episodes\n",
      "finished 7000 episodes\n",
      "finished 7200 episodes\n",
      "finished 7400 episodes\n",
      "finished 7600 episodes\n",
      "finished 7800 episodes\n",
      "finished 8000 episodes\n",
      "finished 8200 episodes\n",
      "finished 8400 episodes\n",
      "finished 8600 episodes\n",
      "finished 8800 episodes\n",
      "finished 9000 episodes\n",
      "finished 9200 episodes\n",
      "finished 9400 episodes\n",
      "finished 9600 episodes\n",
      "finished 9800 episodes\n"
     ]
    }
   ],
   "source": [
    "cartpole_env.run(10000, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cartpole_env.agent.brain.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=2, out_features=144, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc3): Linear(in_features=144, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_input = torch.tensor([1000.0 ,0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.1632, -5.3626], grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(run_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cartpole_env.agent.brain.model.parameters())[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考（書籍のoriginal Environment）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV)  # 実行する課題を設定\n",
    "        num_states = self.env.observation_space.shape[0]  # 課題の状態数4を取得\n",
    "        num_actions = self.env.action_space.n  # CartPoleの行動（右に左に押す）の2を取得\n",
    "        self.agent = Agent(num_states, num_actions)  # 環境内で行動するAgentを生成\n",
    "\n",
    "        \n",
    "    def run(self):\n",
    "        '''実行'''\n",
    "        episode_10_list = np.zeros(10)  # 10試行分の立ち続けたstep数を格納し、平均ステップ数を出力に利用\n",
    "        complete_episodes = 0  # 195step以上連続で立ち続けた試行数\n",
    "        episode_final = False  # 最後の試行フラグ\n",
    "        frames = []  # 最後の試行を動画にするために画像を格納する変数\n",
    "\n",
    "        for episode in range(NUM_EPISODES):  # 最大試行数分繰り返す\n",
    "            observation = self.env.reset()  # 環境の初期化\n",
    "\n",
    "            state = observation  # 観測をそのまま状態sとして使用\n",
    "            state = torch.from_numpy(state).type(\n",
    "                torch.FloatTensor)  # NumPy変数をPyTorchのテンソルに変換\n",
    "            state = torch.unsqueeze(state, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "            for step in range(MAX_STEPS):  # 1エピソードのループ\n",
    "\n",
    "                if episode_final is True:  # 最終試行ではframesに各時刻の画像を追加していく\n",
    "                    frames.append(self.env.render(mode='rgb_array'))\n",
    "\n",
    "                action = self.agent.get_action(state, episode)  # 行動を求める\n",
    "\n",
    "                # 行動a_tの実行により、s_{t+1}とdoneフラグを求める\n",
    "                # actionから.item()を指定して、中身を取り出す\n",
    "                observation_next, _, done, _ = self.env.step(\n",
    "                    action.item())  # rewardとinfoは使わないので_にする\n",
    "\n",
    "                # 報酬を与える。さらにepisodeの終了評価と、state_nextを設定する\n",
    "                if done:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\n",
    "                    state_next = None  # 次の状態はないので、Noneを格納\n",
    "\n",
    "                    # 直近10episodeの立てたstep数リストに追加\n",
    "                    episode_10_list = np.hstack(\n",
    "                        (episode_10_list[1:], step + 1))\n",
    "\n",
    "                    if step < 195:\n",
    "                        reward = torch.FloatTensor(\n",
    "                            [-1.0])  # 途中でこけたら罰則として報酬-1を与える\n",
    "                        complete_episodes = 0  # 連続成功記録をリセット\n",
    "                    else:\n",
    "                        reward = torch.FloatTensor([1.0])  # 立ったまま終了時は報酬1を与える\n",
    "                        complete_episodes = complete_episodes + 1  # 連続記録を更新\n",
    "                else:\n",
    "                    reward = torch.FloatTensor([0.0])  # 普段は報酬0\n",
    "                    state_next = observation_next  # 観測をそのまま状態とする\n",
    "                    state_next = torch.from_numpy(state_next).type(\n",
    "                        torch.FloatTensor)  # numpy変数をPyTorchのテンソルに変換\n",
    "                    state_next = torch.unsqueeze(state_next, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "                # メモリに経験を追加\n",
    "                self.agent.memorize(state, action, state_next, reward)\n",
    "\n",
    "                # Experience ReplayでQ関数を更新する\n",
    "                self.agent.update_q_function()\n",
    "\n",
    "                # 観測の更新\n",
    "                state = state_next\n",
    "\n",
    "                # 終了時の処理\n",
    "                if done:\n",
    "                    print('%d Episode: Finished after %d steps：10試行の平均step数 = %.1lf' % (\n",
    "                        episode, step + 1, episode_10_list.mean()))\n",
    "                    break\n",
    "\n",
    "            if episode_final is True:\n",
    "                # 動画を保存と描画\n",
    "                display_frames_as_gif(frames)\n",
    "                break\n",
    "\n",
    "            # 10連続で200step経ち続けたら成功\n",
    "            if complete_episodes >= 10:\n",
    "                print('10回連続成功')\n",
    "                episode_final = True  # 次の試行を描画を行う最終試行とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main クラス\n",
    "cartpole_env = Environment()\n",
    "cartpole_env.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base2]",
   "language": "python",
   "name": "conda-env-base2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "209px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "671px",
    "left": "0px",
    "right": "1669.6px",
    "top": "110px",
    "width": "194px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
