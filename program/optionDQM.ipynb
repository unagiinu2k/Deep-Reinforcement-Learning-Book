{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git commit -a -m \"checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#パッケージのimport\" data-toc-modified-id=\"パッケージのimport-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>パッケージのimport</a></div><div class=\"lev1 toc-item\"><a href=\"#namedtuple\" data-toc-modified-id=\"namedtuple-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>namedtuple</a></div><div class=\"lev1 toc-item\"><a href=\"#定数の設定\" data-toc-modified-id=\"定数の設定-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>定数の設定</a></div><div class=\"lev1 toc-item\"><a href=\"#経験を保存するメモリクラスを定義します\" data-toc-modified-id=\"経験を保存するメモリクラスを定義します-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>経験を保存するメモリクラスを定義します</a></div><div class=\"lev2 toc-item\"><a href=\"#sandbox\" data-toc-modified-id=\"sandbox-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>sandbox</a></div><div class=\"lev1 toc-item\"><a href=\"#brainstorming\" data-toc-modified-id=\"brainstorming-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>brainstorming</a></div><div class=\"lev1 toc-item\"><a href=\"#Brain\" data-toc-modified-id=\"Brain-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Brain</a></div><div class=\"lev1 toc-item\"><a href=\"#Agent\" data-toc-modified-id=\"Agent-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Agent</a></div><div class=\"lev1 toc-item\"><a href=\"#pricer\" data-toc-modified-id=\"pricer-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>pricer</a></div><div class=\"lev2 toc-item\"><a href=\"#class-version-zero\" data-toc-modified-id=\"class-version-zero-81\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>class version zero</a></div><div class=\"lev1 toc-item\"><a href=\"#replacing-step()-and-state-initialization\" data-toc-modified-id=\"replacing-step()-and-state-initialization-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>replacing step() and state initialization</a></div><div class=\"lev2 toc-item\"><a href=\"#sandbox\" data-toc-modified-id=\"sandbox-91\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>sandbox</a></div><div class=\"lev1 toc-item\"><a href=\"#Environment改（大幅改修した）\" data-toc-modified-id=\"Environment改（大幅改修した）-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Environment改（大幅改修した）</a></div><div class=\"lev2 toc-item\"><a href=\"#Version-1\" data-toc-modified-id=\"Version-1-101\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;</span>Version 1</a></div><div class=\"lev3 toc-item\"><a href=\"#To-be-determined\" data-toc-modified-id=\"To-be-determined-1011\"><span class=\"toc-item-num\">10.1.1&nbsp;&nbsp;</span>To be determined</a></div><div class=\"lev3 toc-item\"><a href=\"#implementation\" data-toc-modified-id=\"implementation-1012\"><span class=\"toc-item-num\">10.1.2&nbsp;&nbsp;</span>implementation</a></div><div class=\"lev1 toc-item\"><a href=\"#sandbox\" data-toc-modified-id=\"sandbox-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>sandbox</a></div><div class=\"lev1 toc-item\"><a href=\"#参考（書籍のoriginal-Environment）\" data-toc-modified-id=\"参考（書籍のoriginal-Environment）-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>参考（書籍のoriginal Environment）</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "** 5.3、5.4  PyTorchでDQN **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パッケージのimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本コードでは、namedtupleを使用します。\n",
    "\n",
    "namedtupleを使うことで、値をフィールド名とペアで格納できます。\n",
    "\n",
    "すると値に対して、フィールド名でアクセスできて便利です。\n",
    "\n",
    "https://docs.python.jp/3/library/collections.html#collections.namedtuple\n",
    "\n",
    "以下は使用例です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr(name_a='名前Aです', value_b=100)\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Tr = namedtuple('tr', ('name_a', 'value_b'))\n",
    "Tr_object = Tr('名前Aです', 100)\n",
    "\n",
    "print(Tr_object)  # 出力：tr(name_a='名前Aです', value_b=100)\n",
    "print(Tr_object.value_b)  # 出力：100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'名前Aです'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tr_object.name_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namedtupleを生成\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ('s', 't', 'action', 'next_s' , \"next_t\", 'reward'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# namedtupleを生成\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'next_state', 'reward'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_price = 127.62\n",
    "volatility = 0.20 # the historical vols or implied vols\n",
    "dividend_rate =  0.0163\n",
    "risk_free_rate = 0.001\n",
    "maturity = 0.1\n",
    "dt = 0.1\n",
    "\n",
    "\n",
    "strike_price = 130\n",
    "\n",
    "\n",
    "#steps = 200\n",
    "pricer_steps = 100\n",
    "\n",
    "#dt = maturity / steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENV = 'CartPole-v0'  # 使用する課題名\n",
    "GAMMA = np.exp(-dt * risk_free_rate)  # 時間割引率\n",
    "MAX_STEPS = 200  # 1試行のstep数\n",
    "NUM_EPISODES = 500  # 最大試行回数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 経験を保存するメモリクラスを定義します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 概ね書籍のままの実装でいけるはず（？）\n",
    "* ただし、おそらくサンプル取得時に完全にランダムにしないで3項ツリー的な３つのnext stateをまとめてmini batfchに含めたほうがいいのではないかという気がする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, CAPACITY):\n",
    "        self.capacity = CAPACITY  # メモリの最大長さ\n",
    "        self.memory = []  # 経験を保存する変数\n",
    "        self.index = 0  # 保存するindexを示す変数\n",
    "\n",
    "    def push(self, state, action, state_next, reward):\n",
    "        '''transition = (state, action, state_next, reward)をメモリに保存する'''\n",
    "\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)  # メモリが満タンでないときは足す\n",
    "\n",
    "        # namedtupleのTransitionを使用し、値とフィールド名をペアにして保存します\n",
    "        self.memory[self.index] = Transition(state, action, state_next, reward)\n",
    "\n",
    "        self.index = (self.index + 1) % self.capacity  # 保存するindexを1つずらす\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        '''batch_size分だけ、ランダムに保存内容を取り出す'''\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''関数lenに対して、現在の変数memoryの長さを返す'''\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = ReplayMemory(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.push(100,0.1, 0 , 101, 0.15 ,0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.push(100,0.1, 0 , 99, 0.15 ,0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# brainstorming\n",
    "\n",
    "* 素直な実装ではnum_statesは２となる。（$S$および$t$）\n",
    "* brainもたぶん書籍のままの実装でいける？？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain\n",
    "\n",
    "エージェントが持つ脳となるクラスです、DQNを実行します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "methodは\n",
    "\n",
    "* replay: Experience Replayでネットワークの結合パラメータを学習\n",
    "* decide_action: アクション決定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q関数をディープラーニングのネットワークをクラスとして定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "CAPACITY = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_actions = num_actions  # CartPoleの行動（右に左に押す）の2を取得\n",
    "\n",
    "        # 経験を記憶するメモリオブジェクトを生成\n",
    "        self.memory = ReplayMemory(CAPACITY)\n",
    "\n",
    "        # ニューラルネットワークを構築\n",
    "        self.model = nn.Sequential()\n",
    "        self.model.add_module('fc1', nn.Linear(num_states, 144))\n",
    "        self.model.add_module('relu1', nn.ReLU())\n",
    "        #self.model.add_module('fc2', nn.Linear(32, 32))\n",
    "        #self.model.add_module('relu2', nn.ReLU())\n",
    "        self.model.add_module('fc3', nn.Linear(144, num_actions))\n",
    "\n",
    "        print(self.model)  # ネットワークの形を出力\n",
    "        print(list(self.model.parameters()))\n",
    "\n",
    "        # 最適化手法の設定\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "\n",
    "    def replay(self):\n",
    "        '''Experience Replayでネットワークの結合パラメータを学習'''\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 1. メモリサイズの確認\n",
    "        # -----------------------------------------\n",
    "        # 1.1 メモリサイズがミニバッチより小さい間は何もしない\n",
    "        #print(\"memory .size : {}\".format(len(self.memory)))\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 2. ミニバッチの作成\n",
    "        # -----------------------------------------\n",
    "        # 2.1 メモリからミニバッチ分のデータを取り出す\n",
    "        transitions = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        # 2.2 各変数をミニバッチに対応する形に変形\n",
    "        # transitionsは1stepごとの(state, action, state_next, reward)が、BATCH_SIZE分格納されている\n",
    "        # つまり、(state, action, state_next, reward)×BATCH_SIZE\n",
    "        # これをミニバッチにしたい。つまり\n",
    "        # (state×BATCH_SIZE, action×BATCH_SIZE, state_next×BATCH_SIZE, reward×BATCH_SIZE)にする\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # 2.3 各変数の要素をミニバッチに対応する形に変形し、ネットワークで扱えるようVariableにする\n",
    "        # 例えばstateの場合、[torch.FloatTensor of size 1x4]がBATCH_SIZE分並んでいるのですが、\n",
    "        # それを torch.FloatTensor of size BATCH_SIZEx4 に変換します\n",
    "        # 状態、行動、報酬、non_finalの状態のミニバッチのVariableを作成\n",
    "        # catはConcatenates（結合）のことです。\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        try:\n",
    "            non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                           if s is not None])\n",
    "        except:\n",
    "            print()\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 3. 教師信号となるQ(s_t, a_t)値を求める\n",
    "        # -----------------------------------------\n",
    "        # 3.1 ネットワークを推論モードに切り替える\n",
    "        self.model.eval()\n",
    "\n",
    "        # 3.2 ネットワークが出力したQ(s_t, a_t)を求める\n",
    "        # self.model(state_batch)は、右左の両方のQ値を出力しており\n",
    "        # [torch.FloatTensor of size BATCH_SIZEx2]になっている。\n",
    "        # ここから実行したアクションa_tに対応するQ値を求めるため、action_batchで行った行動a_tが右か左かのindexを求め\n",
    "        # それに対応するQ値をgatherでひっぱり出す。\n",
    "        state_action_values = self.model(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # 3.3 max{Q(s_t+1, a)}値を求める。ただし次の状態があるかに注意。\n",
    "\n",
    "        # cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成\n",
    "        non_final_mask = torch.ByteTensor(tuple(map(lambda s: s is not None,\n",
    "                                                    batch.next_state)))\n",
    "        # まずは全部0にしておく\n",
    "        next_state_values = torch.zeros(BATCH_SIZE)\n",
    "\n",
    "        # 次の状態があるindexの最大Q値を求める\n",
    "        # 出力にアクセスし、max(1)で列方向の最大値の[値、index]を求めます\n",
    "        # そしてそのQ値（index=0）を出力します\n",
    "        # detachでその値を取り出します\n",
    "        next_state_values[non_final_mask] = self.model(\n",
    "            non_final_next_states).max(1)[0].detach()\n",
    "\n",
    "        # 3.4 教師となるQ(s_t, a_t)値を、Q学習の式から求める\n",
    "        expected_state_action_values = reward_batch + GAMMA * next_state_values\n",
    "\n",
    "        # -----------------------------------------\n",
    "        # 4. 結合パラメータの更新\n",
    "        # -----------------------------------------\n",
    "        # 4.1 ネットワークを訓練モードに切り替える\n",
    "        self.model.train()\n",
    "\n",
    "        # 4.2 損失関数を計算する（smooth_l1_lossはHuberloss）\n",
    "        # expected_state_action_valuesは\n",
    "        # sizeが[minbatch]になっているので、unsqueezeで[minibatch x 1]へ\n",
    "        loss = F.smooth_l1_loss(state_action_values,\n",
    "                                expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # 4.3 結合パラメータを更新する\n",
    "        self.optimizer.zero_grad()  # 勾配をリセット\n",
    "        loss.backward()  # バックプロパゲーションを計算\n",
    "        self.optimizer.step()  # 結合パラメータを更新\n",
    "\n",
    "    def decide_action(self, state, episode):\n",
    "        '''現在の状態に応じて、行動を決定する'''\n",
    "        # ε-greedy法で徐々に最適行動のみを採用する\n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            self.model.eval()  # ネットワークを推論モードに切り替える\n",
    "            with torch.no_grad():\n",
    "                action = self.model(state).max(1)[1].view(1, 1)\n",
    "            # ネットワークの出力の最大値のindexを取り出します = max(1)[1]\n",
    "            # .view(1,1)は[torch.LongTensor of size 1]　を size 1x1 に変換します\n",
    "\n",
    "        else:\n",
    "            # 0,1の行動をランダムに返す\n",
    "            action = torch.LongTensor(\n",
    "                [[random.randrange(self.num_actions)]])  # 0,1の行動をランダムに返す\n",
    "            # actionは[torch.LongTensor of size 1x1]の形になります\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "CartPoleで動くエージェントクラスです、棒付き台車そのものになります\n",
    "\n",
    "methodは\n",
    "\n",
    "\n",
    "* 行動価値Qを更新\n",
    "*  状態を与えると行動を決定\n",
    "*  状態、選択するアクション、次の状態、報酬などを記憶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        '''課題の状態と行動の数を設定する'''\n",
    "        self.brain = Brain(num_states, num_actions)  # エージェントが行動を決定するための頭脳を生成\n",
    "\n",
    "    def update_q_function(self):\n",
    "        '''Q関数を更新する'''\n",
    "        self.brain.replay()\n",
    "\n",
    "    def get_action(self, state, episode):\n",
    "        '''行動を決定する'''\n",
    "        action = self.brain.decide_action(state, episode)\n",
    "        return action\n",
    "\n",
    "    def memorize(self, state, action, state_next, reward):\n",
    "        '''memoryオブジェクトに、state, action, state_next, rewardの内容を保存する'''\n",
    "        self.brain.memory.push(state, action, state_next, reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pricer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import QuantLib as ql "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class version zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cl_am_call:\n",
    "    def price(self , spot_price, strike_price, maturity):\n",
    "        \n",
    "        dummy_strike = strike_price / spot_price\n",
    "        \n",
    "\n",
    "        option_type = ql.Option.Call\n",
    "        payoff = ql.PlainVanillaPayoff(option_type, dummy_strike)\n",
    "        \n",
    "        maturity_date = self.calculation_date + int(365.0 * maturity)\n",
    "        settlement = self.calculation_date\n",
    "        am_exercise = ql.AmericanExercise(settlement, maturity_date)\n",
    "        \n",
    "        american_option = ql.VanillaOption(payoff, am_exercise)\n",
    "        american_option.setPricingEngine(self.binomial_engine)\n",
    "        ql.Settings.instance().evaluationDate = self.calculation_date\n",
    "\n",
    "        return (american_option.NPV() * spot_price)\n",
    "        \n",
    "\n",
    "    def __init__(self , volatility , dividend_rate , risk_free_rate  ,steps):\n",
    "        day_count = ql.Actual365Fixed()\n",
    "        #calendar = ql.UnitedStates()\n",
    "        calendar = ql.Japan()\n",
    "        self.calculation_date = ql.Date(8, 5, 2015)\n",
    "        dummy_spot = 1\n",
    "        \n",
    "        self.spot_handle = ql.QuoteHandle(ql.SimpleQuote(1.0))\n",
    "\n",
    "        ql.Settings.instance().evaluationDate = self.calculation_date\n",
    "\n",
    "\n",
    "        self.flat_ts = ql.YieldTermStructureHandle(\n",
    "            ql.FlatForward(self.calculation_date, risk_free_rate, day_count)\n",
    "        )\n",
    "\n",
    "        self.dividend_yield = ql.YieldTermStructureHandle(\n",
    "            ql.FlatForward(self.calculation_date, dividend_rate, day_count)\n",
    "        )\n",
    "\n",
    "        #### volatility\n",
    "\n",
    "        self.flat_vol_ts = ql.BlackVolTermStructureHandle(\n",
    "            ql.BlackConstantVol(self.calculation_date, calendar, volatility, day_count)\n",
    "        )\n",
    "\n",
    "        #### BS framework\n",
    "\n",
    "        self.bsm_process = ql.BlackScholesMertonProcess(self.spot_handle, \n",
    "                                                   self.dividend_yield, \n",
    "                                                   self.flat_ts, \n",
    "                                                   self.flat_vol_ts)\n",
    "\n",
    "\n",
    "        self.binomial_engine = ql.BinomialVanillaEngine(self.bsm_process, \"crr\", steps)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_call = cl_am_call(volatility ,  dividend_rate ,  risk_free_rate , pricer_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_call.price(spot_price, strike_price, maturity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_call.price(1 , 100, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replacing step() and state initialization\n",
    "\n",
    "* initializationをどうするかな・・\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  decision2next_state:\n",
    "    \n",
    "    def __init__(self ,  S0 , volatility , dividend_rate , risk_free_rate  , dt , Maturity):\n",
    "        self.S0 = S0\n",
    "        self.volatility = volatility\n",
    "        self.dividend_rate = dividend_rate\n",
    "        self.risk_free_rate = risk_free_rate\n",
    "        self.dt = dt\n",
    "        self.Maturity = Maturity\n",
    "        \n",
    "        self.barbeta_dt = (risk_free_rate - dividend_rate - volatility**2 * 0.5) * dt\n",
    "        self.sd = volatility * np.sqrt(dt)\n",
    "    \n",
    "    def reset(self):\n",
    "        T0 = 0    \n",
    "        if True:\n",
    "            state0 = np.array([self.S0 , T0])\n",
    "        else :\n",
    "            state0 = torch.Tensor(np.array([[self.S0 , T0]]))\n",
    "        return state0\n",
    "        \n",
    "    \n",
    "    def step(self , state , action):\n",
    "             #stateはtorch.tensor torch.Size([1, 2])\n",
    "            if False:\n",
    "                #next_state = state.copy()\n",
    "                S = state[0]\n",
    "                T = state[1]\n",
    "            else:               \n",
    "                #next_state = state.clone()\n",
    "                S = state[0][0].item()\n",
    "                T = state[0][1].item()    \n",
    "            T_next = T + self.dt\n",
    "            S_return = self.barbeta_dt + self.sd * np.random.randn()  \n",
    "            \n",
    "            S_next = S * np.exp(S_return)\n",
    "            if True:\n",
    "                state_next =np.array([S_next , T_next])\n",
    "            else:\n",
    "                state_next = torch.Tensor(np.array([[S_next , T_next]]))\n",
    "            done = (action == 1) or (T_next > self.Maturity) \n",
    "            return state_next , done\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stepper = decision2next_state(spot_price, volatility , dividend_rate , risk_free_rate , 0.1 , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_state = torch.Tensor(np.array([[100 , 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_state = np.array([100,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tmp_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stepper.step(tmp_state , 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stepper.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment改（大幅改修した）\n",
    "CartPoleを実行する環境のクラスです\n",
    "\n",
    "* for loopは二重になっている\n",
    "* 外側のループはEPISODEに関して\n",
    "    * Episode = 試行：一回ポールを立てて倒れるか200ステップ経過するまでを１エピソードと数える\n",
    "    * 内側のループはステップに関して\n",
    "        * 初期状態のBrainを使って、1ステップ目から左右にコントロールしていくことからスタート\n",
    "        * 各ステップごとに状態と遷移を記録する。\n",
    "        * 同様に各ステップごとに行動価値関数をアップデートしていく\n",
    "    * 10エピソード連続で200ステップまで持ちこたえられたら成功\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 1\n",
    "* まずはMCパスを発生させて、episode $\\approx$ pathであるような形でやってみよう\n",
    "* 後々、MCパスはtrinomial tree上のみを推移するようにするかもしれないが、ここでは素直にMCパスを普通に発生させてみよう\n",
    "    * MCパス発生はquantlibにやらせてもいいが自分で実装してしまってもいいかな\n",
    "* env.stepを\n",
    "```\n",
    " 1 time step推進\n",
    "```\n",
    "に置き換える。\n",
    "\n",
    "\n",
    "### To be determined\n",
    "* QLはここに取り込む？ $\\Rightarrow$ maybe yes\n",
    "* MCもここで？ $\\Rightarrow$ maybe yes\n",
    "    * gymの場合は時間推進はgymが面倒見てくれていた。そのgymはEnvironmentクラスのメンバーになっている。\n",
    "* 書籍にあったような20回連続でみたいな終了判定基準はもはや適切ではない。ではどのような終了判定基準が良いか\n",
    "```\n",
    "現状では正解がわかっているのでいろいろズルをしよう。例えば、10パス連続で最適行使の判定を正解できたときetc\n",
    "```\n",
    "* reward設計\n",
    "```\n",
    "行使した場合にはrewardを払って行動価値関数がゼロのnext stateに飛ぶ\n",
    "```\n",
    "とする."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import QuantLib as ql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myEnvironment:\n",
    "\n",
    "    def __init__(self, S0 , vol , q , r , K , T , dt ,pricer_steps):\n",
    "        #self.env = gym.make(ENV)  # 実行する課題を設定\n",
    "        self.S0 = S0\n",
    "        self.vol = vol\n",
    "        self.q = q\n",
    "        self.r = r\n",
    "        self.K = K\n",
    "        self.T = T\n",
    "        self.dt = dt\n",
    "        self.stepper = decision2next_state(S0 , vol , q , r , dt, T)\n",
    "        \n",
    "        num_states = 2# S and t\n",
    "        num_actions = 2 # exercise or hold \n",
    "        self.agent = Agent(num_states, num_actions)  # 環境内で行動するAgentを生成\n",
    "        self.pricer = cl_am_call(vol ,  q ,  r , steps = pricer_steps)\n",
    "\n",
    "        \n",
    "    def run(self):\n",
    "        '''実行'''\n",
    "        episode_10_list = np.zeros(10)  # 10試行分の立ち続けたstep数を格納し、平均ステップ数を出力に利用\n",
    "        #complete episodesを終了条件にするのはもはや適切ではない\n",
    "        complete_episodes = 0  # 195step以上連続で立ち続けた試行数\n",
    "        episode_final = False  # 最後の試行フラグ\n",
    "        frames = []  # 最後の試行を動画にするために画像を格納する変数\n",
    "\n",
    "        for episode in range(NUM_EPISODES):  # 最大試行数分繰り返す\n",
    "            #observation = self.env.reset()  # 環境の初期化\n",
    "            observation = self.stepper.reset()\n",
    "\n",
    "            state = observation  # 観測をそのまま状態sとして使用\n",
    "            if True:\n",
    "                #print(type(state))\n",
    "                state = torch.from_numpy(state).type(torch.FloatTensor)  # NumPy変数をPyTorchのテンソルに変換\n",
    "                state = torch.unsqueeze(state, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "            for step in range(MAX_STEPS):  # 1エピソードのループ\n",
    "\n",
    "                action = self.agent.get_action(state, episode)  # 行動を求める。\n",
    "                ### * episodeを食わせるのはQ学習の定義を見れば納得できる。ここは書籍のままの\n",
    "        \n",
    "                \n",
    "                # 行動a_tの実行により、s_{t+1}とdoneフラグを求める\n",
    "                # actionから.item()を指定して、中身を取り出す\n",
    " \n",
    "                observation_next , done = self.stepper.step(state , action.item())\n",
    "                #print(\"done is {}\".format(done))\n",
    "\n",
    "                # 報酬を与える。さらにepisodeの終了評価と、state_nextを設定する\n",
    "                if done:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\n",
    "                    state_next = None  # 次の状態はないので、Noneを格納\n",
    "       \n",
    "\n",
    "                    # 直近10episodeの立てたstep数リストに追加\n",
    "                    episode_10_list = np.hstack(\n",
    "                        (episode_10_list[1:], step + 1))\n",
    "                    if True:\n",
    "                        exercise_value = state[0][0].item() - self.K\n",
    "                        reward = torch.FloatTensor([exercise_value])    \n",
    "                        complete_episodes = complete_episodes + 1  ## * 暫定的な処理\n",
    "                    \n",
    "                    else:\n",
    "\n",
    "                        if step < 195:\n",
    "                            reward = torch.FloatTensor(\n",
    "                                [-1.0])  # 途中でこけたら罰則として報酬-1を与える\n",
    "                            complete_episodes = 0  # 連続成功記録をリセット\n",
    "                        else:\n",
    "                            reward = torch.FloatTensor([1.0])  # 立ったまま終了時は報酬1を与える\n",
    "                            complete_episodes = complete_episodes + 1  # 連続記録を更新\n",
    "                else:\n",
    "                    reward = torch.FloatTensor([0.0])  # 普段は報酬0\n",
    "                    state_next = observation_next  # 観測をそのまま状態とする\n",
    "                    if True:\n",
    "                        state_next = torch.from_numpy(state_next).type(\n",
    "                        torch.FloatTensor)  # numpy変数をPyTorchのテンソルに変換\n",
    "                        state_next = torch.unsqueeze(state_next, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "                # メモリに経験を追加\n",
    "                self.agent.memorize(state, action, state_next, reward)\n",
    "\n",
    "                # Experience ReplayでQ関数を更新する\n",
    "                try:\n",
    "                    self.agent.update_q_function()\n",
    "                except:\n",
    "                    print(\"something is wrong\")\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "                # 終了時の処理\n",
    "                if done:\n",
    "                    #print('%d Episode: Finished after %d steps：10試行の平均step数 = %.1lf' % (\n",
    "                    #    episode, step + 1, episode_10_list.mean()))\n",
    "                    #print(\"exercise value is {}\".format(exercise_value))\n",
    "                    #print(type(state))\n",
    "\n",
    "                    error = exercise_value - self.pricer.price(state[0][0].item(), self.K , state[0][1].item())\n",
    "                    print(\"S : {:.2f} / t : {:.2f} / exercise value : {:.2f} / error : {:.2f}\".format(state[0][0].item() , state[0][1].item() , exercise_value , error))\n",
    "                    break\n",
    "                    ## ** error \n",
    "                                # 観測の更新\n",
    "                state = state_next\n",
    "            if episode_final:\n",
    "                break\n",
    "\n",
    "            # 10連続で200step経ち続けたら成功\n",
    "            if complete_episodes >= 300:\n",
    "                #print('10回連続成功')\n",
    "                episode_final = True  # 次の試行を描画を行う最終試行とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (fc1): Linear(in_features=2, out_features=144, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc3): Linear(in_features=144, out_features=2, bias=True)\n",
      ")\n",
      "[Parameter containing:\n",
      "tensor([[-0.0362,  0.1583],\n",
      "        [ 0.0932, -0.6907],\n",
      "        [-0.3733,  0.1635],\n",
      "        [ 0.6972,  0.6018],\n",
      "        [-0.3605, -0.0572],\n",
      "        [-0.5609,  0.1990],\n",
      "        [ 0.5134,  0.5214],\n",
      "        [-0.1966, -0.6901],\n",
      "        [ 0.2585, -0.2576],\n",
      "        [ 0.3842, -0.5081],\n",
      "        [-0.6247,  0.2014],\n",
      "        [-0.4522, -0.6911],\n",
      "        [-0.0599,  0.1989],\n",
      "        [-0.6263,  0.4936],\n",
      "        [-0.2746, -0.2402],\n",
      "        [-0.4163, -0.2454],\n",
      "        [-0.6015, -0.5589],\n",
      "        [-0.1315,  0.4076],\n",
      "        [-0.0714,  0.0471],\n",
      "        [-0.6430,  0.2099],\n",
      "        [-0.6358, -0.6769],\n",
      "        [ 0.6644, -0.5295],\n",
      "        [ 0.3273,  0.1250],\n",
      "        [-0.2443, -0.6357],\n",
      "        [ 0.5829, -0.5348],\n",
      "        [ 0.3975,  0.1441],\n",
      "        [-0.6064,  0.5394],\n",
      "        [ 0.5569,  0.0545],\n",
      "        [ 0.2688, -0.5247],\n",
      "        [ 0.2330,  0.4114],\n",
      "        [-0.3328,  0.0985],\n",
      "        [ 0.4414, -0.1594],\n",
      "        [-0.1989, -0.3701],\n",
      "        [ 0.2226,  0.1420],\n",
      "        [-0.2290,  0.0289],\n",
      "        [-0.2158, -0.0486],\n",
      "        [ 0.4769,  0.1785],\n",
      "        [ 0.6489, -0.6912],\n",
      "        [ 0.4123, -0.0283],\n",
      "        [-0.7002, -0.4172],\n",
      "        [-0.4734, -0.1754],\n",
      "        [ 0.0539, -0.0355],\n",
      "        [ 0.1594, -0.3386],\n",
      "        [-0.5263,  0.2290],\n",
      "        [-0.6989, -0.6688],\n",
      "        [ 0.1246,  0.6311],\n",
      "        [ 0.3366,  0.0045],\n",
      "        [ 0.3731,  0.0991],\n",
      "        [-0.3688,  0.1758],\n",
      "        [ 0.2314,  0.2491],\n",
      "        [ 0.5828, -0.6834],\n",
      "        [ 0.2167,  0.6803],\n",
      "        [ 0.6398, -0.3860],\n",
      "        [ 0.1119,  0.0109],\n",
      "        [-0.1192,  0.2641],\n",
      "        [-0.6772, -0.3232],\n",
      "        [ 0.1389, -0.2321],\n",
      "        [ 0.1278,  0.0952],\n",
      "        [-0.2937, -0.4166],\n",
      "        [-0.4357,  0.6302],\n",
      "        [ 0.1567, -0.6432],\n",
      "        [-0.2991,  0.4430],\n",
      "        [-0.0981,  0.4643],\n",
      "        [ 0.3238, -0.3589],\n",
      "        [ 0.2650, -0.2821],\n",
      "        [-0.0906, -0.6760],\n",
      "        [-0.2844,  0.4005],\n",
      "        [-0.4917,  0.0713],\n",
      "        [-0.4482,  0.3522],\n",
      "        [ 0.0336, -0.0878],\n",
      "        [ 0.0924,  0.5586],\n",
      "        [ 0.0960, -0.1908],\n",
      "        [-0.5758,  0.0556],\n",
      "        [-0.1143,  0.2558],\n",
      "        [ 0.7032, -0.1835],\n",
      "        [ 0.0683, -0.1134],\n",
      "        [-0.4685, -0.5966],\n",
      "        [ 0.2029,  0.5403],\n",
      "        [-0.3570,  0.6123],\n",
      "        [-0.4986, -0.6452],\n",
      "        [-0.4399,  0.3788],\n",
      "        [-0.1180,  0.1863],\n",
      "        [ 0.0942,  0.2192],\n",
      "        [-0.6153,  0.4748],\n",
      "        [-0.4973,  0.0508],\n",
      "        [ 0.3347,  0.6982],\n",
      "        [-0.1273, -0.6778],\n",
      "        [ 0.1742,  0.3465],\n",
      "        [-0.1890, -0.7009],\n",
      "        [ 0.3302, -0.3224],\n",
      "        [ 0.1740, -0.6608],\n",
      "        [ 0.0324, -0.0484],\n",
      "        [ 0.2972,  0.5750],\n",
      "        [-0.5276,  0.0072],\n",
      "        [-0.5222,  0.5348],\n",
      "        [ 0.1658,  0.0101],\n",
      "        [-0.3588,  0.2465],\n",
      "        [ 0.5580,  0.3030],\n",
      "        [ 0.5317,  0.5167],\n",
      "        [ 0.2684, -0.3767],\n",
      "        [ 0.0112, -0.5568],\n",
      "        [-0.0434, -0.2959],\n",
      "        [-0.6566,  0.1193],\n",
      "        [-0.3021, -0.2201],\n",
      "        [-0.0710,  0.3925],\n",
      "        [ 0.1569,  0.2180],\n",
      "        [ 0.4404, -0.5126],\n",
      "        [ 0.4344,  0.5394],\n",
      "        [ 0.5775, -0.3686],\n",
      "        [ 0.4525, -0.3810],\n",
      "        [ 0.2198,  0.5934],\n",
      "        [-0.2525, -0.5085],\n",
      "        [-0.0295, -0.0592],\n",
      "        [-0.6195, -0.3218],\n",
      "        [-0.0037,  0.2663],\n",
      "        [-0.3151,  0.5272],\n",
      "        [-0.0214, -0.2755],\n",
      "        [-0.6903,  0.6229],\n",
      "        [ 0.2998, -0.1273],\n",
      "        [ 0.3907, -0.0028],\n",
      "        [ 0.5622,  0.5626],\n",
      "        [-0.3270, -0.0074],\n",
      "        [ 0.6455,  0.3436],\n",
      "        [ 0.4499,  0.5323],\n",
      "        [-0.2590, -0.4379],\n",
      "        [ 0.2619, -0.5775],\n",
      "        [ 0.1019,  0.7051],\n",
      "        [ 0.4100,  0.2782],\n",
      "        [ 0.5431,  0.1071],\n",
      "        [-0.5324,  0.0980],\n",
      "        [-0.1394, -0.0971],\n",
      "        [-0.6935, -0.5212],\n",
      "        [-0.2659, -0.2594],\n",
      "        [ 0.1861,  0.5893],\n",
      "        [-0.3371, -0.5083],\n",
      "        [-0.5834,  0.2373],\n",
      "        [-0.6016, -0.6477],\n",
      "        [-0.1025, -0.1121],\n",
      "        [ 0.3517,  0.4307],\n",
      "        [-0.0548,  0.0862],\n",
      "        [-0.3684, -0.4335],\n",
      "        [-0.7058,  0.3898],\n",
      "        [-0.1806,  0.3609],\n",
      "        [ 0.1834, -0.1625]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.6499,  0.6474, -0.1149, -0.6849,  0.4377,  0.6220, -0.1663, -0.6062,\n",
      "         0.0890,  0.0057, -0.5197, -0.4529, -0.4718, -0.5422, -0.2031,  0.0724,\n",
      "         0.2055, -0.2663, -0.6546, -0.0308, -0.2508,  0.3759,  0.4026,  0.3953,\n",
      "        -0.6190,  0.4268,  0.0253,  0.1533,  0.2019, -0.5776, -0.5396,  0.5224,\n",
      "        -0.2072,  0.2661,  0.5964, -0.1239,  0.6769,  0.5348,  0.4724,  0.5594,\n",
      "         0.3361,  0.6943, -0.4098,  0.2273, -0.1065,  0.5206, -0.5417, -0.2604,\n",
      "        -0.2001,  0.1648,  0.2475, -0.0709,  0.5252, -0.4831, -0.5294, -0.6759,\n",
      "         0.3338, -0.5167, -0.7069, -0.1476, -0.3152,  0.6099,  0.1861,  0.2440,\n",
      "        -0.4228,  0.2308, -0.4326, -0.4261,  0.5392,  0.1991, -0.4363, -0.3721,\n",
      "         0.2335,  0.4148,  0.4741, -0.1476, -0.4648,  0.3636, -0.2746, -0.6572,\n",
      "         0.4750, -0.6334,  0.0475, -0.7062,  0.0925,  0.2550,  0.6446, -0.1197,\n",
      "         0.6760, -0.4691,  0.3706, -0.4085,  0.0965,  0.2137,  0.5731,  0.5833,\n",
      "        -0.2893,  0.3574,  0.6701, -0.5355, -0.0418,  0.0494,  0.6054,  0.1220,\n",
      "        -0.4979,  0.2863,  0.6332,  0.5703,  0.2039, -0.6820,  0.0367, -0.2995,\n",
      "        -0.0112, -0.3053,  0.3066,  0.1474, -0.6243,  0.4267, -0.0731,  0.2705,\n",
      "         0.4936,  0.1604,  0.1619, -0.5950,  0.0200, -0.6130,  0.3376, -0.1092,\n",
      "         0.2771, -0.5898, -0.6892,  0.3009, -0.3723,  0.2241, -0.6458, -0.5582,\n",
      "         0.4585,  0.2214, -0.2509, -0.4953, -0.0215,  0.2619, -0.1691, -0.1214],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0393,  0.0702, -0.0107,  0.0699, -0.0591, -0.0495, -0.0468, -0.0475,\n",
      "          0.0086, -0.0429, -0.0009, -0.0239, -0.0564,  0.0186,  0.0833,  0.0010,\n",
      "          0.0560,  0.0317,  0.0270,  0.0331,  0.0489,  0.0162, -0.0774,  0.0127,\n",
      "          0.0657, -0.0521,  0.0503, -0.0089, -0.0620,  0.0697, -0.0121,  0.0690,\n",
      "          0.0348, -0.0515, -0.0567,  0.0272,  0.0194,  0.0416,  0.0090, -0.0396,\n",
      "         -0.0004,  0.0785, -0.0223, -0.0612,  0.0025, -0.0752,  0.0468, -0.0806,\n",
      "          0.0827,  0.0491,  0.0802, -0.0670,  0.0578,  0.0080,  0.0475,  0.0196,\n",
      "          0.0220, -0.0049, -0.0487, -0.0284, -0.0440, -0.0745, -0.0130,  0.0426,\n",
      "          0.0651, -0.0736, -0.0273, -0.0478, -0.0344,  0.0046, -0.0781, -0.0489,\n",
      "          0.0464,  0.0818,  0.0116,  0.0037, -0.0812,  0.0077,  0.0698,  0.0712,\n",
      "         -0.0350,  0.0447, -0.0320, -0.0448,  0.0221, -0.0516, -0.0448,  0.0377,\n",
      "         -0.0774, -0.0815, -0.0273,  0.0282, -0.0266, -0.0119, -0.0689,  0.0222,\n",
      "          0.0238, -0.0091, -0.0750, -0.0444,  0.0559, -0.0323,  0.0092,  0.0818,\n",
      "          0.0322,  0.0440,  0.0832,  0.0754, -0.0504, -0.0636, -0.0161, -0.0350,\n",
      "          0.0447, -0.0304, -0.0787,  0.0027,  0.0131,  0.0437,  0.0208,  0.0803,\n",
      "         -0.0015,  0.0623,  0.0310, -0.0199,  0.0195, -0.0572,  0.0641,  0.0318,\n",
      "         -0.0162,  0.0038,  0.0779, -0.0731, -0.0335,  0.0531,  0.0419,  0.0220,\n",
      "          0.0123,  0.0356,  0.0602, -0.0709,  0.0008,  0.0415, -0.0039, -0.0106],\n",
      "        [ 0.0734,  0.0345,  0.0807,  0.0610, -0.0067,  0.0263,  0.0165,  0.0825,\n",
      "         -0.0188, -0.0078,  0.0193,  0.0670, -0.0714,  0.0308,  0.0554,  0.0745,\n",
      "         -0.0144, -0.0498,  0.0019, -0.0444,  0.0008, -0.0377, -0.0396, -0.0524,\n",
      "          0.0763, -0.0322, -0.0501,  0.0549, -0.0196, -0.0445,  0.0546, -0.0480,\n",
      "          0.0622, -0.0177, -0.0022, -0.0560, -0.0235, -0.0755,  0.0049,  0.0668,\n",
      "          0.0640,  0.0490,  0.0571, -0.0168,  0.0402, -0.0633,  0.0301, -0.0797,\n",
      "          0.0215, -0.0612,  0.0584,  0.0191,  0.0736,  0.0572,  0.0373, -0.0699,\n",
      "          0.0659, -0.0228, -0.0778, -0.0481, -0.0719,  0.0197, -0.0482,  0.0046,\n",
      "         -0.0529, -0.0658, -0.0636, -0.0550,  0.0349, -0.0288, -0.0525,  0.0604,\n",
      "         -0.0525, -0.0409, -0.0280, -0.0179,  0.0635,  0.0165, -0.0612,  0.0734,\n",
      "         -0.0174, -0.0254,  0.0232,  0.0211,  0.0059,  0.0010, -0.0078, -0.0616,\n",
      "         -0.0276, -0.0698,  0.0127, -0.0631,  0.0364,  0.0766, -0.0429,  0.0571,\n",
      "         -0.0817, -0.0660, -0.0140,  0.0402, -0.0151, -0.0027,  0.0347, -0.0067,\n",
      "         -0.0474,  0.0223,  0.0530,  0.0332,  0.0466, -0.0390, -0.0559, -0.0666,\n",
      "          0.0216,  0.0740,  0.0258, -0.0777, -0.0088,  0.0336, -0.0289,  0.0409,\n",
      "         -0.0418, -0.0774, -0.0373,  0.0736,  0.0154,  0.0805,  0.0260,  0.0001,\n",
      "         -0.0659, -0.0661, -0.0477, -0.0062,  0.0819, -0.0116,  0.0700, -0.0662,\n",
      "          0.0543, -0.0389, -0.0715,  0.0043,  0.0122, -0.0736, -0.0187, -0.0398]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0052, -0.0829], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# main クラス\n",
    "cartpole_env = myEnvironment(spot_price , volatility , dividend_rate , risk_free_rate , strike_price , maturity , dt , pricer_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S : 132.72 / t : 0.10 / exercise value : 2.72 / error : -2.01\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 141.70 / t : 0.10 / exercise value : 11.70 / error : -0.21\n",
      "S : 134.35 / t : 0.10 / exercise value : 4.35 / error : -1.47\n",
      "S : 134.72 / t : 0.10 / exercise value : 4.72 / error : -1.36\n",
      "S : 133.32 / t : 0.10 / exercise value : 3.32 / error : -1.78\n",
      "S : 121.38 / t : 0.10 / exercise value : -8.62 / error : -9.14\n",
      "S : 123.23 / t : 0.10 / exercise value : -6.77 / error : -7.61\n",
      "S : 133.85 / t : 0.10 / exercise value : 3.85 / error : -1.62\n",
      "S : 133.75 / t : 0.10 / exercise value : 3.75 / error : -1.65\n",
      "S : 120.37 / t : 0.10 / exercise value : -9.63 / error : -10.03\n",
      "S : 131.73 / t : 0.10 / exercise value : 1.73 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 130.59 / t : 0.10 / exercise value : 0.59 / error : -2.89\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 130.89 / t : 0.10 / exercise value : 0.89 / error : -2.75\n",
      "S : 130.74 / t : 0.10 / exercise value : 0.74 / error : -2.82\n",
      "S : 127.17 / t : 0.10 / exercise value : -2.83 / error : -4.77\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 131.61 / t : 0.10 / exercise value : 1.61 / error : -2.43\n",
      "S : 127.22 / t : 0.10 / exercise value : -2.78 / error : -4.74\n",
      "S : 121.27 / t : 0.10 / exercise value : -8.73 / error : -9.24\n",
      "S : 125.99 / t : 0.10 / exercise value : -4.01 / error : -5.56\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.09 / t : 0.10 / exercise value : -2.91 / error : -4.82\n",
      "S : 110.82 / t : 0.10 / exercise value : -19.18 / error : -19.19\n",
      "S : 118.37 / t : 0.10 / exercise value : -11.63 / error : -11.85\n",
      "S : 135.35 / t : 0.10 / exercise value : 5.35 / error : -1.19\n",
      "S : 115.99 / t : 0.10 / exercise value : -14.01 / error : -14.11\n",
      "S : 135.74 / t : 0.10 / exercise value : 5.74 / error : -1.10\n",
      "S : 117.89 / t : 0.10 / exercise value : -12.11 / error : -12.30\n",
      "S : 141.85 / t : 0.10 / exercise value : 11.85 / error : -0.20\n",
      "S : 131.35 / t : 0.10 / exercise value : 1.35 / error : -2.54\n",
      "S : 126.59 / t : 0.10 / exercise value : -3.41 / error : -5.14\n",
      "S : 114.94 / t : 0.10 / exercise value : -15.06 / error : -15.13\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 108.36 / t : 0.10 / exercise value : -21.64 / error : -21.65\n",
      "S : 132.57 / t : 0.10 / exercise value : 2.57 / error : -2.06\n",
      "S : 109.68 / t : 0.10 / exercise value : -20.32 / error : -20.33\n",
      "S : 129.17 / t : 0.10 / exercise value : -0.83 / error : -3.60\n",
      "S : 116.14 / t : 0.10 / exercise value : -13.86 / error : -13.96\n",
      "S : 122.68 / t : 0.10 / exercise value : -7.32 / error : -8.06\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.43 / t : 0.10 / exercise value : -2.57 / error : -4.61\n",
      "S : 133.42 / t : 0.10 / exercise value : 3.42 / error : -1.75\n",
      "S : 128.12 / t : 0.10 / exercise value : -1.88 / error : -4.19\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 134.55 / t : 0.10 / exercise value : 4.55 / error : -1.41\n",
      "S : 129.03 / t : 0.10 / exercise value : -0.97 / error : -3.68\n",
      "S : 134.57 / t : 0.10 / exercise value : 4.57 / error : -1.40\n",
      "S : 122.66 / t : 0.10 / exercise value : -7.34 / error : -8.07\n",
      "S : 122.48 / t : 0.10 / exercise value : -7.52 / error : -8.22\n",
      "S : 130.57 / t : 0.10 / exercise value : 0.57 / error : -2.90\n",
      "S : 123.64 / t : 0.10 / exercise value : -6.36 / error : -7.27\n",
      "S : 130.82 / t : 0.10 / exercise value : 0.82 / error : -2.79\n",
      "S : 122.82 / t : 0.10 / exercise value : -7.18 / error : -7.94\n",
      "S : 132.11 / t : 0.10 / exercise value : 2.11 / error : -2.24\n",
      "S : 123.59 / t : 0.10 / exercise value : -6.41 / error : -7.31\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 116.28 / t : 0.10 / exercise value : -13.72 / error : -13.83\n",
      "S : 126.21 / t : 0.10 / exercise value : -3.79 / error : -5.40\n",
      "S : 113.69 / t : 0.10 / exercise value : -16.31 / error : -16.35\n",
      "S : 134.06 / t : 0.10 / exercise value : 4.06 / error : -1.56\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 121.19 / t : 0.10 / exercise value : -8.81 / error : -9.31\n",
      "S : 120.35 / t : 0.10 / exercise value : -9.65 / error : -10.04\n",
      "S : 125.16 / t : 0.10 / exercise value : -4.84 / error : -6.13\n",
      "S : 131.61 / t : 0.10 / exercise value : 1.61 / error : -2.43\n",
      "S : 131.94 / t : 0.10 / exercise value : 1.94 / error : -2.30\n",
      "S : 145.36 / t : 0.10 / exercise value : 15.36 / error : -0.04\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 136.19 / t : 0.10 / exercise value : 6.19 / error : -0.99\n",
      "S : 138.56 / t : 0.10 / exercise value : 8.56 / error : -0.55\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 136.73 / t : 0.10 / exercise value : 6.73 / error : -0.87\n",
      "S : 111.09 / t : 0.10 / exercise value : -18.91 / error : -18.92\n",
      "S : 124.71 / t : 0.10 / exercise value : -5.29 / error : -6.47\n",
      "S : 127.90 / t : 0.10 / exercise value : -2.10 / error : -4.33\n",
      "S : 132.01 / t : 0.10 / exercise value : 2.01 / error : -2.28\n",
      "S : 134.23 / t : 0.10 / exercise value : 4.23 / error : -1.51\n",
      "S : 120.67 / t : 0.10 / exercise value : -9.33 / error : -9.76\n",
      "S : 115.75 / t : 0.10 / exercise value : -14.25 / error : -14.34\n",
      "S : 123.40 / t : 0.10 / exercise value : -6.60 / error : -7.47\n",
      "S : 114.72 / t : 0.10 / exercise value : -15.28 / error : -15.34\n",
      "S : 130.78 / t : 0.10 / exercise value : 0.78 / error : -2.81\n",
      "S : 140.35 / t : 0.10 / exercise value : 10.35 / error : -0.33\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 143.39 / t : 0.10 / exercise value : 13.39 / error : -0.11\n",
      "S : 118.44 / t : 0.10 / exercise value : -11.56 / error : -11.79\n",
      "S : 132.89 / t : 0.10 / exercise value : 2.89 / error : -1.94\n",
      "S : 136.77 / t : 0.10 / exercise value : 6.77 / error : -0.86\n",
      "S : 125.79 / t : 0.10 / exercise value : -4.21 / error : -5.69\n",
      "S : 125.43 / t : 0.10 / exercise value : -4.57 / error : -5.94\n",
      "S : 117.57 / t : 0.10 / exercise value : -12.43 / error : -12.60\n",
      "S : 127.71 / t : 0.10 / exercise value : -2.29 / error : -4.44\n",
      "S : 129.66 / t : 0.10 / exercise value : -0.34 / error : -3.35\n",
      "S : 147.20 / t : 0.10 / exercise value : 17.20 / error : -0.01\n",
      "S : 148.34 / t : 0.10 / exercise value : 18.34 / error : -0.00\n",
      "S : 122.64 / t : 0.10 / exercise value : -7.36 / error : -8.09\n",
      "S : 123.44 / t : 0.10 / exercise value : -6.56 / error : -7.44\n",
      "S : 132.94 / t : 0.10 / exercise value : 2.94 / error : -1.92\n",
      "S : 126.34 / t : 0.10 / exercise value : -3.66 / error : -5.31\n",
      "S : 127.48 / t : 0.10 / exercise value : -2.52 / error : -4.58\n",
      "S : 113.31 / t : 0.10 / exercise value : -16.69 / error : -16.73\n",
      "S : 119.65 / t : 0.10 / exercise value : -10.35 / error : -10.68\n",
      "S : 124.92 / t : 0.10 / exercise value : -5.08 / error : -6.31\n",
      "S : 127.01 / t : 0.10 / exercise value : -2.99 / error : -4.87\n",
      "S : 135.74 / t : 0.10 / exercise value : 5.74 / error : -1.10\n",
      "S : 136.16 / t : 0.10 / exercise value : 6.16 / error : -1.00\n",
      "S : 123.35 / t : 0.10 / exercise value : -6.65 / error : -7.51\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 122.81 / t : 0.10 / exercise value : -7.19 / error : -7.95\n",
      "S : 119.58 / t : 0.10 / exercise value : -10.42 / error : -10.74\n",
      "S : 120.93 / t : 0.10 / exercise value : -9.07 / error : -9.54\n",
      "S : 130.08 / t : 0.10 / exercise value : 0.08 / error : -3.13\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 130.38 / t : 0.10 / exercise value : 0.38 / error : -2.99\n",
      "S : 122.65 / t : 0.10 / exercise value : -7.35 / error : -8.08\n",
      "S : 123.18 / t : 0.10 / exercise value : -6.82 / error : -7.65\n",
      "S : 127.00 / t : 0.10 / exercise value : -3.00 / error : -4.88\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 129.26 / t : 0.10 / exercise value : -0.74 / error : -3.55\n",
      "S : 129.47 / t : 0.10 / exercise value : -0.53 / error : -3.44\n",
      "S : 139.78 / t : 0.10 / exercise value : 9.78 / error : -0.40\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 126.25 / t : 0.10 / exercise value : -3.75 / error : -5.37\n",
      "S : 127.98 / t : 0.10 / exercise value : -2.02 / error : -4.27\n",
      "S : 131.05 / t : 0.10 / exercise value : 1.05 / error : -2.68\n",
      "S : 127.49 / t : 0.10 / exercise value : -2.51 / error : -4.58\n",
      "S : 123.93 / t : 0.10 / exercise value : -6.07 / error : -7.06\n",
      "S : 120.41 / t : 0.10 / exercise value : -9.59 / error : -9.99\n",
      "S : 129.80 / t : 0.10 / exercise value : -0.20 / error : -3.27\n",
      "S : 125.44 / t : 0.10 / exercise value : -4.56 / error : -5.93\n",
      "S : 129.98 / t : 0.10 / exercise value : -0.02 / error : -3.17\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 132.11 / t : 0.10 / exercise value : 2.11 / error : -2.24\n",
      "S : 130.70 / t : 0.10 / exercise value : 0.70 / error : -2.84\n",
      "S : 120.18 / t : 0.10 / exercise value : -9.82 / error : -10.20\n",
      "S : 129.96 / t : 0.10 / exercise value : -0.04 / error : -3.18\n",
      "S : 119.89 / t : 0.10 / exercise value : -10.11 / error : -10.46\n",
      "S : 124.87 / t : 0.10 / exercise value : -5.13 / error : -6.35\n",
      "S : 135.89 / t : 0.10 / exercise value : 5.89 / error : -1.07\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 124.94 / t : 0.10 / exercise value : -5.06 / error : -6.29\n",
      "S : 130.43 / t : 0.10 / exercise value : 0.43 / error : -2.97\n",
      "S : 122.54 / t : 0.10 / exercise value : -7.46 / error : -8.17\n",
      "S : 138.25 / t : 0.10 / exercise value : 8.25 / error : -0.60\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 123.05 / t : 0.10 / exercise value : -6.95 / error : -7.75\n",
      "S : 129.82 / t : 0.10 / exercise value : -0.18 / error : -3.26\n",
      "S : 129.28 / t : 0.10 / exercise value : -0.72 / error : -3.55\n",
      "S : 132.52 / t : 0.10 / exercise value : 2.52 / error : -2.08\n",
      "S : 135.87 / t : 0.10 / exercise value : 5.87 / error : -1.07\n",
      "S : 115.51 / t : 0.10 / exercise value : -14.49 / error : -14.58\n",
      "S : 122.73 / t : 0.10 / exercise value : -7.27 / error : -8.01\n",
      "S : 136.83 / t : 0.10 / exercise value : 6.83 / error : -0.85\n",
      "S : 121.57 / t : 0.10 / exercise value : -8.43 / error : -8.98\n",
      "S : 134.54 / t : 0.10 / exercise value : 4.54 / error : -1.41\n",
      "S : 123.89 / t : 0.10 / exercise value : -6.11 / error : -7.09\n",
      "S : 138.98 / t : 0.10 / exercise value : 8.98 / error : -0.50\n",
      "S : 130.47 / t : 0.10 / exercise value : 0.47 / error : -2.95\n",
      "S : 126.06 / t : 0.10 / exercise value : -3.94 / error : -5.51\n",
      "S : 135.46 / t : 0.10 / exercise value : 5.46 / error : -1.17\n",
      "S : 122.55 / t : 0.10 / exercise value : -7.45 / error : -8.16\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n",
      "S : 127.62 / t : 0.00 / exercise value : -2.38 / error : -2.38\n"
     ]
    }
   ],
   "source": [
    "cartpole_env.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([144, 2])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cartpole_env.agent.brain.model.parameters())[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考（書籍のoriginal Environment）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV)  # 実行する課題を設定\n",
    "        num_states = self.env.observation_space.shape[0]  # 課題の状態数4を取得\n",
    "        num_actions = self.env.action_space.n  # CartPoleの行動（右に左に押す）の2を取得\n",
    "        self.agent = Agent(num_states, num_actions)  # 環境内で行動するAgentを生成\n",
    "\n",
    "        \n",
    "    def run(self):\n",
    "        '''実行'''\n",
    "        episode_10_list = np.zeros(10)  # 10試行分の立ち続けたstep数を格納し、平均ステップ数を出力に利用\n",
    "        complete_episodes = 0  # 195step以上連続で立ち続けた試行数\n",
    "        episode_final = False  # 最後の試行フラグ\n",
    "        frames = []  # 最後の試行を動画にするために画像を格納する変数\n",
    "\n",
    "        for episode in range(NUM_EPISODES):  # 最大試行数分繰り返す\n",
    "            observation = self.env.reset()  # 環境の初期化\n",
    "\n",
    "            state = observation  # 観測をそのまま状態sとして使用\n",
    "            state = torch.from_numpy(state).type(\n",
    "                torch.FloatTensor)  # NumPy変数をPyTorchのテンソルに変換\n",
    "            state = torch.unsqueeze(state, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "            for step in range(MAX_STEPS):  # 1エピソードのループ\n",
    "\n",
    "                if episode_final is True:  # 最終試行ではframesに各時刻の画像を追加していく\n",
    "                    frames.append(self.env.render(mode='rgb_array'))\n",
    "\n",
    "                action = self.agent.get_action(state, episode)  # 行動を求める\n",
    "\n",
    "                # 行動a_tの実行により、s_{t+1}とdoneフラグを求める\n",
    "                # actionから.item()を指定して、中身を取り出す\n",
    "                observation_next, _, done, _ = self.env.step(\n",
    "                    action.item())  # rewardとinfoは使わないので_にする\n",
    "\n",
    "                # 報酬を与える。さらにepisodeの終了評価と、state_nextを設定する\n",
    "                if done:  # ステップ数が200経過するか、一定角度以上傾くとdoneはtrueになる\n",
    "                    state_next = None  # 次の状態はないので、Noneを格納\n",
    "\n",
    "                    # 直近10episodeの立てたstep数リストに追加\n",
    "                    episode_10_list = np.hstack(\n",
    "                        (episode_10_list[1:], step + 1))\n",
    "\n",
    "                    if step < 195:\n",
    "                        reward = torch.FloatTensor(\n",
    "                            [-1.0])  # 途中でこけたら罰則として報酬-1を与える\n",
    "                        complete_episodes = 0  # 連続成功記録をリセット\n",
    "                    else:\n",
    "                        reward = torch.FloatTensor([1.0])  # 立ったまま終了時は報酬1を与える\n",
    "                        complete_episodes = complete_episodes + 1  # 連続記録を更新\n",
    "                else:\n",
    "                    reward = torch.FloatTensor([0.0])  # 普段は報酬0\n",
    "                    state_next = observation_next  # 観測をそのまま状態とする\n",
    "                    state_next = torch.from_numpy(state_next).type(\n",
    "                        torch.FloatTensor)  # numpy変数をPyTorchのテンソルに変換\n",
    "                    state_next = torch.unsqueeze(state_next, 0)  # size 4をsize 1x4に変換\n",
    "\n",
    "                # メモリに経験を追加\n",
    "                self.agent.memorize(state, action, state_next, reward)\n",
    "\n",
    "                # Experience ReplayでQ関数を更新する\n",
    "                self.agent.update_q_function()\n",
    "\n",
    "                # 観測の更新\n",
    "                state = state_next\n",
    "\n",
    "                # 終了時の処理\n",
    "                if done:\n",
    "                    print('%d Episode: Finished after %d steps：10試行の平均step数 = %.1lf' % (\n",
    "                        episode, step + 1, episode_10_list.mean()))\n",
    "                    break\n",
    "\n",
    "            if episode_final is True:\n",
    "                # 動画を保存と描画\n",
    "                display_frames_as_gif(frames)\n",
    "                break\n",
    "\n",
    "            # 10連続で200step経ち続けたら成功\n",
    "            if complete_episodes >= 10:\n",
    "                print('10回連続成功')\n",
    "                episode_final = True  # 次の試行を描画を行う最終試行とする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main クラス\n",
    "cartpole_env = Environment()\n",
    "cartpole_env.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base2]",
   "language": "python",
   "name": "conda-env-base2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "209px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "1377px",
    "left": "0px",
    "right": "1669.6px",
    "top": "111px",
    "width": "194px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
